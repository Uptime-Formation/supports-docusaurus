
Code examples : [*https://github.com/prometheus-up-and-running-2e/examples*](https://github.com/prometheus-up-and-running-2e/examples).

# Part I. - Introduction

# Chapter 1. - What Is Prometheus?

Prometheus is an open source, metrics-based monitoring system. Of
course, Prometheus is far from the only one of those out there, so what
makes it notable?

Prometheus does one thing and it does it well. It has a simple yet
powerful data model and a query language that lets you analyze how your
applications and infrastructure are performing. It does not try to solve
problems outside of the metrics space, leaving those to other more
appropriate tools.

Since its beginnings with no more than a handful of developers working
in SoundCloud in 2012, a community and ecosystem has grown around
Prometheus. Prometheus is primarily written in Go and licensed under the
Apache 2.0 license. There are hundreds of people who have contributed to
the project itself, which is not controlled by any one company. It is
always hard to tell how many users an open source project has, but we
estimate that as of 2022, hundreds of thousands of organizations are
using Prometheus in production. In 2016 the Prometheus project became
the second
member^[1](#ch01.xhtml#idm45207119593264){#ch01.xhtml#idm45207119593264-marker
data-type="noteref"}^ of the Cloud Native Computing Foundation
(CNCF).[]{#ch01.xhtml#idm45207116100688
primary="Cloud Native Computing Foundation (CNCF)"
data-type="indexterm"}

For instrumenting your own code, there are client libraries in all the
popular languages and runtimes, including Go, Java/JVM, C#/.Net, Python,
Ruby, Node.js, Haskell, Erlang, and Rust. Many popular applications are
already instrumented with Prometheus client libraries, like Kubernetes,
Docker, Envoy, and Vault. []{#ch01.xhtml#idm45207116099776
primary="client libraries" data-type="indexterm"}For third-party
software that exposes metrics in a non-Prometheus format, there are
hundreds of integrations available. []{#ch01.xhtml#idm45207116098944
primary="formats"
secondary="third-party software exposing metrics in non-Prometheus format"
data-type="indexterm"}[]{#ch01.xhtml#idm45207116098032
primary="exporters" data-type="indexterm"}These are called exporters,
and include HAProxy, MySQL, PostgreSQL, Redis, JMX, SNMP, Consul, and
Kafka. A friend of Brian's even added support for monitoring Minecraft
servers, as he cares a lot about his frames per second.

A simple text
format^[2](#ch01.xhtml#idm45207116096848){#ch01.xhtml#idm45207116096848-marker
data-type="noteref"}^ makes it easy to expose metrics to Prometheus.
Other monitoring systems, both open source and commercial, have added
support for this format. []{#ch01.xhtml#idm45207119489216
primary="formats" secondary="Prometheus text format and OpenMetrics"
seealso="exposition formats"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119488144
primary="text format (Prometheus)"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119487504
primary="OpenMetrics" data-type="indexterm"}This allows all of these
monitoring systems to focus more on core features, rather than each
having to spend time duplicating effort to support every single piece of
software a user like you may wish to monitor.

The data model identifies each time series not just with a name, but
also with an unordered set of key-value pairs called labels.
[]{#ch01.xhtml#idm45207119486320 primary="labels"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119485616 primary="PromQL"
data-type="indexterm"}The PromQL query language allows aggregation
across any of these labels, so you can analyze not just per process but
also per datacenter and per service or by any other labels that you have
defined. These can be graphed in dashboard systems such as
[Grafana](https://oreil.ly/f5uMZ) and [Perses](https://oreil.ly/YF-xW).

Alerts can be defined using the exact same PromQL query language that
you use for graphing. If you can graph it, you can alert on
it.[]{#ch01.xhtml#idm45207119276336 primary="alerts"
data-type="indexterm"} Labels make maintaining alerts easier, as you can
create a single alert covering all possible label values. In some other
monitoring systems you would have to individually create an alert per
machine/application. Relatedly, service discovery can automatically
determine what applications and machines should be scraped from sources
such as Kubernetes, Consul, Amazon Elastic Compute Cloud (EC2), Azure,
Google Compute Engine (GCE), and
OpenStack.[]{#ch01.xhtml#idm45207119275504 primary="service discovery"
data-type="indexterm"}

For all these features and benefits, Prometheus is efficient and simple
to run. A single Prometheus server can ingest millions of samples per
second. It is a single, statically linked binary with a configuration
file. All components of Prometheus can be run in containers, and they
avoid doing anything fancy that would get in the way of configuration
management tools. It is designed to be integrated into the
infrastructure you already have and built on top of, not to be a
management platform itself.

Now that you have an overview of what Prometheus is, let's step back for
a minute and look at what is meant by "monitoring" in order to provide
some context. Following that, we will look at what the main components
of Prometheus are, and what Prometheus is not.

::: {.section pdf-bookmark="What Is Monitoring?" data-type="sect1"}
::: {#ch01.xhtml#what_is_monitoring .sect1}
# What Is Monitoring?

In secondary school, one of Brian's teachers told him that if you were
to ask 10 economists what economics means, you'd get 11
answers.[]{#ch01.xhtml#ix_mntr primary="monitoring"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119270816
primary="monitoring" secondary="about" data-type="indexterm"} Monitoring
has a similar lack of consensus as to what exactly it means. When he
tells others what he does, people think his job entails everything from
keeping an eye on temperature in factories, to employee monitoring where
he is the one to find out who is accessing Facebook during working
hours, and even detecting intruders on networks.

Prometheus wasn't built to do any of those
things.^[3](#ch01.xhtml#idm45207119698704){#ch01.xhtml#idm45207119698704-marker
data-type="noteref"}^ It was built to aid software developers and
administrators in the operation of production computer systems, such as
the applications, tools, databases, and networks backing popular
websites.

So what is monitoring in that context?[]{#ch01.xhtml#idm45207119697472
primary="operational monitoring of computer systems"
data-type="indexterm"} Let's narrow this sort of operational monitoring
of computer systems down to four things:

Alerting

:   Knowing when things are going wrong is usually the most important
    thing that you want monitoring for.[]{#ch01.xhtml#idm45207119694752
    primary="alerting" data-type="indexterm"} You want the monitoring
    system to call in a human to take a look.

Debugging

:   Now that you have called in a human, they need to investigate to
    determine the root cause and ultimately resolve whatever the issue
    is.[]{#ch01.xhtml#idm45207119692640 primary="debugging"
    data-type="indexterm"}

Trending

:   Alerting and debugging usually happen on timescales on the order of
    minutes to hours. []{#ch01.xhtml#idm45207119497168
    primary="trending" data-type="indexterm"}While less urgent, the
    ability to see how your systems are being used and changing over
    time is also useful. Trending can feed into design decisions and
    processes such as capacity planning.

Plumbing

:   When all you have is a hammer, everything starts to look like a
    nail.[]{#ch01.xhtml#idm45207119495056 primary="plumbing"
    data-type="indexterm"} At the end of the day, all monitoring systems
    are data processing pipelines. Sometimes it is more convenient to
    appropriate part of your monitoring system for another purpose,
    rather than building a bespoke solution. This is not strictly
    monitoring, but it is common in practice so we like to include it.

Depending on who you talk to and their background, they may consider
only some of these to be monitoring. This leads to many discussions
about monitoring going around in circles, leaving everyone frustrated.
To help you understand where others are coming from, we're going to look
at a small bit of the history of monitoring.

::: {.section .less_space .pagebreak-before pdf-bookmark="A Brief and Incomplete History of Monitoring" data-type="sect2"}
::: {#ch01.xhtml#idm45207119493504 .sect2}
## A Brief and Incomplete History of Monitoring

Monitoring has seen a shift toward tools including Prometheus in the
past few years.[]{#ch01.xhtml#idm45207119491296 primary="monitoring"
secondary="brief history of" data-type="indexterm"} For a long time, the
dominant solution has been some combination of Nagios and Graphite or
their variants.[]{#ch01.xhtml#idm45207119809600 primary="Nagios"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119808928
primary="Graphite" data-type="indexterm"}

When we say Nagios, we are including any software within the same broad
family, such as Icinga, Zmon, and Sensu. They work primarily by
regularly executing scripts called *checks*. If a check fails by
returning a nonzero exit code, an alert is generated. Nagios was
initially started by Ethan Galstad in 1996 as an MS-DOS application used
to perform pings. It was first released as NetSaint in 1999, and renamed
Nagios in 2002.

To talk about the history of Graphite, we need to go back to 1994.
Tobias Oetiker created a Perl script that became Multi Router Traffic
Grapher, or MRTG 1.0, in 1995. As the name indicates, it was mainly used
for network monitoring via the Simple Network Management Protocol
(SNMP). It could also obtain metrics by executing
scripts.^[4](#ch01.xhtml#idm45207119807040){#ch01.xhtml#idm45207119807040-marker
data-type="noteref"}^ The year 1997 brought big changes with a move of
some code to C, and the creation of the Round Robin Database (RRD),
which was used to store metric data. This brought notable performance
improvements, and RRD was the basis for other tools, including Smokeping
and Graphite.

Started in 2006, Graphite uses Whisper for metrics storage, which has a
similar design to RRD. Graphite does not collect data itself, rather it
is sent in by collection tools such as collectd and StatsD, which were
created in 2005 and 2010, respectively.

The key takeaway here is that graphing and alerting were once completely
separate concerns performed by different tools. You could write a check
script to evaluate a query in Graphite and generate alerts on that
basis, but most checks tended to be on unexpected states such as a
process not running.

Another holdover from this era is the relatively manual approach to
administering computer services.[]{#ch01.xhtml#idm45207119804864
primary="services" secondary="administering, manual approach to"
data-type="indexterm"} Services were deployed on individual machines and
lovingly cared for by system administrators. Alerts that might
potentially indicate a problem were jumped upon by devoted engineers. As
cloud and cloud native technologies such as EC2, Docker, and Kubernetes
have come to prominence, treating individual machines and services like
pets with each getting individual attention does not scale. Rather, they
tend to be looked at more as cattle and administered and monitored as a
group. In the same way that the industry has moved from doing management
by hand, to tools like Chef and Ansible, to now starting to use
technologies like Kubernetes, monitoring also needs to make a similar
transition. This means moving from checks on individual processes on
individual machines to monitoring based on service health as a
whole.[]{#ch01.xhtml#idm45207119803744 primary="processes"
secondary="moving from checks on individual processes to service health as a whole"
data-type="indexterm"}

Moving to a more recent time, OpenTelemetry is born from two other open
source projects, OpenCensus and OpenTracing.
OTel^[5](#ch01.xhtml#idm45207119802368){#ch01.xhtml#idm45207119802368-marker
data-type="noteref"}^ is a specification and a set of
[components]{.keep-together} that aim to offer built-in telemetry for
projects. []{#ch01.xhtml#idm45207119589008
primary="OTel (OpenTelemetry)"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119588304
primary="OpenTelemetry (OTel)" data-type="indexterm"}Its metrics
component is compatible with Prometheus with the addition of the
OpenTelemetry
collector,^[6](#ch01.xhtml#idm45207119587392){#ch01.xhtml#idm45207119587392-marker
data-type="noteref"}^ which is responsible for collecting and providing
metrics to your Prometheus server.

You may have noticed that we didn't mention logging, tracing, and
profiling. Historically, logs have been used as something that you use
`tail`, `grep`, and `awk` on by hand. You might have had an analysis
tool such as AWStats to produce reports hourly or daily. In more recent
years, logs have also been used as a significant part of monitoring,
such as with the Elasticsearch, Logstash, and Kibana (ELK) and
OpenSearch stack. Tracing and profiling are generally done with their
own software stack: Zipkin and Jaeger are made for tracing, while Parca
and Pyroscope deal with continuous profiling.

Now that we have looked a bit at graphing and alerting, let's look at
how metrics and logs fit into the landscape. Are there more categories
of monitoring than those two?
:::
:::

::: {.section pdf-bookmark="Categories of Monitoring" data-type="sect2"}
::: {#ch01.xhtml#idm45207119584208 .sect2}
## Categories of Monitoring

At the end of the day, most monitoring is about the same thing:
events.[]{#ch01.xhtml#idm45207119582624 primary="monitoring"
secondary="categories of"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119581568 primary="events"
data-type="indexterm"} Events can be almost anything, including:

-   Receiving an HTTP request

-   Sending an HTTP 400 response

-   Entering a function

-   Reaching the `else` of an `if` statement

-   Leaving a function

-   A user logging in

-   Writing data to disk

-   Reading data from the network

-   Requesting more memory from the kernel

All events also have context. []{#ch01.xhtml#idm45207119373808
primary="context of events" data-type="indexterm"}An HTTP request will
have the IP address it is coming from and going to, the URL being
requested, the cookies that are set, and the user who made the request.
An HTTP response will have how long the response took, the HTTP status
code, and the length of the response body. Events involving functions
have the call stack of the functions above them, and whatever triggered
this part of the stack, such as an HTTP request.

Having all the context for all the events would be great for debugging
and understanding how your systems are performing in both technical and
business terms, but that amount of data is not practical to process and
store. Thus, we see roughly four ways to approach reducing that volume
of data to something workable, namely profiling, tracing, logging, and
metrics.

::: {.section pdf-bookmark="Profiling" data-type="sect3"}
::: {#ch01.xhtml#idm45207119687328 .sect3}
### Profiling

Profiling takes the approach that you can't have all the context for all
of the events all of the time, but you can have some of the context for
limited periods of time.[]{#ch01.xhtml#idm45207119685760
primary="profiling" data-type="indexterm"}

Tcpdump is one example of a profiling tool.
[]{#ch01.xhtml#idm45207119684672 primary="tcpdump"
data-type="indexterm"}It allows you to record network traffic based on a
specified filter. It's an essential debugging tool, but you can't really
turn it on all the time as you will run out of disk space.

Debug builds of binaries that track profiling data are another example.
They provide a plethora of useful information, but the performance
impact of gathering all that information, such as timings of every
function call, means that it is not generally practical to run it in
production on an ongoing basis.

In the Linux kernel, enhanced Berkeley Packet Filters (eBPF) allow
detailed profiling of kernel events from filesystem operations to
network oddities.[]{#ch01.xhtml#idm45207119682880 primary="Linux"
secondary="profiling of kernel events"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119681840
primary="eBPF (enhanced Berkeley Packet Filters)" data-type="indexterm"}
These provide access to a level of insight that was not generally
available previously. eBPF comes with other advantages, such as
portability and safety. We'd recommend reading [Brendan Gregg's
writings](https://oreil.ly/n15mM) on the subject.

Profiling is largely for tactical debugging. If it is being used on a
longer-term basis, then the data volume must be cut down in order to fit
into one of the other categories of monitoring, or you'd need to move to
*continuous profiling*, which enables the collection over longer
runs.[]{#ch01.xhtml#idm45207119679536 primary="continuous profiling"
data-type="indexterm"}

What's new with continuous profiling is that in order to cut down the
data volume and keep a relatively low overhead, it reduces the profiling
frequency. One of the emerging continuous profiling tools, the
eBPF-based [Parca](https://parca.dev) Agent, uses a 19Hz
frequency.^[7](#ch01.xhtml#idm45207119677712){#ch01.xhtml#idm45207119677712-marker
data-type="noteref"}^ As a consequence, it tries to get statistically
significant data over minutes rather than seconds, while still providing
the data required to understand how the CPU time is spent in an
infrastructure, and helping to improve application efficiency where it's
needed.
:::
:::

::: {.section pdf-bookmark="Tracing" data-type="sect3"}
::: {#ch01.xhtml#idm45207119676896 .sect3}
### Tracing

Tracing doesn't typically look at all events, rather it takes some
proportion of events such as one in a hundred that pass through some
functions of interest.[]{#ch01.xhtml#idm45207119675360 primary="tracing"
data-type="indexterm"} Tracing will note the functions in the stack
trace of the points of interest, and often also how long each of these
functions took to execute. From this you can get an idea of where your
program is spending time and which code paths are most contributing to
latency.

Rather than doing snapshots of stack traces at points of interest, some
tracing systems trace and record timings of every function call below
the function of interest. []{#ch01.xhtml#idm45207119673952
primary="HTTP requests" data-type="indexterm"}For example, one in a
hundred user HTTP requests might be sampled, and for those requests you
could see how much time was spent talking to backends such as databases
and caches. []{#ch01.xhtml#idm45207119672928 primary="caches"
data-type="indexterm"}This allows you to see how timings differ based on
factors like cache hits versus cache misses.

Distributed tracing takes this a step further.
[]{#ch01.xhtml#idm45207114297312 primary="distributed tracing"
data-type="indexterm"}It makes tracing work across processes by
attaching unique IDs to requests that are passed from one process to
another in remote procedure calls (RPCs) in addition to whether this
request is one that should be traced. The traces from different
processes and machines can be stitched back together based on the
request ID. This is a vital tool for debugging distributed microservices
architectures. Technologies in this space include OpenZipkin and Jaeger.

For tracing, it is the sampling that keeps the data volumes and
instrumentation performance impact within
reason.[]{#ch01.xhtml#idm45207114295968 primary="sampling"
secondary="for tracing" secondary-sortas="tracing"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Logging" data-type="sect3"}
::: {#ch01.xhtml#logging .sect3}
### Logging

Logging looks at a limited set of events and records some of the context
for each of these events. []{#ch01.xhtml#idm45207114292544
primary="logging" data-type="indexterm"}For example, it may look at all
incoming HTTP requests, or all outgoing database calls. To avoid
consuming too many resources, as a rule of thumb you are limited to
somewhere around a hundred fields per log entry. Beyond that, bandwidth
and storage space tend to become a concern.

For example, for a server handling 1,000 requests per second, a log
entry with 100 fields each taking 10 bytes works out as 1 megabyte per
second. That's a nontrivial proportion of a 100 Mbit network card, and
84 GB of storage per day just for logging.

A big benefit of logging is that there is (usually) no sampling of
events, so even though there is a limit on the number of fields, it is
practical to determine how slow requests are affecting one particular
user talking to one particular API endpoint.

Just as monitoring means different things to different people, logging
also means different things depending on who you ask, which can cause
confusion. Different types of logging have different uses, durability,
and retention requirements.[]{#ch01.xhtml#idm45207114290224
primary="logging" secondary="categories of" data-type="indexterm"} As we
see it, there are four general and somewhat overlapping categories:

Transaction logs

:   These are the critical business records that you must keep safe at
    all costs, likely forever.[]{#ch01.xhtml#idm45207114287312
    primary="transaction logs" data-type="indexterm"} Anything touching
    on money or that is used for critical user-facing features tends to
    be in this category.

Request logs

:   If you are tracking every HTTP request, or every database call,
    that's a request log.[]{#ch01.xhtml#idm45207114284896
    primary="request logs" data-type="indexterm"} They may be processed
    in order to implement user facing features, or just for internal
    optimizations. You don't generally want to lose them, but it's not
    the end of the world if some of them go missing.

Application logs

:   Not all logs are about requests; some are about the process itself.
    Startup messages, background maintenance tasks, and other
    process-level log lines are typical.[]{#ch01.xhtml#idm45207118500928
    primary="application logs" data-type="indexterm"} These logs are
    often read directly by a human, so you should try to avoid having
    more than a few per minute in normal operations.

Debug logs

:   Debug logs tend to be very detailed and thus expensive to create and
    store. They are often only used in very narrow debugging situations,
    and are trending toward profiling due to their data
    volume.[]{#ch01.xhtml#idm45207118498816 primary="debug logs"
    data-type="indexterm"} Reliability and retention requirements tend
    to be low, and debug logs may not even leave the machine they are
    generated on.

Treating the differing types of logs all in the same way can put you in
the worst of all worlds, where you have the data volume of debug logs
combined with the extreme reliability requirements of transaction logs.
Thus as your system grows, you should plan on splitting out the debug
logs so that they can be handled separately.

Examples of logging systems include the ELK stack, OpenSearch, Grafana
Loki, and Graylog.[]{#ch01.xhtml#idm45207118496672
primary="logging systems" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Metrics" data-type="sect3"}
::: {#ch01.xhtml#idm45207118495840 .sect3}
### Metrics

Metrics largely ignore context, instead tracking aggregations over time
of different types of events. []{#ch01.xhtml#idm45207118494352
primary="metrics" data-type="indexterm"}To keep resource usage sane, the
amount of different numbers being tracked needs to be limited: 10,000
per process is a reasonable upper bound for you to keep in mind.

Examples of the sort of metrics you might have would be the number of
times you received HTTP requests, how much time was spent handling
requests, and how many requests are currently in
progress.[]{#ch01.xhtml#idm45207118492656 primary="HTTP requests"
secondary="metrics on" data-type="indexterm"} By excluding any
information about context, the data volumes and processing required are
kept reasonable.

That is not to say, though, that context is always ignored. For an HTTP
request you could decide to have a metric for each URL path. But the
10,000 metric guideline has to be kept in mind, as each distinct path
now counts as a metric. Using context such as a user's email address
would be unwise, as they have an unbounded
cardinality.^[8](#ch01.xhtml#idm45207118491040){#ch01.xhtml#idm45207118491040-marker
data-type="noteref"}^

You can use metrics to track the latency and data volumes handled by
each of the subsystems in your applications, making it easier to
determine the cause of a slowdown. Logs cannot record that many fields,
but once you know which subsystem is to blame, logs can help you figure
out which exact user requests are involved.

This is where the trade-off between logs and metrics becomes most
apparent. Metrics allow you to collect information about events from all
over your process, but with generally no more than one or two fields of
context with bounded cardinality. Logs allow you to collect information
about all of one type of event, but can only track a hundred fields of
context with unbounded cardinality. Cardinality and the limits it places
on metrics is important to understand, and we explore it in later
chapters.

As a metrics-based monitoring system, Prometheus is designed to track
overall system health, behavior, and performance rather than individual
events. Put another way, Prometheus cares that there were 15 requests in
the last minute that took 4 seconds to handle, resulted in 40 database
calls, 17 cache hits, and 2 purchases by customers.
[]{#ch01.xhtml#idm45207118489264 primary="caches"
data-type="indexterm"}The cost and code paths of the individual calls
would be the concern of profiling or logging.

Now that you have an understanding of where Prometheus fits in the
overall monitoring space, let's look at the various components of
Prometheus.[]{#ch01.xhtml#idm45207118487776 primary="monitoring"
startref="ix_mntr" data-type="indexterm"}
:::
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Prometheus Architecture" data-type="sect1"}
::: {#ch01.xhtml#idm45207118486416 .sect1}
# Prometheus Architecture

[Figure 1-1](#ch01.xhtml#architecture_diagram){data-type="xref"} shows
the overall architecture of Prometheus. Prometheus discovers targets to
scrape from service discovery.[]{#ch01.xhtml#ix_Prmtarch
primary="Prometheus" secondary="architecture"
data-type="indexterm"}[]{#ch01.xhtml#ix_archPr
primary="architecture of Prometheus" data-type="indexterm"} These can be
your own instrumented applications or third-party applications you can
scrape via an exporter. The scraped data is stored, and you can use it
in dashboards using PromQL or send alerts to the Alertmanager, which
will convert them into pages, emails, and other notifications.

<figure>
<div id="ch01.xhtml#architecture_diagram" class="figure">
<img src="assets/pur2_0101.png" width="600" height="306"
alt="Architecture diagram" />
<h6><span class="label">Figure 1-1. </span>The Prometheus
architecture</h6>
</div>
</figure>

::: {.section pdf-bookmark="Client Libraries" data-type="sect2"}
::: {#ch01.xhtml#idm45207113065296 .sect2}
## Client Libraries

Metrics do not typically magically spring forth from applications;
someone has to add the instrumentation that produces
them.[]{#ch01.xhtml#idm45207113063584 primary="client libraries"
data-type="indexterm"} This is where client libraries come in. With
usually only two or three lines of code, you can both define a metric
and add your desired instrumentation inline in code you control. This is
referred to as direct instrumentation.

Client libraries are available for all the major languages and runtimes.
The Prometheus project provides official client libraries in Go, Python,
Java/JVM, Ruby, and Rust. There are also a variety of third-party client
libraries, such as for C#/.Net, Node.js, Haskell, and Erlang.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch01.xhtml#idm45207113062240 .sidebar}
# Official Versus Unofficial

Don't be put off by integrations such as client libraries being
unofficial or third party. []{#ch01.xhtml#idm45207113060832
primary="client libraries" secondary="official versus unofficial"
data-type="indexterm"}With hundreds of applications and systems that you
may wish to integrate with, it is not possible for the Prometheus
project team to have the time and expertise to create and maintain them
all. Thus the vast majority of integrations in the ecosystem are third
party. In order to keep things reasonably consistent and working as you
would expect, guidelines are available on how to write integrations.
:::

```{=html}
</aside>
```
Client libraries take care of all the nitty-gritty details such as
thread safety, bookkeeping, and producing the Prometheus text and/or
OpenMetrics exposition format in response to HTTP
requests.[]{#ch01.xhtml#idm45207113058672 primary="metrics"
secondary="details handled by client libraries"
data-type="indexterm"}[]{#ch01.xhtml#idm45207113057680
primary="HTTP requests" secondary="exposition format in response to"
data-type="indexterm"} As metrics-based monitoring does not track
individual events, client library memory usage does not increase with
the more events you have. Rather, memory is related to the number of
metrics you have.

If one of the library dependencies of your application has Prometheus
instrumentation, it will automatically be picked up. Thus by
instrumenting a key library such as your RPC client, you can get
instrumentation for it in all of your applications.

Some metrics, such as CPU usage and garbage collection statistics, are
typically provided out of the box by client libraries, depending on the
library and runtime environment.

Client libraries are not restricted to outputting metrics in the
Prometheus and OpenMetrics text formats. Prometheus is an open
ecosystem, and the same APIs used to feed the text format generation can
be used to produce metrics in other formats or to feed into other
instrumentation systems. Similarly, it is possible to take metrics from
other instrumentation systems and plumb them into a Prometheus client
library, if you haven't quite converted everything to Prometheus
instrumentation yet.
:::
:::

::: {.section pdf-bookmark="Exporters" data-type="sect2"}
::: {#ch01.xhtml#idm45207118483856 .sect2}
## Exporters

Not all code you run is code that you can control or even have access
to, and thus adding direct instrumentation isn't really an
option.[]{#ch01.xhtml#idm45207118482288 primary="exporters"
data-type="indexterm"} For example, it is unlikely that operating system
kernels will start outputting Prometheus-formatted metrics over HTTP
anytime soon.

Such software often has some interface through which you can access
metrics. This might be an ad hoc format requiring custom parsing and
handling, such as is required for many Linux metrics, or a
well-established standard such as SNMP.

An exporter is a piece of software that you deploy right beside the
application you want to obtain metrics from. It takes in requests from
Prometheus, gathers the required data from the application, transforms
it into the correct format, and finally returns it in a response to
Prometheus. You can think of an exporter as a small one-to-one proxy,
converting data between the metrics interface of an application and the
Prometheus exposition format.

Unlike the direct instrumentation you would use for code you control,
exporters use a different style of[]{#ch01.xhtml#idm45207118479744
primary="ConstMetrics"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118479040
primary="collectors" secondary="custom"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118478096
primary="MustNewConstMetric function"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118477456 primary="Go"
secondary="custom collectors written in" data-type="indexterm"}
instrumentation known as *custom collectors* or
*ConstMetrics*.^[9](#ch01.xhtml#idm45207118475536){#ch01.xhtml#idm45207118475536-marker
data-type="noteref"}^

The good news is that given the size of the Prometheus community, the
exporter you need probably already exists and can be used with little
effort on your part. If the exporter is missing a metric you are
interested in, you can always send a pull request to improve it, making
it better for the next person to use it.
:::
:::

::: {.section pdf-bookmark="Service Discovery" data-type="sect2"}
::: {#ch01.xhtml#idm45207118474048 .sect2}
## Service Discovery

Once you have all your applications instrumented and your exporters
running, Prometheus needs to know where they
are.[]{#ch01.xhtml#idm45207118472352 primary="service discovery"
data-type="indexterm"} This is so Prometheus will know what to monitor,
and be able to notice if something it is meant to be monitoring is not
responding. With dynamic environments you cannot simply provide a list
of applications and exporters once, as it will get out of date. This is
where service discovery comes in.

You probably already have some database of your machines, applications,
and what they do. It might be inside Chef's database, an inventory file
for Ansible, based on tags on your EC2 instance, in labels and
annotations in Kubernetes, or maybe just sitting in your documentation
wiki.

Prometheus has integrations with many common service discovery
mechanisms, such as Kubernetes, EC2, and Consul. There is also a generic
integration for those whose setup is a little off the beaten path (see
["File"](#ch08.xhtml#file_sd){data-type="xref"} and
["HTTP"](#ch08.xhtml#http_sd){data-type="xref"}).

This still leaves a problem, though. Just because Prometheus has a list
of machines and services doesn't mean we know how they fit into your
architecture. For example, you might be using the EC2 `Name`
tag^[10](#ch01.xhtml#idm45207118468016){#ch01.xhtml#idm45207118468016-marker
data-type="noteref"}^ to indicate what application runs on a machine,
whereas others might use a tag called `app`.

As every organization does it slightly differently, Prometheus allows
you to configure how metadata from service discovery is mapped to
monitoring targets and their labels using
*relabeling*.[]{#ch01.xhtml#idm45207118465232 primary="labels"
secondary="relabeling and"
data-type="indexterm"}[]{#ch01.xhtml#idm45207119371568
primary="relabeling" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Scraping" data-type="sect2"}
::: {#ch01.xhtml#idm45207119370736 .sect2}
## Scraping

Service discovery and relabeling give us a list of targets to be
monitored.[]{#ch01.xhtml#idm45207119369552 primary="scraping"
data-type="indexterm"} Now Prometheus needs to fetch the metrics.
Prometheus does this by sending an HTTP request called a *scrape*. The
response to the scrape is parsed and ingested into storage. Several
useful metrics are also added in, such as if the scrape succeeded and
how long it took. Scrapes happen regularly; usually you would configure
it to happen every 10 to 60 seconds for each target.

```{=html}
<aside class="less_space" data-type="sidebar" epub:type="sidebar">
```
::: {#ch01.xhtml#idm45207119367904 .sidebar}
# Pull Versus Push

Prometheus is a pull-based system. It decides when and what to scrape,
based on its configuration. There are also push-based systems, where the
monitoring target decides if it is going to be monitored and how often.

There is vigorous debate online about the two designs, which often bears
similarities to debates around Vim versus
EMACS.[]{#ch01.xhtml#idm45207119365568 primary="Vim"
data-type="indexterm"} Suffice to say both have pros and cons, and
overall it doesn't matter much.

As a Prometheus user you should understand that pull is ingrained in the
core of Prometheus, and attempting to make it do push instead is at best
unwise.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Storage" data-type="sect2"}
::: {#ch01.xhtml#idm45207119363968 .sect2}
## Storage

Prometheus stores data locally in a custom
database.[]{#ch01.xhtml#idm45207119362640 primary="storage"
data-type="indexterm"} Distributed systems are challenging to make
reliable, so Prometheus does not attempt to do any form of
clustering.[]{#ch01.xhtml#idm45207119361680 primary="clustering"
data-type="indexterm"} In addition to reliability, this makes Prometheus
easier to run.

Over the years, storage has gone through a number of redesigns, with the
storage system in Prometheus 2.0 being the third iteration. The storage
system can handle ingesting millions of samples per second, making it
possible to monitor thousands of machines with a single Prometheus
server. The compression algorithm used can achieve 1.3 bytes per sample
on real-world data. An SSD is recommended, but not strictly required.
:::
:::

::: {.section pdf-bookmark="Dashboards" data-type="sect2"}
::: {#ch01.xhtml#idm45207119360160 .sect2}
## Dashboards

Prometheus has a number of HTTP APIs that allow you to both request raw
data and evaluate PromQL queries. []{#ch01.xhtml#idm45207119358704
primary="dashboards" seealso="Grafana" data-type="indexterm"}These can
be used to produce graphs and dashboards. Out of the box, Prometheus
provides the *expression browser*. []{#ch01.xhtml#idm45207119357008
primary="expression browser" data-type="indexterm"}It uses these APIs
and is suitable for ad hoc querying and data exploration, but it is not
a general dashboard system.

It is recommended that you use Grafana for dashboards.
[]{#ch01.xhtml#idm45207119355568 primary="Grafana"
data-type="indexterm"}It has a wide variety of features, including
official support for Prometheus as a data source. It can produce a wide
variety of dashboards, such as the one in
[Figure 1-2](#ch01.xhtml#grafana_screenshot){data-type="xref"}. Grafana
supports talking to multiple Prometheus servers, even within a single
dashboard panel.

<figure>
<div id="ch01.xhtml#grafana_screenshot" class="figure">
<img src="assets/pur2_0102.png" width="600" height="363"
alt="A Grafana dashboard" />
<h6><span class="label">Figure 1-2. </span><a
href="https://oreil.ly/ytkNa">A Grafana dashboard</a></h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="Recording Rules and Alerts" data-type="sect2"}
::: {#ch01.xhtml#idm45207118970048 .sect2}
## Recording Rules and Alerts

Although PromQL and the storage engine are powerful and efficient,
aggregating metrics from thousands of machines on the fly every time you
render a graph can get a little laggy.[]{#ch01.xhtml#idm45207118968560
primary="recording rules"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118967856
primary="alerting rules" data-type="indexterm"} Recording rules allow
PromQL expressions to be evaluated on a regular basis and their results
ingested into the storage engine.

Alerting rules are another form of recording rules. They also evaluate
PromQL expressions regularly, and any results from those expressions
become alerts. Alerts are sent to the *Alertmanager*.
:::
:::

::: {.section pdf-bookmark="Alert Management" data-type="sect2"}
::: {#ch01.xhtml#idm45207118965664 .sect2}
## Alert Management

The Alertmanager receives alerts from Prometheus servers and turns them
into notifications. Notifications can include email, chat applications
such as Slack, and services such as
PagerDuty.[]{#ch01.xhtml#idm45207118963744 primary="Alertmanager"
data-type="indexterm"}

The Alertmanager does more than blindly turn alerts into notifications
on a one-to-one basis. Related alerts can be aggregated into one
notification, throttled to reduce pager
storms,^[11](#ch01.xhtml#idm45207118962192){#ch01.xhtml#idm45207118962192-marker
data-type="noteref"}^ and different routing and notification outputs can
be configured for each of your different teams. Alerts can also be
silenced, perhaps to snooze an issue you are already aware of in advance
when you know maintenance is scheduled.

The Alertmanager's role stops at sending notifications; to manage human
responses to incidents you should use services such as PagerDuty and
ticketing systems.

::: {data-type="tip"}
###### Tip

Alerts and their thresholds are configured in Prometheus, not in the
Alertmanager.
:::
:::
:::

::: {.section pdf-bookmark="Long-Term Storage" data-type="sect2"}
::: {#ch01.xhtml#idm45207118959280 .sect2}
## Long-Term Storage

Since []{#ch01.xhtml#idm45207118957952 primary="long-term storage"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118957216 primary="storage"
secondary="long-term" data-type="indexterm"}Prometheus stores data only
on the local machine, you are limited by how much disk space you can fit
on that
machine.^[12](#ch01.xhtml#idm45207118956016){#ch01.xhtml#idm45207118956016-marker
data-type="noteref"}^ While you usually care only about the most recent
day or so worth of data, for long-term capacity planning, a longer
retention period is desirable.[]{#ch01.xhtml#idm45207118954272
primary="LTS" see="long-term storage" data-type="indexterm"}

Prometheus does not offer a clustered storage solution to store data
across multiple machines, but there are remote read and write APIs that
allow other systems to hook in and take on this role. These allow PromQL
queries to be run transparently against both local and remote
data.[]{#ch01.xhtml#idm45207118952896
primary="architecture of Prometheus" startref="ix_archPr"
data-type="indexterm"}[]{#ch01.xhtml#idm45207118951904
primary="Prometheus" secondary="architecture" startref="ix_Prmtarch"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="What Prometheus Is Not" data-type="sect1"}
::: {#ch01.xhtml#idm45207118950304 .sect1}
# What Prometheus Is Not

Now that you have an idea of where Prometheus fits in the broader
monitoring landscape and what its major components are, let's look at
some use cases for which Prometheus is not a particularly good
choice.[]{#ch01.xhtml#idm45207118948768 primary="Prometheus"
secondary="use cases not suited for" data-type="indexterm"}

As a metrics-based system, Prometheus is not suitable for storing event
logs or individual events. Nor is it the best choice for
high-cardinality data, such as email addresses or usernames.

Prometheus is designed for operational monitoring, where small
inaccuracies and race conditions due to factors like kernel scheduling
and failed scrapes are a fact of life. Prometheus makes trade-offs and
prefers giving you data that is 99.9% correct over your monitoring
breaking while waiting for perfect data. Thus in applications involving
money or billing, Prometheus should be used with
caution.[]{#ch01.xhtml#idm45207113439824 primary="billing"
data-type="indexterm"}

In the next chapter we will show you how to run Prometheus and do some
basic [monitoring]{.keep-together}.
:::
:::

::: {data-type="footnotes"}
^[1](#ch01.xhtml#idm45207119593264-marker)^ Kubernetes was the first
member.

^[2](#ch01.xhtml#idm45207116096848-marker)^ Next to the simple text
format, a more standardized version, slightly different, called
OpenMetrics has been created out of the Prometheus text format.

^[3](#ch01.xhtml#idm45207119698704-marker)^ Temperature monitoring of
machines and datacenters is actually not uncommon. There are even a few
users using Prometheus to track the weather for fun.

^[4](#ch01.xhtml#idm45207119807040-marker)^ Brian has fond memories of
setting up MRTG in the early 2000s, writing scripts to report
temperature and network usage on my home computers.

^[5](#ch01.xhtml#idm45207119802368-marker)^ OTel is an informal name for
OpenTelemetry.

^[6](#ch01.xhtml#idm45207119587392-marker)^ At the time of writing,
developers at a Prometheus developer summit have decided that the
Prometheus server will support the OTel protocol natively in the future,
but there is no firm decision about when and how this will happen.

^[7](#ch01.xhtml#idm45207119677712-marker)^ To be compared to Go
runtime's 100Hz frequency or even 10,000Hz in Chromium.

^[8](#ch01.xhtml#idm45207118491040-marker)^ Email addresses also tend to
be personally identifiable information (PII), which bring with them
compliance and privacy concerns that are best avoided in monitoring.

^[9](#ch01.xhtml#idm45207118475536-marker)^ The term ConstMetric is
colloquial, and comes from the Go client library's `MustNewConstMetric`
function used to produce metrics by exporters written in Go.

^[10](#ch01.xhtml#idm45207118468016-marker)^ The EC2 `Name` tag is the
display name of an EC2 instance in the EC2 web console.

^[11](#ch01.xhtml#idm45207118962192-marker)^ A *page* is a notification
to an on call engineer that they are expected to promptly investigate or
deal with. While you may receive a page via a traditional radio pager,
these days it more likely comes to your mobile phone in the form of an
SMS, notification, or phone call. A pager storm is when you receive a
string of pages in rapid succession.

^[12](#ch01.xhtml#idm45207118956016-marker)^ However, modern machines
can hold a lot of data locally, so a []{#ch01.xhtml#idm45207118955488
primary="clustered storage system" data-type="indexterm"}separate
clustered storage system may not be necessary for you.
:::
:::
:::

[]{#ch02.xhtml}

::: {#ch02.xhtml#sbo-rt-content}
::: {#ch02.xhtml#chapter_getting_started .chapter}
# [Chapter 2. ]{.label}Getting Started with Prometheus

In this chapter you will set up and run Prometheus, the Node Exporter,
and the Alertmanager. This simple example will monitor a single machine
and give you a small taste of what a full Prometheus deployment looks
like. Later chapters will look at each aspect of this setup in
detail.[]{#ch02.xhtml#ix_run primary="running Prometheus"
data-type="indexterm"}

This chapter requires a machine running any reasonable, modern version
of Linux. Either bare metal or a virtual machine will
do.[]{#ch02.xhtml#idm45207113434608 primary="running Prometheus"
secondary="requirements" data-type="indexterm"} You will use the command
line and access services on the machine using a web browser. For
simplicity we will assume that everything is running on localhost; if
this is not the case, adjust the URLs as appropriate.

::: {data-type="tip"}
###### Tip

A basic setup similar to the one used in this chapter is publicly
available on the [Prometheus demo site](https://oreil.ly/KHxZC).
:::

::: {.section pdf-bookmark="Running Prometheus" data-type="sect1"}
::: {#ch02.xhtml#idm45207113431168 .sect1}
# Running Prometheus

Prebuilt versions of Prometheus and other components are available from
the [Prometheus download page](https://oreil.ly/e_S6d). Go to that page
and download the latest version of Prometheus for the Linux OS with Arch
amd64; the download page will look something like
[Figure 2-1](#ch02.xhtml#prometheus_download){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_download" class="figure">
<img src="assets/pur2_0201.png" width="600" height="174"
alt="A fragment of the Prometheus download page." />
<h6><span class="label">Figure 2-1. </span>Part of the Prometheus
download page; the Linux/amd64 version is in the middle</h6>
</div>
</figure>

Here we are using Prometheus 2.37.0, so
*prometheus-2.37.0.linux-amd64.tar.gz* is the
filename.[]{#ch02.xhtml#idm45207113424416
primary="versions of Prometheus" data-type="indexterm"}

```{=html}
<aside class="less_space" data-type="sidebar" epub:type="sidebar">
```
::: {#ch02.xhtml#idm45207113423584 .sidebar}
# Long-Term Support

Minor releases of Prometheus are scheduled every six weeks. Upgrading at
such a cadence can be challenging, therefore some versions are defined
as *Long Term Support (LTS) releases*.[]{#ch02.xhtml#idm45207113421296
primary="Long Term Support (LTS) releases"
data-type="indexterm"}[]{#ch02.xhtml#idm45207113420528
primary="LTS (Long Term Support) releases" data-type="indexterm"} LTS
releases are supported for a longer period of time than regular
releases: instead of six weeks, LTS releases are updated with bug fixes
and security fixes for one year. You can find the complete schedule at
the [Prometheus website](https://oreil.ly/ZxU-S).
:::

```{=html}
</aside>
```
::: {data-type="tip"}
###### Tip

Prometheus upgrades are intended to be safe between minor versions, such
as from 2.0.0 to 2.0.1, 2.1.0, or 2.3.1. Even so, as with all software
it is wise to read through the
changelog.[]{#ch02.xhtml#idm45207113417680
primary="stability guarantees, Prometheus versions"
data-type="indexterm"}

Any 2.x.x version of Prometheus should suffice for this chapter.
:::

Extract the tarball on the command line and change into its
directory:^[1](#ch02.xhtml#idm45207119482448){#ch02.xhtml#idm45207119482448-marker
data-type="noteref"}^

``` {data-type="programlisting"}
hostname $ tar -xzf prometheus-*.linux-amd64.tar.gz
hostname $ cd prometheus-*.linux-amd64/
```

Now change the file called *prometheus.yml* to
contain[]{#ch02.xhtml#idm45207119479008 primary="running Prometheus"
secondary="configuration" data-type="indexterm"} the following text:

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
       - localhost:9090
```

::: {data-type="caution"}
###### Caution

The Prometheus ecosystem uses YAML (YAML Ain't Markup Language) for its
configuration files, as it is both approachable to humans and can be
processed by tools.[]{#ch02.xhtml#idm45207113467744 primary="YAML"
data-type="indexterm"}[]{#ch02.xhtml#idm45207113467136
primary="configuration files, use of YAML" data-type="indexterm"} The
format is sensitive to whitespace though, so make sure to copy examples
exactly and use spaces rather than
tabs.^[2](#ch02.xhtml#idm45207119760336){#ch02.xhtml#idm45207119760336-marker
data-type="noteref"}^
:::

By default Prometheus runs on TCP port 9090, so this configuration
instructs to scrape itself every 10 seconds. You can now run the
Prometheus binary with [**`./prometheus`**]{.keep-together}:

``` {data-type="programlisting"}
hostname $ ./prometheus
level=info ... msg="No time or size retention was set so using the default
    time retention" duration=15d
level=info ... msg="Starting Prometheus" version="(version=2.37.0, branch=HEAD,
    revision=b41e0750abf5cc18d8233161560731de05199330)"
level=info ... build_context="(go=go1.18.4, user=root@0ebb6827e27f,
    date=20220714-15:13:18)"
level=info ... host_details="(Linux 5.18.12 #1-NixOS SMP PREEMPT..."
level=info ... fd_limits="(soft=1024, hard=1048576)"
level=info ... msg="Start listening for connections" address=0.0.0.0:9090
level=info ... msg="Starting TSDB ..."
level=info ... msg="TSDB started"
level=info ... component=web msg="TLS is disabled." http2=false
level=info ... msg="Loading configuration file" filename=prometheus.yml
level=info ... msg="Server is ready to receive web requests."
```

As you can see, Prometheus logs various useful information at startup,
including its exact version and details of the machine it is running
on.[]{#ch02.xhtml#idm45207119756768 primary="running Prometheus"
secondary="expression browser"
data-type="indexterm"}[]{#ch02.xhtml#idm45207119755792
primary="expression browser" data-type="indexterm"} Now you can access
the Prometheus UI in your browser at *http://localhost:9090/*, which
will look like
[Figure 2-2](#ch02.xhtml#empty_expression_browser){data-type="xref"}.

<figure>
<div id="ch02.xhtml#empty_expression_browser" class="figure">
<img src="assets/pur2_0202.png" width="600" height="292"
alt="An empty Prometheus Expression Browser" />
<h6><span class="label">Figure 2-2. </span>The Prometheus expression
browser</h6>
</div>
</figure>

This is the *expression browser* from which you can run PromQL queries.
[]{#ch02.xhtml#idm45207116159712 primary="Targets page"
data-type="indexterm"}There are also several other pages in the UI to
help you understand what Prometheus is doing, such as the Targets page
under the Status tab, which looks like
[Figure 2-3](#ch02.xhtml#targets_page_single_prometheus){data-type="xref"}.

<figure>
<div id="ch02.xhtml#targets_page_single_prometheus" class="figure">
<img src="assets/pur2_0203.png" width="600" height="288"
alt="A Prometheus Targets page showing a single up Prometheus." />
<h6><span class="label">Figure 2-3. </span>The target status page</h6>
</div>
</figure>

On this page there is only a single Prometheus server in the `UP` state,
meaning that the last scrape was successful. If there had been a problem
with the last scrape, there would be a message in the Error field.

Another page you should look at is the */metrics* page of Prometheus
itself, as somewhat unsurprisingly Prometheus is itself instrumented
with Prometheus metrics.[]{#ch02.xhtml#idm45207116154112
primary="metrics" secondary="/metrics page of Prometheus"
secondary-sortas="metrics" data-type="indexterm"} These are metrics
available on *http://localhost:9090/metrics* and are human readable, as
you can see in
[Figure 2-4](#ch02.xhtml#prometheus_metrics_endpoint){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_metrics_endpoint" class="figure">
<img src="assets/pur2_0204.png" width="600" height="293"
alt="The first part of Prometheus&#39;s /metrics page" />
<h6><span class="label">Figure 2-4. </span>The first part of
Prometheus’s <em>/metrics</em> page</h6>
</div>
</figure>

Note that there are not just metrics from the Prometheus code itself,
but also about the Go runtime and the
process.[]{#ch02.xhtml#idm45207118615776 primary="running Prometheus"
startref="ix_run" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Using the Expression Browser" data-type="sect1"}
::: {#ch02.xhtml#idm45207113430192 .sect1}
# Using the Expression Browser

The expression browser is useful for running ad hoc queries, developing
PromQL expressions, and debugging both PromQL and the data inside
Prometheus.[]{#ch02.xhtml#ix_expbr primary="expression browser"
secondary="using" data-type="indexterm"}

To start, make sure you are in the Console view, enter the expression
**`up`**, and click Execute.[]{#ch02.xhtml#idm45207118610784
primary="up" data-type="indexterm"}

As [Figure 2-5](#ch02.xhtml#prometheus_eb_up){data-type="xref"} shows,
there is a single result with the value 1 and the name
`up{instance="localhost:9090",job="prometheus"}`. `up` is a special
metric added by Prometheus when it performs a scrape; 1 indicates that
the scrape was successful.[]{#ch02.xhtml#idm45207118607552
primary="scraping"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118606848
primary="instance labels" data-type="indexterm"} The `instance` is a
label, indicating the target that was scraped. In this case it indicates
it is the Prometheus server itself.

<figure>
<div id="ch02.xhtml#prometheus_eb_up" class="figure">
<img src="assets/pur2_0205.png" width="600" height="279"
alt="The expression browser console view with a single result." />
<h6><span class="label">Figure 2-5. </span>The result of <code>up</code>
in the expression browser</h6>
</div>
</figure>

The `job` label here comes from the `job_name` in the *prometheus.yml*.
Prometheus does not magically know that it is scraping a Prometheus
server and thus that it should use a `job` label with the value
`prometheus`. []{#ch02.xhtml#idm45207119394848 primary="job labels"
data-type="indexterm"}Rather, this is a convention that requires
configuration by the user. The `job` label indicates the type of
application.[]{#ch02.xhtml#idm45207119393600
primary="process_resident_memory_bytes" data-type="indexterm"}

Next, you should evaluate **`process_resident_memory_bytes`**, as shown
in
[Figure 2-6](#ch02.xhtml#prometheus_eb_rss_console){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_rss_console" class="figure">
<img src="assets/pur2_0206.png" width="600" height="279"
alt="The expression browser console view with a single result for process_resident_memory_bytes." />
<h6><span class="label">Figure 2-6. </span>The result of
<code>process_resident_memory_bytes</code> in the expression
browser</h6>
</div>
</figure>

Our Prometheus is using about 73 MB of memory.
[]{#ch02.xhtml#idm45207119387920 primary="memory"
secondary="results of process_resident_memory_bytes"
data-type="indexterm"}You may wonder why this metric is exposed using
bytes rather than megabytes or gigabytes, which may be more readable.
The answer is that what is more readable depends a lot on context, and
even the same binary in different environments may have values that
differ by many orders of
magnitude.^[3](#ch02.xhtml#idm45207119386848){#ch02.xhtml#idm45207119386848-marker
data-type="noteref"}^ An internal RPC may take microseconds, while
polling a long-running process might take hours or even days.
[]{#ch02.xhtml#idm45207119385648 primary="base units"
data-type="indexterm"}Thus the convention in Prometheus is to use base
units such as bytes and seconds, and leave pretty printing it to
frontend tools like
Grafana.^[4](#ch02.xhtml#idm45207119384688){#ch02.xhtml#idm45207119384688-marker
data-type="noteref"}^

Knowing the current memory usage is great and all, but what would be
really nice would be to see how it has changed over time. To do so,
click Graph to switch to the graph view, as shown in
[Figure 2-7](#ch02.xhtml#prometheus_eb_rss_graph){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_rss_graph" class="figure">
<img src="assets/pur2_0207.png" width="600" height="475"
alt="An expression browser graph with a single plot for process_resident_memory_bytes." />
<h6><span class="label">Figure 2-7. </span>A graph of
<code>process_resident_memory_bytes</code> in the expression
browser</h6>
</div>
</figure>

Metrics like `process_resident_memory_bytes` are called *gauges*.
[]{#ch02.xhtml#idm45207113338528 primary="gauges"
data-type="indexterm"}[]{#ch02.xhtml#idm45207113337792
primary="counters"
data-type="indexterm"}[]{#ch02.xhtml#idm45207113337120
primary="process_resident_memory_bytes"
secondary="graph of in expression browser)" data-type="indexterm"}A
gauge's current absolute value is what is important to you. There is a
second core type of metric called the *counter*. Counters track how many
events have happened, or the total size of all the
events.[]{#ch02.xhtml#idm45207113335456 primary="events"
secondary="counters tracking" data-type="indexterm"} Let's look at a
counter by graphing
**`prometheus_tsdb_head_​`[`samples_appended_total`]{.keep-together}**,
the number of samples Prometheus has ingested, which will look like
[Figure 2-8](#ch02.xhtml#prometheus_eb_samples_appended){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_samples_appended" class="figure">
<img src="assets/pur2_0208.png" width="600" height="475"
alt="An expression browser graph with a single plot for prometheus_tsdb_head_samples_appended_total going up and to the right." />
<h6><span class="label">Figure 2-8. </span>A graph of
<code>prometheus_tsdb_head_samples_appended_total</code> in the
expression browser</h6>
</div>
</figure>

Counters are always increasing. This creates nice up-and-to-the-right
graphs, but the values of counters are not much use on their own. What
you really want to know is how fast the counter is increasing, which is
where the `rate` function comes in.[]{#ch02.xhtml#idm45207113329056
primary="rate function" data-type="indexterm"} The `rate` function
calculates how fast a counter is increasing per second. Adjust your
expression to
**`rate(prometheus_tsdb_head_samples_appended_total[1m])`**, which will
calculate how many samples Prometheus is ingesting per second averaged
over one minute and produce a result such as that shown in
[Figure 2-9](#ch02.xhtml#prometheus_eb_samples_appended_rate){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_samples_appended_rate" class="figure">
<img src="assets/pur2_0209.png" width="600" height="475"
alt="An expression browser graph with a single mostly horizontal plot." />
<h6><span class="label">Figure 2-9. </span>A graph of
<code>rate(prometheus_tsdb_head_samples_appended_total[1m])</code> in
the expression browser</h6>
</div>
</figure>

You can see now that Prometheus is ingesting 28 or so samples per second
on average. The `rate` function automatically handles
[]{#ch02.xhtml#idm45207118082192 primary="expression browser"
secondary="using" startref="ix_expbr" data-type="indexterm"}counters
resetting due to processes restarting and samples not being exactly
aligned.^[5](#ch02.xhtml#idm45207118080816){#ch02.xhtml#idm45207118080816-marker
data-type="noteref"}^
:::
:::

::: {.section pdf-bookmark="Running the Node Exporter" data-type="sect1"}
::: {#ch02.xhtml#idm45207118614544 .sect1}
# Running the Node Exporter

The []{#ch02.xhtml#ix_NdeExp primary="Node Exporter"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118076192 primary="metrics"
secondary="exposed by Node Exporter"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118075232 primary="Unix"
secondary="Note Exporter exposing metrics on" data-type="indexterm"}Node
Exporter exposes kernel- and machine-level metrics on Unix systems, such
as
Linux.^[6](#ch02.xhtml#idm45207118074144){#ch02.xhtml#idm45207118074144-marker
data-type="noteref"}^ It provides all the standard metrics such as CPU,
memory, disk space, disk I/O, and network
bandwidth.[]{#ch02.xhtml#idm45207118072544 primary="I/O, disk"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118071872
primary="disk I/O" data-type="indexterm"} In addition it provides myriad
additional metrics exposed by the kernel, from load average to
motherboard temperature.[]{#ch02.xhtml#idm45207118070944
primary="exporters" secondary="Node Exporter" see="Node Exporter"
data-type="indexterm"}

What the Node Exporter does not expose is metrics about individual
processes, nor proxy metrics from other exporters or applications. In
the Prometheus architecture you monitor applications and services
directly, rather than entwining them into the machine
metrics.[]{#ch02.xhtml#idm45207118069472 primary="Node Exporter"
secondary="downloading and installing" data-type="indexterm"}

You can download a prebuilt version of the Node Exporter from the
[Prometheus download page](https://oreil.ly/Bc4js). Go to that page and
download the latest version of Node Exporter for the Linux OS with Arch
amd64.

Again, the tarball will need to be extracted, but no configuration file
is required, so it can be run directly:

``` {data-type="programlisting"}
hostname $ tar -xzf node_exporter-*.linux-amd64.tar.gz
hostname $ cd node_exporter-*.linux-amd64/
hostname $ ./node_exporter
level=info ... msg="Starting node_exporter" version="(version=1.3.1,
    branch=HEAD, revision=a2321e7b940ddcff26873612bccdf7cd4c42b6b6)"
level=info ... msg="Build context" build_context="(go=go1.17.3,
    user=root@243aafa5525c, date=20211205-11:09:49)"
level=info ... msg="Enabled collectors"
level=info ... collector=arp
level=info ... collector=bcache
level=info ... collector=bonding
...
various other collectors
...
level=info ... msg="Listening on" address=:9100
level=info ... msg="TLS is disabled." http2=false
```

You can now access the Node Exporter in your browser at
*http://localhost:9100/* and visit its */metrics* endpoint.

To get Prometheus to monitor the Node Exporter, you need to update the
*prometheus.yml* by adding an additional
scrape[]{#ch02.xhtml#idm45207118062032 primary="Node Exporter"
secondary="configuring Prometheus to monitor" data-type="indexterm"}
config:

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
       - localhost:9090
 - job_name: node 
   static_configs:
    - targets:
       - localhost:9100
```

[![1](assets/1.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO1-1){#ch02.xhtml#callout_getting_started_with_prometheus_CO1-1 .co}

:   The Node Exporter scrape job.

Restart Prometheus to pick up the new configuration by using Ctrl-C to
shut it down and then start it
again.^[7](#ch02.xhtml#idm45207118848240){#ch02.xhtml#idm45207118848240-marker
data-type="noteref"}^ []{#ch02.xhtml#idm45207118847024
primary="Targets page" secondary="showing Prometheus and Node Exporter"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118846048 primary="up"
data-type="indexterm"}If you look at the Targets page, you should now
see two targets, both in the `UP` state, as shown in
[Figure 2-10](#ch02.xhtml#prometheus_targets_ne_prom){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_targets_ne_prom" class="figure">
<img src="assets/pur2_0210.png" width="600" height="357"
alt="A Prometheus Targets page showing an up Prometheus and Node Exporter." />
<h6><span class="label">Figure 2-10. </span>The target status page with
Node Exporter</h6>
</div>
</figure>

If you now evaluate **`up`** in the Console view of the expression
browser, you will see two entries, as shown in
[Figure 2-11](#ch02.xhtml#prometheus_eb_up_ne_prom){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_up_ne_prom" class="figure">
<img src="assets/pur2_0211.png" width="600" height="224"
alt="The expression browser console view with two results." />
<h6><span class="label">Figure 2-11. </span>There are now two results
for <code>up</code></h6>
</div>
</figure>

As you add more jobs and scrape configs, it is rare that you will want
to look at the same metric from different jobs at the same
time.[]{#ch02.xhtml#idm45207118810432 primary="memory"
secondary="usage by Prometheus and Node Exporter" data-type="indexterm"}
The memory usage of a Prometheus and a Node Exporter are very different,
for example, and extraneous data makes debugging and investigation
harder. You can graph the memory usage of just the Node Exporters with
**`process_resident_memory_bytes{job="node"}`**. The `job="node"` is
called a *label matcher*, and it restricts the metrics that are
returned, as you can see in
[Figure 2-12](#ch02.xhtml#prometheus_eb_rss_ne){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_rss_ne" class="figure">
<img src="assets/pur2_0212.png" width="600" height="463"
alt="An expression browser graph with a single plot." />
<h6><span class="label">Figure 2-12. </span>A graph of the resident
memory of just the Node Exporter</h6>
</div>
</figure>

The `process_resident_memory_bytes` here is the memory used by the Node
Exporter process itself (as is hinted by the `process` prefix) and not
the machine as a whole.[]{#ch02.xhtml#idm45207118803920
primary="process_resident_memory_bytes" data-type="indexterm"} Knowing
the resource usage of the Node Exporter is handy and all, but it is not
why you run it.[]{#ch02.xhtml#idm45207118802960 primary="rate function"
data-type="indexterm"}

As a final example, evaluate
**`rate(node_network_receive_bytes_total[1m])`** in Graph view to
produce a graph like the one shown in
[Figure 2-13](#ch02.xhtml#prometheus_eb_network_in){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_network_in" class="figure">
<img src="assets/pur2_0213.png" width="600" height="557"
alt="An expression browser graph with several plots." />
<h6><span class="label">Figure 2-13. </span>A graph of the network
traffic received on several interfaces</h6>
</div>
</figure>

`node_network_receive_bytes_total` is a counter for how many bytes have
been received by network interfaces. The Node Exporter automatically
picked up all the network interfaces, and they can be worked with as a
group in PromQL. This is useful for alerting, as labels avoid the need
to exhaustively list every single thing you wish to alert
on.[]{#ch02.xhtml#idm45207118797472 primary="Node Exporter"
startref="ix_NdeExp" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Alerting" data-type="sect1"}
::: {#ch02.xhtml#intro_alerting .sect1}
# Alerting

There are two parts to alerting. First, adding alerting rules to
Prometheus, defining the logic of what constitutes an
alert.[]{#ch02.xhtml#ix_alrt2 primary="alerting" data-type="indexterm"}
Second, the Alertmanager converts firing alerts into notifications, such
as emails, pages, and chat messages.

Let's start off by creating a condition that you might want to alert on.
Stop the Node Exporter with Ctrl-C.[]{#ch02.xhtml#idm45207118792256
primary="Targets page" secondary="scrape errors on"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118791280
primary="alerting rules"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118790608
primary="DOWN state, alerting on"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118789936
primary="connection refused error"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118789200
primary="scrape errors" data-type="indexterm"} After the next scrape,
the Targets page will show the Node Exporter in the `DOWN` state, as
shown in
[Figure 2-14](#ch02.xhtml#prometheus_targets_ne_down){data-type="xref"},
with the error *connection refused*, as nothing is listening on the TCP
port and the[]{#ch02.xhtml#idm45207118786720 primary="HTTP requests"
secondary="rejected" data-type="indexterm"} HTTP request is being
rejected.^[8](#ch02.xhtml#idm45207118785616){#ch02.xhtml#idm45207118785616-marker
data-type="noteref"}^

::: {data-type="tip"}
###### Tip

Prometheus does not include failed scrapes in its application logs, as a
failed scrape is an expected occurrence that does not indicate any
problems in Prometheus itself. []{#ch02.xhtml#idm45207118782848
primary="debug logs" secondary="scrape errors on"
data-type="indexterm"}Aside from the Targets page, scrape errors are
also available in the debug logs of Prometheus, which you can enable by
passing the `--log.level debug` command-line flag.
:::

<figure>
<div id="ch02.xhtml#prometheus_targets_ne_down" class="figure">
<img src="assets/pur2_0214.png" width="600" height="316"
alt="A Prometheus Targets page showing an up Prometheus and a down Node Exporter." />
<h6><span class="label">Figure 2-14. </span>The target status page
showing the Node Exporter as down</h6>
</div>
</figure>

Manually looking at the Targets page for down instances is not a good
use of your time.[]{#ch02.xhtml#idm45207118778656 primary="up"
data-type="indexterm"} Luckily, the `up` metric has your back, and when
evaluating **`up`** in the Console view of the expression browser, you
will see that it now has a value of 0 for the Node Exporter, as shown in
[Figure 2-15](#ch02.xhtml#prometheus_eb_up_ne_down){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_eb_up_ne_down" class="figure">
<img src="assets/pur2_0215.png" width="600" height="241"
alt="The expression browser console view with two results, the Node Exporter has a value of 0 for up." />
<h6><span class="label">Figure 2-15. </span><code>up</code> is now 0 for
the Node Exporter</h6>
</div>
</figure>

For alerting rules, you need a PromQL expression that returns only the
results that you wish to alert on.[]{#ch02.xhtml#idm45207118772592
primary="PromQL" secondary="alerting rule, expression for"
data-type="indexterm"}[]{#ch02.xhtml#idm45207118771712
primary="== operator" data-type="indexterm"} In this case, that is easy
to do using the `==` operator. `==` will
filter^[9](#ch02.xhtml#idm45207118770080){#ch02.xhtml#idm45207118770080-marker
data-type="noteref"}^ away any time series whose values don't match. If
you evaluate **`up == 0`** in the expression browser, only the down
instance is returned, as
[Figure 2-16](#ch02.xhtml#prometheus_eb_up_ne_only_down){data-type="xref"}
shows.

<figure>
<div id="ch02.xhtml#prometheus_eb_up_ne_only_down" class="figure">
<img src="assets/pur2_0216.png" width="600" height="228"
alt="The expression browser console view with one result, the Node Exporter with a value of 0 for up." />
<h6><span class="label">Figure 2-16. </span>Only <code>up</code> metrics
with the value 0 are returned</h6>
</div>
</figure>

Next, you need to add this expression in an alerting rule in Prometheus.
We are also going to jump ahead a little and have you tell Prometheus
which Alertmanager it will be talking
to.[]{#ch02.xhtml#idm45207118763536 primary="Alertmanager"
secondary="telling Prometheus which one" data-type="indexterm"} You will
need to expand your *prometheus.yml* to have the content from
[Example 2-1](#ch02.xhtml#intro_prometheus_rules_yml){data-type="xref"}.

::: {#ch02.xhtml#intro_prometheus_rules_yml data-type="example"}
##### [Example 2-1. ]{.label}*prometheus.yml* scraping two targets, loading a rule file, and talking to an Alertmanager

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
  evaluation_interval: 10s
rule_files: 
 - rules.yml
alerting: 
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093
scrape_configs: 
 - job_name: prometheus
   static_configs:
    - targets:
       - localhost:9090
 - job_name: node
   static_configs:
    - targets:
       - localhost:9100
```
:::

[![1](assets/1.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO2-1){#ch02.xhtml#callout_getting_started_with_prometheus_CO2-1 .co}

:   The rule files configuration.

[![2](assets/2.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO2-2){#ch02.xhtml#callout_getting_started_with_prometheus_CO2-2 .co}

:   The alerting configuration.

[![3](assets/3.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO2-3){#ch02.xhtml#callout_getting_started_with_prometheus_CO2-3 .co}

:   The scrape jobs.

Next, create a new *rules.yml* file with the contents from
[Example 2-2](#ch02.xhtml#intro_rules_yml){data-type="xref"}, and then
restart Prometheus.

::: {#ch02.xhtml#intro_rules_yml data-type="example"}
##### [Example 2-2. ]{.label}*rules.yml* with a single alerting rule

``` {code-language="yaml" data-type="programlisting"}
groups:
 - name: example
   rules:
   - alert: InstanceDown
     expr: up == 0
     for: 1m
```
:::

The `InstanceDown` alert will be evaluated every 10 seconds in
accordance with the `evaluation_interval`.
[]{#ch02.xhtml#idm45207113096528 primary="InstanceDown alert"
data-type="indexterm"}If a series is continuously returned for at least
a
minute^[10](#ch02.xhtml#idm45207113095760){#ch02.xhtml#idm45207113095760-marker
data-type="noteref"}^ (the `for`), then the alert will be considered to
be firing. Until the required minute is up, the alert will be in a
*pending* state.[]{#ch02.xhtml#idm45207113093696 primary="alerts"
secondary="firing alert on Alerts page" data-type="indexterm"} On the
Alerts page you can click this alert and see more detail, including its
labels, as shown in
[Figure 2-17](#ch02.xhtml#prometheus_alerts_instancedown_firing){data-type="xref"}.

<figure>
<div id="ch02.xhtml#prometheus_alerts_instancedown_firing"
class="figure">
<img src="assets/pur2_0217.png" width="600" height="268"
alt="A single firing alert on the Alerts page" />
<h6><span class="label">Figure 2-17. </span>A firing alert on the Alerts
page</h6>
</div>
</figure>

Now that you have a firing alert, you need an Alertmanager to do
something with it.[]{#ch02.xhtml#idm45207118530144
primary="Alertmanager" secondary="downloading and installing"
data-type="indexterm"} Download the [latest
version](https://oreil.ly/Bc4js) of the Alertmanager for the Linux OS
with Arch amd64. Untar the Alertmanager and **`cd`** into its directory:

``` {data-type="programlisting"}
hostname $ tar -xzf alertmanager-*.linux-amd64.tar.gz
hostname $ cd alertmanager-*.linux-amd64/
```

You now need a configuration for the Alertmanager.
[]{#ch02.xhtml#idm45207118525616 primary="Alertmanager"
secondary="configuring" data-type="indexterm"}There are a variety of
ways that the Alertmanager can notify you, but most of the ones that
work out of the box use commercial providers and have setup instructions
that tend to change over time.[]{#ch02.xhtml#idm45207118520144
primary="SMPT" data-type="indexterm"}[]{#ch02.xhtml#idm45207118519472
primary="email alerts" data-type="indexterm"} Thus we are going to
presume that you have an open SMTP smarthost
available.^[11](#ch02.xhtml#idm45207118518672){#ch02.xhtml#idm45207118518672-marker
data-type="noteref"}^ You should base your *alertmanager.yml* on
[Example 2-3](#ch02.xhtml#intro_alertmanager_yml){data-type="xref"},
adjusting `smtp_smarthost`, `smtp_from`, and `to` to match your setup
and email address.

::: {#ch02.xhtml#intro_alertmanager_yml .pagebreak-before data-type="example"}
##### [Example 2-3. ]{.label}*alertmanager.yml* sending all alerts to email

``` {code-language="yaml" data-type="programlisting"}
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'yourprometheus@example.org' 
route:
  receiver: example-email
  group_by: [alertname]
receivers:
 - name: example-email
   email_configs:
    - to: 'youraddress@example.org' 
```
:::

[![1](assets/1.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO3-1){#ch02.xhtml#callout_getting_started_with_prometheus_CO3-1 .co}

:   The email address that will be used as the *From* field.

[![2](assets/2.png){height="12" width="12"}](#ch02.xhtml#co_getting_started_with_prometheus_CO3-2){#ch02.xhtml#callout_getting_started_with_prometheus_CO3-2 .co}

:   The email address the emails will be sent to.

You can now []{#ch02.xhtml#idm45207117484528 primary="Alertmanager"
secondary="starting" data-type="indexterm"}start the Alertmanager with
**`./alertmanager`**:

``` {data-type="programlisting"}
hostname $ ./alertmanager
level=info ... msg="Starting Alertmanager" version="(version=0.24.0,
    branch=HEAD, revision=f484b17fa3c583ed1b2c8bbcec20ba1db2aa5f11)"
level=info ... build_context="(go=go1.17.8, user=root@265f14f5c6fc,
    date=20220325-09:31:33)"
level=info ... component=cluster msg="setting advertise address
    explicitly" addr=192.168.10.52 port=9094
level=info ... component=cluster msg="Waiting for gossip to settle..."
    interval=2s
level=info ... component=configuration msg="Loading configuration file"
    file=alertmanager.yml
level=info ... component=configuration msg="Completed loading of
    configuration file" file=alertmanager.yml
level=info ... msg=Listening address=:9093
level=info ... msg="TLS is disabled." http2=false
level=info component=cluster ... msg="gossip not settled" polls=0 before=0
    now=1 elapsed=2.00004715s
level=info component=cluster ... msg="gossip settled; proceeding"
    elapsed=10.001771352s
    polls=0 before=0 now=1 elapsed=2.00011639s
```

You can now access the Alertmanager in your browser at
*http://localhost:9093/* where you will see your firing alert, which
should look similar to
[Figure 2-18](#ch02.xhtml#alertmanager_single_instancedown){data-type="xref"}.

<figure>
<div id="ch02.xhtml#alertmanager_single_instancedown" class="figure">
<img src="assets/pur2_0218.png" width="600" height="382"
alt="A single firing alert on the Alertmanager status page" />
<h6><span class="label">Figure 2-18. </span>An InstanceDown alert in the
Alertmanager</h6>
</div>
</figure>

If everything is set up and working correctly, after
[]{#ch02.xhtml#idm45207117530880 primary="InstanceDown alert"
secondary="viewing in Alertmanager" data-type="indexterm"}a minute or
two you should receive a notification from the Alertmanager in your
email inbox that looks like
[Figure 2-19](#ch02.xhtml#alertmanager_instancedown_email){data-type="xref"}.

<figure>
<div id="ch02.xhtml#alertmanager_instancedown_email" class="figure">
<img src="assets/pur2_0219.png" width="600" height="435"
alt="An email sent by the Alertmanager." />
<h6><span class="label">Figure 2-19. </span>An email notification for an
InstanceDown alert</h6>
</div>
</figure>

This basic setup has given you a small taste of what Prometheus can do.
You could add more targets to the *prometheus.yml*, and your alert would
automatically work for them too.

In the next chapter we are going to focus on a specific aspect of using
Prometheus---adding instrumentation to your own
applications.[]{#ch02.xhtml#idm45207116031312 primary="alerting"
startref="ix_alrt2" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch02.xhtml#idm45207119482448-marker)^ This uses a glob for the
version in case you are using a different version than we are. The star
will match any text.

^[2](#ch02.xhtml#idm45207119760336-marker)^ You may wonder why
Prometheus doesn't use JSON. JSON has its own issues, such as being
picky about commas, and unlike YAML, does not support comments. As JSON
is a subset of YAML, you can use JSON instead if you really want to.

^[3](#ch02.xhtml#idm45207119386848-marker)^ You can get the number in MB
by running a query like `process_resident_memory_bytes / (1024*1024)`.

^[4](#ch02.xhtml#idm45207119384688-marker)^ This is the same logic
behind why dates and times are generally best stored in UTC, and time
zone transformations only applied just before they are shown to a human.

^[5](#ch02.xhtml#idm45207118080816-marker)^ This can lead to rates on
integers returning noninteger results, but the results are correct on
average. For more information, see
["rate"](#ch16.xhtml#rate){data-type="xref"}.

^[6](#ch02.xhtml#idm45207118074144-marker)^ Windows users should use the
[Windows Exporter](https://oreil.ly/dB6ZZ) rather than the Node
Exporter.

^[7](#ch02.xhtml#idm45207118848240-marker)^ It is possible to get
Prometheus to reload the configuration file without restarting by using
a `SIGHUP`.

^[8](#ch02.xhtml#idm45207118785616-marker)^ Another common error is
*context deadline exceeded*. []{#ch02.xhtml#idm45207118784576
primary="context deadline exceeded error" data-type="indexterm"}This
indicates a timeout, usually due either to the other end being too slow
or the network dropping packets.

^[9](#ch02.xhtml#idm45207118770080-marker)^ There is also a `bool` mode
that does not filter, covered in the section ["bool
modifier"](#ch15.xhtml#bool){data-type="xref"}.

^[10](#ch02.xhtml#idm45207113095760-marker)^ Usually a `for` of at least
5 minutes is recommended to reduce noise and mitigate various races
inherent in monitoring. We are only using a minute here, so you don't
have to wait too long when trying this out.

^[11](#ch02.xhtml#idm45207118518672-marker)^ Given how email security
has evolved over the past decade, this is not a good assumption, but
your ISP will probably have one.
:::
:::
:::

[]{#part02.xhtml}

::: {#part02.xhtml#sbo-rt-content}
::: {#part02.xhtml#part2 .part pdf-bookmark="Part II. Application Monitoring" data-type="part"}
# [Part II. ]{.label}Application Monitoring

You will realize the full benefits of Prometheus when you have easy
access to the metrics you added to your own applications. This section
covers adding and using this instrumentation.

In [Chapter 3](#ch03.xhtml#instrumentation_chapter){data-type="xref"}
you will learn how to add basic instrumentation, and what is beneficial
instrumentation to have.

In [Chapter 4](#ch04.xhtml#exposition_chapter){data-type="xref"} we
cover making the metrics from your application available to
[Prometheus]{.keep-together}.

In [Chapter 5](#ch05.xhtml#labels_chapter){data-type="xref"} you will
learn about one of the most powerful features of Prometheus and how to
use it in instrumentation.

After you have your application metrics in Prometheus,
[Chapter 6](#ch06.xhtml#grafana_chapter){data-type="xref"} will show you
how you can create dashboards that group related graphs together.
:::
:::

[]{#ch03.xhtml}

::: {#ch03.xhtml#sbo-rt-content}
::: {#ch03.xhtml#instrumentation_chapter .chapter}
# [Chapter 3. ]{.label}Instrumentation

The largest payoffs you will get from Prometheus are through
instrumenting your own applications using *direct instrumentation* and a
*client library*. []{#ch03.xhtml#ix_instr primary="instrumentation"
data-type="indexterm"}Client libraries are available in a variety of
languages, with official client libraries in Go, Python, Java, Rust, and
Ruby.[]{#ch03.xhtml#idm45207116018752 primary="client libraries"
data-type="indexterm"}

We use Python 3 here as an example, but the same general principles
apply to other languages and runtimes, although the syntax and utility
methods will vary.[]{#ch03.xhtml#idm45207116017696 primary="Python"
secondary="client libraries in Python 3" data-type="indexterm"}

Most modern OSes come with Python 3. In the unlikely event that you
don't already have it, download and install [Python
3](https://oreil.ly/6sAX9).

You will also need to install the latest Python client library. You can
do this with **`pip install prometheus_client`**. You can find the
instrumentation examples [on GitHub](https://oreil.ly/-IbFJ).

::: {.section pdf-bookmark="A Simple Program" data-type="sect1"}
::: {#ch03.xhtml#idm45207116013776 .sect1}
# A Simple Program

To start things off, we have written a simple HTTP server shown in
[Example 3-1](#ch03.xhtml#simple_program){data-type="xref"}.
[]{#ch03.xhtml#ix_instrprg primary="instrumentation"
secondary="example Python program exposing Prometheus metrics"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116009904 primary="metrics"
secondary="for simple HTTP server in Python (example)"
secondary-sortas="simple"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116008656
primary="HTTP server in Python (example)" data-type="indexterm"}If you
run it with Python 3 and then visit *http://localhost:8001/* in your
browser, you will get a Hello World response.

::: {#ch03.xhtml#simple_program .less_space data-type="example"}
##### [Example 3-1. ]{.label}A simple Hello World program that also exposes Prometheus metrics

``` {code-language="python" data-type="programlisting"}
import http.server
from prometheus_client import start_http_server

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")

if __name__ == "__main__":
    start_http_server(8000)
    server = http.server.HTTPServer(('localhost', 8001), MyHandler)
    server.serve_forever()
```
:::

The `start_http_server(8000)` starts up an HTTP server on port 8000 to
serve metrics to Prometheus. You can view these metrics at
*http://localhost:8000/*, which will look like
[Figure 3-1](#ch03.xhtml#simple_program_bare_metrics){data-type="xref"}.
Which metrics are returned out of the box varies based on the platform,
with Linux platforms tending to have the most metrics.

<figure>
<div id="ch03.xhtml#simple_program_bare_metrics" class="figure">
<img src="assets/pur2_0301.png" width="600" height="212"
alt="A basic /metrics for Python." />
<h6><span class="label">Figure 3-1. </span>The <em>/metrics</em> page
when the simple program runs on Linux with CPython</h6>
</div>
</figure>

Although you can manually review a */metrics* page, getting the metrics
into Prometheus is what you really want.[]{#ch03.xhtml#idm45207113819712
primary="python_info expression" data-type="indexterm"} To do this, set
up Prometheus with the configuration in
[Example 3-2](#ch03.xhtml#prometheus_yml_8000){data-type="xref"} and get
it running.

::: {#ch03.xhtml#prometheus_yml_8000 data-type="example"}
##### [Example 3-2. ]{.label}*prometheus.yml* to scrape [*http://localhost:8000/metrics*](http://localhost:8000/metrics){.bare}

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: example
   static_configs:
    - targets:
      - localhost:8000
```
:::

If you enter the PromQL expression **`python_info`** in the expression
browser at *http://localhost:9090/*, you should see something like
[Figure 3-2](#ch03.xhtml#prometheus_eb_python_info){data-type="xref"}.

<figure>
<div id="ch03.xhtml#prometheus_eb_python_info" class="figure">
<img src="assets/pur2_0302.png" width="600" height="203"
alt="Prometheus expression browser with one result for python_info." />
<h6><span class="label">Figure 3-2. </span>Evaluating the expression
<code>python_info</code> produces one result</h6>
</div>
</figure>

In the rest of this chapter we will presume that you have Prometheus
running and scraping your example application. You will use the
expression browser as you go along to work with the metrics you create.
:::
:::

::: {.section pdf-bookmark="The Counter" data-type="sect1"}
::: {#ch03.xhtml#idm45207116013120 .sect1}
# The Counter

Counters are the type of metric you will probably use most often in
instrumentation.[]{#ch03.xhtml#ix_cntr3 primary="counters"
data-type="indexterm"}[]{#ch03.xhtml#instr3cntr
primary="instrumentation" secondary="counters" data-type="indexterm"}
Counters track either the number or size of events. They are mainly used
to track how often a particular code path is executed.

Enhance [Example 3-1](#ch03.xhtml#simple_program){data-type="xref"} by
including a new metric that tracks the number of times "Hello World" has
been requested, as demonstrated in
[Example 3-3](#ch03.xhtml#simple_program_one_counter){data-type="xref"}.

::: {#ch03.xhtml#simple_program_one_counter data-type="example"}
##### [Example 3-3. ]{.label}`REQUESTS` tracks the number of Hello Worlds returned

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Counter

REQUESTS = Counter('hello_worlds_total',
        'Hello Worlds requested.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        REQUESTS.inc()
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
```
:::

There are three parts here---the import, the metric definition, and the
[instrumentation]{.keep-together}:

Import

:   Python requires that you import functions and classes from other
    modules in order to use them. Accordingly, you must import the
    `Counter` class from the `prometheus_client` library at the top of
    the file.[]{#ch03.xhtml#idm45207117187344 primary="imports"
    data-type="indexterm"}

Definition

:   Prometheus metrics must be defined before they are used. Here we
    define a counter called `hello_worlds_total`. It has a help string
    of `Hello Worlds `[`requested.`]{.keep-together}, which will appear
    on the */metrics* page to help you understand what the metric
    means.[]{#ch03.xhtml#idm45207117183552 primary="metrics"
    secondary="definitions of" data-type="indexterm"}

    Metrics are automatically []{#ch03.xhtml#idm45207117181968
    primary="metrics"
    secondary="automatic registration with client library"
    data-type="indexterm"}[]{#ch03.xhtml#idm45207117181024
    primary="client libraries" secondary="registration of metrics with"
    data-type="indexterm"}registered with the client library in the
    *default
    registry*.^[1](#ch03.xhtml#idm45207117179600){#ch03.xhtml#idm45207117179600-marker
    data-type="noteref"}^ A registry is a place where metrics are
    registered, to be exposed. []{#ch03.xhtml#idm45207117178752
    primary="registry" data-type="indexterm"}The default registry is the
    registry used by default when querying */metrics*. There are some
    cases when passing a custom registry can be useful; one of the main
    cases is when writing libraries used by other
    software.[]{#ch03.xhtml#idm45207117177568
    primary="custom registries" data-type="indexterm"}

    In the Java library, for example, an extra function call is
    required, and depending on how you use the Go library, you may also
    need to explicitly register metrics. You do not need to pull the
    metric back to the `start_http_server` call; in fact, how the code
    is instrumented is completely decoupled from the exposition. If you
    have a transient dependency that includes Prometheus
    instrumentation, it will appear on your */metrics* page
    automatically.

    Metrics must have unique names, and client libraries should report
    an error if you try to register the same metric twice. To avoid
    this, define your metrics at file level, not at class, function, or
    method level. An alternative pattern is to use custom explicit
    registries and local definitions.

Instrumentation

:   Now that you have the metric object defined, you can use it. The
    `inc` method increments the counter's value by one.

    Prometheus client libraries take care of all the nitty-gritty
    details like bookkeeping and thread safety for you, so that is all
    there is to it.

When you run the program, the new metric will appear on the */metrics*
page. It will start at zero and increase by
one^[2](#ch03.xhtml#idm45207117171568){#ch03.xhtml#idm45207117171568-marker
data-type="noteref"}^ every time you view the main URL of the
application.[]{#ch03.xhtml#idm45207117170400 primary="rate function"
data-type="indexterm"} You can view this in the expression browser and
use the PromQL expression **`rate(hello_worlds_total[1m])`** to see how
many Hello World requests are happening per second, as
[Figure 3-3](#ch03.xhtml#prometheus_eb_hello_worlds_rate){data-type="xref"}
shows.

<figure>
<div id="ch03.xhtml#prometheus_eb_hello_worlds_rate" class="figure">
<img src="assets/pur2_0303.png" width="600" height="490"
alt="A graph of Hello Worlds per seconds in the expression browser." />
<h6><span class="label">Figure 3-3. </span>A graph of Hello Worlds per
second</h6>
</div>
</figure>

With just two lines of code, you can add a counter to any library or
application. These counters are useful to track how many times errors
and unexpected situations occur. While you probably don't want to alert
every single time there is an error, knowing how errors are trending
over time is useful for debugging. But this is not restricted to errors.
Knowing which are the most popular features and code paths of your
application allows you to optimize how you allocate your development
efforts.

::: {.section pdf-bookmark="Counting Exceptions" data-type="sect2"}
::: {#ch03.xhtml#idm45207117165968 .sect2}
## Counting Exceptions

Client libraries provide not just core functionality, but also utilities
and methods for common use cases.[]{#ch03.xhtml#idm45207117164048
primary="instrumentation" secondary="counters"
tertiary="counting exceptions"
data-type="indexterm"}[]{#ch03.xhtml#idm45207117162800
primary="exceptions, counting"
data-type="indexterm"}[]{#ch03.xhtml#idm45207117162128
primary="counters" secondary="counting exceptions"
data-type="indexterm"} One of these in Python is the ability to count
exceptions. You don't have to write your own instrumentation using a
`try…​except`; instead, you can take advantage of the `count_exceptions`
context manager and decorator, as shown in
[Example 3-4](#ch03.xhtml#simple_program_exception){data-type="xref"}.

::: {#ch03.xhtml#simple_program_exception data-type="example"}
##### [Example 3-4. ]{.label}`EXCEPTIONS` counts the number of exceptions using a context manager

``` {code-language="python" data-type="programlisting"}
import random
from prometheus_client import Counter

REQUESTS = Counter('hello_worlds_total',
        'Hello Worlds requested.')
EXCEPTIONS = Counter('hello_world_exceptions_total',
        'Exceptions serving Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        REQUESTS.inc()
        with EXCEPTIONS.count_exceptions():
          if random.random() < 0.2:
            raise Exception
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
```
:::

`count_exceptions` will take care of passing the exception up by raising
it, so it does not interfere with application logic. You can see the
rate of exceptions with **`rate(hello_world_exceptions_total[1m])`**.
The number of exceptions isn't that useful without knowing how many
requests are going through. You can calculate the more useful ratio of
exceptions with:

``` {data-type="programlisting"}
  rate(hello_world_exceptions_total[1m])
/
  rate(hello_worlds_total[1m])
```

in the expression browser. This is how to generally expose ratios:
expose two counters, then rate and divide them in
PromQL.[]{#ch03.xhtml#idm45207113263920
primary="ratio, calculating for exceptions" data-type="indexterm"}

::: {.note data-type="note"}
###### Note

You may notice gaps in the exception ratio graph for periods when there
are no requests. []{#ch03.xhtml#idm45207113262224 primary="gaps"
secondary="in exception ratio graph for no requests"
secondary-sortas="exception" data-type="indexterm"}This is because you
are dividing by zero, which in floating-point math results in a *NaN*,
or Not a Number. []{#ch03.xhtml#idm45207113260496
primary="floating-point math"
secondary="dividing by zero, resulting in NaN"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113259520
primary="NaN (not a number)" data-type="indexterm"}Returning a zero
would be incorrect as the exception ratio is not zero, it is undefined.
:::

You can also use `count_exceptions` as a function decorator:

``` {code-language="python" data-type="programlisting"}
EXCEPTIONS = Counter('hello_world_exceptions_total',
        'Exceptions serving Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    @EXCEPTIONS.count_exceptions()
    def do_GET(self):
      ...
```
:::
:::

::: {.section pdf-bookmark="Counting Size" data-type="sect2"}
::: {#ch03.xhtml#idm45207117468336 .sect2}
## Counting Size

Prometheus uses 64-bit floating-point numbers for values so you are not
limited to incrementing counters by one.
[]{#ch03.xhtml#idm45207113176624 primary="floating-point numbers"
secondary="64-bit, use by Prometheus"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113161360
primary="counters" secondary="counting size"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113160416
primary="instrumentation" secondary="counters" tertiary="counting size"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113159200
primary="size, counting" data-type="indexterm"}You can in fact increment
counters by any non-negative number. This allows you to track the number
of records processed, bytes served, or sales in euros, as shown in
[Example 3-5](#ch03.xhtml#simple_program_count_size){data-type="xref"}.

::: {#ch03.xhtml#simple_program_count_size data-type="example"}
##### [Example 3-5. ]{.label}`SALES` tracks sale value in euros

``` {code-language="python" data-type="programlisting"}
import random
from prometheus_client import Counter

REQUESTS = Counter('hello_worlds_total',
        'Hello Worlds requested.')
SALES = Counter('hello_world_sales_euro_total',
        'Euros made serving Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        REQUESTS.inc()
        euros = random.random()
        SALES.inc(euros)
        self.send_response(200)
        self.end_headers()
        self.wfile.write("Hello World for {} euros.".format(euros).encode())
```
:::

You can see the rate of sales in euros per second in the expression
browser using the expression
**`rate(hello_world_sales_euro_total[1m])`**, the same as for integer
counters.

::: {data-type="caution"}
###### Caution

Attempting to increase a counter by a negative number is considered to
be a programming error on your part, and will cause an exception to be
raised.[]{#ch03.xhtml#idm45207117413312 primary="counters"
secondary="attempting to increase by negative number"
data-type="indexterm"}

It is important for PromQL that counters only ever increase, so that
`rate` and friends don't misinterpret the decrease as counters resetting
to zero when an application restarts. This also means there's no need to
persist counter state across runs of an application, or reset counters
on every scrape. This allows multiple Prometheus servers to scrape the
same application without affecting each
other.[]{#ch03.xhtml#idm45207117411632 primary="instrumentation"
secondary="counters" startref="instrcntr"
data-type="indexterm"}[]{#ch03.xhtml#idm45207117410384
primary="counters" startref="ix_cntr3" data-type="indexterm"}
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="The Gauge" data-type="sect1"}
::: {#ch03.xhtml#idm45207113841840 .sect1}
# The Gauge

Gauges are a snapshot of some current state. While for counters how fast
it is increasing is what you care about, for gauges it is the actual
value of the gauge.[]{#ch03.xhtml#ix_instrgauge
primary="instrumentation" secondary="gauges"
data-type="indexterm"}[]{#ch03.xhtml#ix_gauge primary="gauges"
data-type="indexterm"} Accordingly, the values can go both up and down.

Examples of gauges include:

-   The number of items in a queue

-   Memory []{#ch03.xhtml#idm45207117402800 primary="caches"
    secondary="memory usage of" data-type="indexterm"}usage of a cache

-   Number of active threads

-   The last time a record was processed

-   Average requests per second in the last
    minute^[3](#ch03.xhtml#idm45207117331136){#ch03.xhtml#idm45207117331136-marker
    data-type="noteref"}^

::: {.section pdf-bookmark="Using Gauges" data-type="sect2"}
::: {#ch03.xhtml#idm45207117329904 .sect2}
## Using Gauges

Gauges have three main methods you can use:
`inc`,^[4](#ch03.xhtml#idm45207117327696){#ch03.xhtml#idm45207117327696-marker
data-type="noteref"}^ `dec`, and `set`. []{#ch03.xhtml#idm45207117325552
primary="inc method"
data-type="indexterm"}[]{#ch03.xhtml#idm45207117324816
primary="dec method"
data-type="indexterm"}[]{#ch03.xhtml#idm45207117324144
primary="set method" data-type="indexterm"}Similar to the methods on
counters, `inc` and `dec` default to changing a gauge's value by one.
You can pass an argument with a different value to change by if you
want. [Example 3-6](#ch03.xhtml#simple_program_gauge){data-type="xref"}
shows how gauges can be used to track the number of calls in progress
and determine when the last one was completed.

::: {#ch03.xhtml#simple_program_gauge data-type="example"}
##### [Example 3-6. ]{.label}`INPROGRESS` and `LAST` track the number of calls in progress and when the last one was completed

``` {code-language="python" data-type="programlisting"}
import time
from prometheus_client import Gauge

INPROGRESS = Gauge('hello_worlds_inprogress',
        'Number of Hello Worlds in progress.')
LAST = Gauge('hello_world_last_time_seconds',
        'The last time a Hello World was served.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        INPROGRESS.inc()
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
        LAST.set(time.time())
        INPROGRESS.dec()
```
:::

These metrics can be used directly in the expression browser without any
additional functions. For example, **`hello_world_last_time_seconds`**
can be used to determine when the last Hello World was served. The main
use case for such a metric is detecting if it has been too long since a
request was handled.[]{#ch03.xhtml#idm45207113741072 primary="time"
data-type="indexterm"} The PromQL expression
**`time() - hello_world_last_time_seconds`** will tell you how many
seconds it is since the last request.

These are both very common use cases, so utility functions are also
provided for them, as you can see in
[Example 3-7](#ch03.xhtml#simple_program_gauge_utility){data-type="xref"}.
`track_inprogress` has the advantage of being both shorter and taking
care of correctly handling exceptions for you.
`set_to_​`[`current_time`]{.keep-together} is a little less useful in
Python, as `time.time()` returns Unix time,
[]{#ch03.xhtml#idm45207113698000 primary="Unix" secondary="time"
data-type="indexterm"}in
seconds;^[5](#ch03.xhtml#idm45207113696864){#ch03.xhtml#idm45207113696864-marker
data-type="noteref"}^ but in other languages' client libraries, the
`set_to_current_time` equivalents make usage simpler and clearer.

::: {#ch03.xhtml#simple_program_gauge_utility data-type="example"}
##### [Example 3-7. ]{.label}The same example as [Example 3-6](#ch03.xhtml#simple_program_gauge){data-type="xref"} but using the gauge utilities

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Gauge

INPROGRESS = Gauge('hello_worlds_inprogress',
        'Number of Hello Worlds in progress.')
LAST = Gauge('hello_world_last_time_seconds',
        'The last time a Hello World was served.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    @INPROGRESS.track_inprogress()
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
        LAST.set_to_current_time()
```
:::

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch03.xhtml#idm45207113663472 .sidebar}
# Metric Suffixes

You may have noticed that the example counter metrics all ended with
`_total`, while there is no such suffix on
gauges.[]{#ch03.xhtml#idm45207113643136 primary="metrics"
secondary="suffixes"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113642160
primary="suffixes (metrics)" data-type="indexterm"} This is a convention
within Prometheus that makes it easier to identify what type of metric
you are working with.

With OpenMetrics, this suffix is mandated.
[]{#ch03.xhtml#idm45207113640848 primary="OpenMetrics"
secondary="metric suffixes" data-type="indexterm"}As the
prometheus_client Python library is the reference implementation for
OpenMetrics, if you do not add the suffix, the library will add it for
you.

In addition to `_total`, the `_count`, `_sum`, and `_bucket` suffixes
also have other meanings and should not be used as suffixes in your
metric names to avoid confusion.

It is []{#ch03.xhtml#idm45207113637024 primary="units"
secondary="in metric names" secondary-sortas="metric"
data-type="indexterm"}also strongly recommended that you include the
unit of your metric at the end of its name. For example, a counter for
bytes processed might be
`myapp_requests_​`[`processed_bytes_total`]{.keep-together}.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Callbacks" data-type="sect2"}
::: {#ch03.xhtml#idm45207113583472 .sect2}
## Callbacks

To track the size or number of items in a cache, you should generally
add `inc` and `dec` calls in each function where items are added or
removed from the cache. []{#ch03.xhtml#idm45207113581392
primary="caches" secondary="tracking size or number of files in"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113580448 primary="gauges"
secondary="using callbacks"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113579504
primary="callbacks" data-type="indexterm"}With more complex logic this
can get a bit tricky to get right and maintain as the code changes. The
good news is that client libraries offer a shortcut to implement this,
without having to use the interfaces that writing an exporter require.

In Python, gauges have a `set_function` method, which allows you to
specify a function to be called at exposition
time.[]{#ch03.xhtml#idm45207113577904 primary="set_function method"
data-type="indexterm"} Your function must return a floating-point value
for the metric when called, as demonstrated in
[Example 3-8](#ch03.xhtml#simple_program_gauge_set_function){data-type="xref"}.
However, this strays a bit outside of direct instrumentation, so you
will need to consider thread safety and may need to use mutexes when
designing those callback functions.

::: {#ch03.xhtml#simple_program_gauge_set_function data-type="example"}
##### [Example 3-8. ]{.label}A trivial example of `set_function` to have a metric return the current time^[6](#ch03.xhtml#idm45207113574528){#ch03.xhtml#idm45207113574528-marker data-type="noteref"}^

``` {code-language="python" data-type="programlisting"}
import time
from prometheus_client import Gauge

TIME = Gauge('time_seconds',
        'The current time.')
TIME.set_function(lambda: time.time())
```
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="The Summary" data-type="sect1"}
::: {#ch03.xhtml#summary_metric .sect1}
# The Summary

Knowing how long your application took []{#ch03.xhtml#idm45207113554048
primary="instrumentation" secondary="gauges" startref="ix_instrgauge"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113552800 primary="gauges"
startref="ix_gauge" data-type="indexterm"}to respond to a request or the
latency of a backend are vital metrics when you are trying to understand
the performance of your systems.[]{#ch03.xhtml#idm45207113551728
primary="summary" data-type="indexterm"}[]{#ch03.xhtml#idm45207113551056
primary="instrumentation" secondary="summary" data-type="indexterm"}
Other instrumentation systems offer some form of Timer metric, but
Prometheus views things more generically. Just as counters can be
incremented by values other than one, you may wish to track things about
events other than their latency. For example, in addition to backend
latency you may also wish to track the size of the responses you get
back.

The primary method of a summary is `observe`, to which you pass the size
of the event.[]{#ch03.xhtml#idm45207113548960 primary="observe method"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113548256 primary="time"
secondary="tracking latency"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113547312 primary="latency"
secondary="tracking for Hello World program (example)"
data-type="indexterm"} This must be a nonnegative value. Using
`time.time()` you can track latency, as shown in
[Example 3-9](#ch03.xhtml#simple_program_latency_manual){data-type="xref"}.

::: {#ch03.xhtml#simple_program_latency_manual data-type="example"}
##### [Example 3-9. ]{.label}`LATENCY` tracks how long the Hello World handler takes to run

``` {code-language="python" data-type="programlisting"}
import time
from prometheus_client import Summary

LATENCY = Summary('hello_world_latency_seconds',
        'Time for a request Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        start = time.time()
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
        LATENCY.observe(time.time() - start)
```
:::

If you look at the */metrics* page, you will see that the
`hello_world_latency_​seconds` metric has two time series:
`hello_world_latency_seconds_count` and
`hello_world_latency_seconds_sum`.

`hello_world_latency_seconds_count` is the number of `observe` calls
that have been made, so
**`rate(hello_world_latency_seconds_count[1m])`** in the expression
browser would return the per-second rate of Hello World requests.

`hello_world_latency_seconds_sum` is the sum of the values passed to
`observe`, so `rate(hello_world_latency_seconds_sum[1m])` is the amount
of time spent responding to requests per second.

If you divide these two expressions, you get the average latency over
the last minute. The full expression for average latency would be:

``` {data-type="programlisting"}
  rate(hello_world_latency_seconds_sum[1m])
/
  rate(hello_world_latency_seconds_count[1m])
```

Let's take an example. Say in the last minute you had three requests
that took 2, 4, and 9 seconds. The count would be 3 and the sum would be
15 seconds, so the average latency is 5 seconds. `rate` is per second
rather than per minute, so in principle you need to divide both sides by
60, but that cancels out.[]{#ch03.xhtml#idm45207119194480
primary="rate function" data-type="indexterm"}

::: {.note data-type="note"}
###### Note

Even though the `hello_world_latency_seconds` metric is using seconds as
its unit in line with Prometheus conventions, this does not mean it only
has second precision.[]{#ch03.xhtml#idm45207119192304 primary="time"
secondary="precision in Prometheus" data-type="indexterm"} Prometheus
uses 64-bit floating-point values that can handle metrics ranging from
days to nanoseconds. The preceding example takes about a quarter of a
millisecond on our machine, for example.
:::

As summaries are usually used to track latency, there is a `time`
context manager and function decorator that makes this simpler, as you
can see in
[Example 3-10](#ch03.xhtml#simple_program_latency){data-type="xref"}.
[]{#ch03.xhtml#idm45207119189472 primary="time"
secondary="context manager and function decorator"
data-type="indexterm"}It also handles exceptions and time going backward
for
you.^[7](#ch03.xhtml#idm45207119139280){#ch03.xhtml#idm45207119139280-marker
data-type="noteref"}^

::: {#ch03.xhtml#simple_program_latency data-type="example"}
##### [Example 3-10. ]{.label}`LATENCY` tracking latency using the `time` function decorator

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Summary

LATENCY = Summary('hello_world_latency_seconds',
        'Time for a request Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    @LATENCY.time()
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
```
:::

Summary metrics may also include quantiles, although the Python client
does not currently support these client-side quantiles.
[]{#ch03.xhtml#idm45207119046464 primary="quantiles"
data-type="indexterm"}These should generally be avoided as you cannot do
math such as averages on top of quantiles, preventing you from
aggregating client-side quantiles from across the instances of your
service. In addition, client-side quantiles are expensive compared to
other instrumentation in terms of CPU usage (a factor of a hundred
slower is not unusual). While the benefits of instrumentation generally
greatly outweigh their resource costs, this may not be the case for
quantiles.
:::
:::

::: {.section pdf-bookmark="The Histogram" data-type="sect1"}
::: {#ch03.xhtml#histogram .sect1}
# The Histogram

A summary will provide the average latency, but what if you want a
quantile?[]{#ch03.xhtml#ix_hist primary="histograms"
data-type="indexterm"}[]{#ch03.xhtml#ix_instrhist
primary="instrumentation" secondary="histograms" data-type="indexterm"}
Quantiles tell you that a certain proportion of events had a size below
a given value. For example, the 0.95 quantile being 300 ms means that
95% of requests took less than 300 ms.

Quantiles are useful when reasoning about actual end-user experience. If
a user's browser makes 20 concurrent requests to your application, then
it is the slowest of them that determines the user-visible latency. In
this case, the 95th percentile captures that
latency.[]{#ch03.xhtml#idm45207119107552 primary="units"
secondary="quantiles and percentiles" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

The 95th percentile is the 0.95 quantile.
[]{#ch03.xhtml#idm45207119105632 primary="quantiles"
secondary="and percentiles" secondary-sortas="percentiles"
data-type="indexterm"}[]{#ch03.xhtml#idm45207119104384
primary="percentiles"
data-type="indexterm"}[]{#ch03.xhtml#idm45207119103712
primary="base units" secondary="quantiles and percentiles"
data-type="indexterm"}As Prometheus prefers base units, it always uses
quantiles, in the same way that ratios are preferred to percentages.
:::

The instrumentation for histograms is the same as for summaries. The
`observe` method allows you to do manual observations, and the `time`
context manager and function decorator allow for easier timings, as
shown in
[Example 3-11](#ch03.xhtml#simple_program_latency_histogram){data-type="xref"}.

::: {#ch03.xhtml#simple_program_latency_histogram data-type="example"}
##### [Example 3-11. ]{.label}`LATENCY` histogram tracking latency using the `time` function decorator

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Histogram

LATENCY = Histogram('hello_world_latency_seconds',
        'Time for a request Hello World.')

class MyHandler(http.server.BaseHTTPRequestHandler):
    @LATENCY.time()
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
```
:::

This will produce a set of time series with the name
`hello_world_latency_seconds_bucket`, which are a set of counters. A
histogram has a set of buckets, such as [1 ms]{.keep-together}, 10 ms,
and 25 ms, that track the number of events that fall into each
bucket.[]{#ch03.xhtml#idm45207118983472
primary="histogram_quantile function" data-type="indexterm"} The
`histogram_quantile` PromQL function can calculate a quantile from the
buckets. For example, the 0.95 quantile (95th percentile) would be:

``` {data-type="programlisting"}
histogram_quantile(0.95, rate(hello_world_latency_seconds_bucket[1m]))
```

The `rate` is needed as the buckets' time series are counters.

::: {.section pdf-bookmark="Buckets" data-type="sect2"}
::: {#ch03.xhtml#idm45207118980576 .sect2}
## Buckets

The default buckets cover a range of latencies from 1 ms to 10 s.
[]{#ch03.xhtml#idm45207118978848 primary="histograms"
secondary="buckets"
data-type="indexterm"}[]{#ch03.xhtml#idm45207118977872
primary="buckets (in histograms)" data-type="indexterm"}This is intended
to capture the typical range of latencies for a web application. But you
can also override them and provide your own buckets when defining
metrics. This might be done if the defaults are not suitable for your
use case, or to add an explicit bucket for latency quantiles mentioned
in your Service-Level Agreements (SLAs). In order to help you detect
typos, the provided buckets must be sorted:

``` {code-language="python" data-type="programlisting"}
LATENCY = Histogram('hello_world_latency_seconds',
        'Time for a request Hello World.',
        buckets=[0.0001, 0.0002, 0.0005, 0.001, 0.01, 0.1])
```

If you want linear or exponential buckets, you can use Python list
comprehensions. []{#ch03.xhtml#idm45207115849264 primary="lists"
secondary="list comprehensions" data-type="indexterm"}Client libraries
for languages that do not have an equivalent to list comprehensions may
include utility functions for these:

``` {code-language="python" data-type="programlisting"}
buckets=[0.1 * x for x in range(1, 10)]    # Linear
buckets=[0.1 * 2**x for x in range(1, 10)] # Exponential
```

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch03.xhtml#cumulative_histograms .sidebar}
# Cumulative Histograms

If you have looked at a */metrics* page for a histogram, you probably
noticed that the buckets aren't just a count of events that fall into
them.[]{#ch03.xhtml#idm45207115785616 primary="cumulative histograms"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115784912
primary="histograms" secondary="cumulative"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115783968
primary="buckets (in histograms)" secondary="cumulative histograms"
data-type="indexterm"} The buckets also include a count of events in all
the smaller buckets, all the way up to the `+Inf` bucket, which is the
total number of events. This is known as a cumulative histogram, and why
the bucket label is called `le`, standing for less than or equal to.

This is in addition to buckets being counters, so Prometheus histograms
are cumulative in two different ways.

The reason they're cumulative is that if the number of buckets becomes a
performance problem, some extraneous
buckets^[8](#ch03.xhtml#idm45207115781024){#ch03.xhtml#idm45207115781024-marker
data-type="noteref"}^ can be dropped using
`metric_relabel_​`[`configs`]{.keep-together} (see
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"})
in Prometheus while still allowing quantiles to be calculated. There is
an example of this in
[Example 8-27](#ch08.xhtml#prometheus_drop_buckets){data-type="xref"}.
:::

```{=html}
</aside>
```
You may be wondering how many buckets you should have for sufficient
accuracy. We recommend sticking to somewhere around 10. This may seem
like a small number, but buckets are not free, as each is an extra time
series to be
stored.^[9](#ch03.xhtml#idm45207115776752){#ch03.xhtml#idm45207115776752-marker
data-type="noteref"}^ Fundamentally, a metrics-based system like
Prometheus is not going to provide 100% accurate quantiles. For that you
would need to calculate the quantiles from a log-based system. But what
Prometheus provides is good enough for most practical alerting and
debugging purposes.

The best way to think of buckets (and metrics generally) is that while
they may not always be perfect, they generally give you sufficient
information to determine the next step when you are debugging. So, for
example, if Prometheus indicates that the 0.95 quantile jumped from 300
ms to 350 ms, but it was actually from 305 ms to [355
ms]{.keep-together}, that doesn't matter that much. You still know that
there was a big jump, and the next step in your investigation would be
the same either way.

::: {.note data-type="note"}
###### Note

At the time of writing this book, there is a new experimental feature in
Prometheus and some client libraries, called Native
Histograms.[]{#ch03.xhtml#idm45207115773824
primary="Native Histograms (experimental feature)"
data-type="indexterm"} They use dynamic buckets and fix most of the
issues of "old" histograms.

Ganesh Vernekar and Björn Rabenstein talked about that experimental
feature at PromCon 2022:

-   [Native Histograms in Prometheus talk](https://oreil.ly/uLAxm)

-   [PromQL for Native Histograms talk](https://oreil.ly/8CNJx)
:::

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch03.xhtml#idm45207115736128 .sidebar}
# SLAs and Quantiles

Latency SLAs will often be expressed as *95th percentile latency is at
most 500 ms*. There is a nonobvious trap here, in that you may focus on
the wrong number.[]{#ch03.xhtml#idm45207115734176 primary="latency"
secondary="latency SLAs and quantiles"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115733264
primary="quantiles" secondary="latency SLAs and" data-type="indexterm"}

Calculating the 95th percentile accurately is tricky, requiring what may
be significant computing resources if you want to get it perfect.
Calculating how the proportion of requests that took more than 500 ms is
easy though---you only need two counters: one for all requests and
another for requests that took up to 500 ms.

By having a 500 ms bucket in your histogram, you can accurately
calculate the ratio of requests that take over 500 ms using:

``` {data-type="programlisting"}
  my_latency_seconds_bucket{le="0.5"}
/ ignoring(le)
  my_latency_seconds_bucket{le="+Inf"}
```

to determine if you are meeting your SLA. The rest of the buckets will
still give you a good estimate of the 95th percentile latency.

Tools like [Pyrra](https://pyrra.dev) can assist in managing your SLOs,
calculating error budget, and producing recording and alerting rules.
:::

```{=html}
</aside>
```
Quantiles are limited in that once you calculate them, you cannot do any
further math on them. []{#ch03.xhtml#idm45207115728624
primary="quantiles" secondary="limitations of" data-type="indexterm"}It
is not statistically correct to add, subtract, or average them, for
example. This affects not just what you might attempt in PromQL, but
also how you reason about a system while debugging it. A frontend may
report a latency increase in the 0.95 quantile, yet the backend that
caused it may show no such increase (or even a decrease!).

This can be very counterintuitive, especially when you have been woken
up in the middle of the night to debug a problem. Averages, on the other
hand, do not have this problem; they can be added and
subtracted.^[10](#ch03.xhtml#idm45207115727136){#ch03.xhtml#idm45207115727136-marker
data-type="noteref"}^ For example, if you see a 20 ms increase in
latency in a frontend due to one of its backends, you will see a
matching latency increase of around 20 ms in the backend. But there is
no such guarantee with quantiles. So while quantiles are good for
capturing end-user experience, they are tricky to debug with.

We recommend debugging latency issues primarily with averages rather
than quantiles.[]{#ch03.xhtml#idm45207115725344 primary="latency"
secondary="calculating average latency" data-type="indexterm"} Averages
work the way you think they do, and once you have narrowed down the
subsystem to blame for a latency increase using averages, you can switch
back to quantiles if appropriate. To this end, the histogram also
includes `_sum` and `_count` time series. Just like with a summary, you
can calculate average latency with:

``` {data-type="programlisting"}
  rate(hello_world_latency_seconds_sum[1m])
/
  rate(hello_world_latency_seconds_count[1m])
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Unit Testing Instrumentation" data-type="sect1"}
::: {#ch03.xhtml#idm45207119111296 .sect1}
# Unit Testing Instrumentation

Unit tests are a good way to avoid accidentally breaking your code as it
changes over time.[]{#ch03.xhtml#idm45207115720896
primary="instrumentation" secondary="histograms" startref="ix_instrhist"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115719648
primary="histograms" startref="ix_hist" data-type="indexterm"} You
should approach unit testing instrumentation the same way you approach
unit tests for logs.[]{#ch03.xhtml#idm45207115718464
primary="instrumentation" secondary="unit testing"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115717520
primary="unit tests for instrumentation" data-type="indexterm"} Just as
you would probably not test a debug-level log statement, neither should
you test the majority of metrics that you sprinkle across your code
base.

You would usually only unit test log statements for transaction logs and
sometimes request
logs.^[11](#ch03.xhtml#idm45207115716256){#ch03.xhtml#idm45207115716256-marker
data-type="noteref"}^ Similarly, it usually makes sense to unit test
metrics where the metric is a key part of your application or library.
For example, if you are writing an RPC library, it would make sense to
have at least some basic tests to make sure the key requests, latency,
and error metrics are working.

Without tests, some of the noncritical metrics you might use for
debugging may not work, and in our experience this will be the case for
around 5% of debug metrics. Requiring all metrics to be unit tested
would add friction to instrumentation, so rather than ending up with 20
metrics of which 19 are usable, you might instead end up with only 5
tested metrics. It would no longer be a case of adding two lines of code
to add a metric. When it comes to using metrics for debugging and deep
performance analysis, a wider breadth of metrics is always useful.

The Python client offers a `get_sample_value` function that will
effectively scrape the registry and look for a time series.
[]{#ch03.xhtml#idm45207115713120 primary="get_sample_value function"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115712352 primary="Python"
secondary="unit testing a counter in"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115711392
primary="counters" secondary="unit testing in Python"
data-type="indexterm"}You can use `get_sample_value` as shown in
[Example 3-12](#ch03.xhtml#python_unittest){data-type="xref"} to test
counter instrumentation. It is the increase of a counter that you care
about, so you should compare the value of the counter before and after,
rather than the absolute value. This will work even if other tests have
also caused the counter to be incremented.

::: {#ch03.xhtml#python_unittest data-type="example"}
##### [Example 3-12. ]{.label}Unit testing a counter in Python

``` {code-language="python" data-type="programlisting"}
import unittest
from prometheus_client import Counter, REGISTRY

FOOS = Counter('foos_total', 'The number of foo calls.')

def foo():
    FOOS.inc()

class TestFoo(unittest.TestCase):
    def test_counter_inc(self):
        before = REGISTRY.get_sample_value('foos_total')
        foo()
        after = REGISTRY.get_sample_value('foos_total')
        self.assertEqual(1, after - before)
```
:::
:::
:::

::: {.section pdf-bookmark="Approaching Instrumentation" data-type="sect1"}
::: {#ch03.xhtml#idm45207115695888 .sect1}
# Approaching Instrumentation

Now that you know how to use instrumentation, it is important to know
where and how much you should apply it.

::: {.section pdf-bookmark="What Should I Instrument?" data-type="sect2"}
::: {#ch03.xhtml#idm45207115654064 .sect2}
## What Should I Instrument?

When instrumenting, you will usually be looking to either instrument
services or libraries.[]{#ch03.xhtml#idm45207115652720
primary="instrumentation" secondary="deciding what to instrument"
data-type="indexterm"}

::: {.section pdf-bookmark="Service instrumentation" data-type="sect3"}
::: {#ch03.xhtml#service_instrumentation .sect3}
### Service instrumentation

Broadly speaking, there are three types of services, each with their own
key metrics: online-serving systems, offline-serving systems, and batch
jobs.[]{#ch03.xhtml#idm45207115649680 primary="instrumentation"
secondary="deciding what to instrument"
tertiary="service instrumentation"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115648464
primary="services" secondary="instrumentation" data-type="indexterm"}

Online-serving systems are those where either a human or another service
is waiting on a response.[]{#ch03.xhtml#idm45207115647024
primary="online-serving systems" data-type="indexterm"} These include
web servers and databases. The key metrics to include in service
instrumentation are the request rate, latency, and error rate. Having
request rate, latency, and error rate metrics is sometimes called the
RED method, for Rate, Errors, and Duration.
[]{#ch03.xhtml#idm45207115646192
primary="RED method (rate, errors, duration)"
data-type="indexterm"}[]{#ch03.xhtml#idm45207115645552
primary="durations" secondary="metrics on"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116517104
primary="rate, errors, and duration (RED method)"
data-type="indexterm"}These metrics are not just useful to you from the
server side, but also the client side. If you notice that the client is
seeing more latency than the server, you might have network issues or an
overloaded client.

::: {data-type="tip"}
###### Tip

When instrumenting duration, don't be tempted to exclude
failures.[]{#ch03.xhtml#idm45207116515360 primary="durations"
secondary="instrumenting, not excluding failures" data-type="indexterm"}
If you were to include only successes, then you might not notice high
latency caused by many slow but failing requests.
:::

Offline-serving systems do not have someone waiting on
them.[]{#ch03.xhtml#idm45207116513776 primary="offline-serving systems"
data-type="indexterm"} They usually batch up work and have multiple
stages in a pipeline with queues between them. A log processing system
is an example of an offline-serving system. For each stage you should
have metrics for the amount of queued work, how much work is in
progress, how fast you are processing items, and errors that occur.
These metrics are also known as the USE method, for Utilization,
Saturation, and Errors. []{#ch03.xhtml#idm45207116512944
primary="USE method (utilization, saturation, and errors)"
data-type="indexterm"}Utilization is how full your service is,
saturation is the amount of queued work, and errors is self-explanatory.
If you are using batches, then it is useful to have metrics both for the
batches and the individual items.

Batch jobs are the third type of service, and they are similar to
offline-serving systems.[]{#ch03.xhtml#idm45207116511680
primary="batch jobs" data-type="indexterm"} However, batch jobs run on a
regular schedule, whereas offline-serving systems run continuously. As
batch jobs are not always running, scraping them doesn't work too well,
so techniques such as the Pushgateway (discussed in
["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}) and the Node
Exporter textfile collector (discussed in ["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}) are used.
At the end of a batch job you should record how long it took to run, how
long each stage of the job took, and the time at which the job last
succeeded. You can add alerts if the job hasn't succeeded recently
enough, allowing you to tolerate individual batch job run failures.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch03.xhtml#batch_idempotency .sidebar}
# Idempotency for Batch Jobs

Idempotency is the property of getting the same result from an operation
or function regardless of how many times you run it.
[]{#ch03.xhtml#idm45207116507104 primary="batch jobs"
secondary="idempotency for"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116506128
primary="idempotency for batch jobs" data-type="indexterm"}This is a
useful property for batch jobs as it means handling a failed job is
simply a matter of retrying, so you don't have to worry as much about
individual failures.

To achieve this you should avoid passing which items of work (such as
the previous day's data) a batch job should work on. Instead, you should
have the batch job infer that and continue from where it left off.

This has the additional benefit that you can have your batch jobs retry
themselves. For example, you might have a daily batch job run instead a
few times per day, so that even if there is a transient failure, the
next run will take care of it. Alert thresholds can be increased
accordingly, as you will need to manually intervene less often.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Library instrumentation" data-type="sect3"}
::: {#ch03.xhtml#idm45207116504432 .sect3}
### Library instrumentation

Services are what you care about at a high
level.[]{#ch03.xhtml#idm45207116503024 primary="libraries"
secondary="instrumentation"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116502048
primary="instrumentation" secondary="deciding what to instrument"
tertiary="library instrumentation" data-type="indexterm"} Within each of
your services there are libraries that you can think of as mini
services. The majority will be online-serving subsystems, which is to
say, synchronous function calls, and benefit from the same metrics of
requests, latency, and errors. []{#ch03.xhtml#idm45207116500736
primary="caches" secondary="metrics for cache overall and cache misses"
data-type="indexterm"}For a cache, you would want these metrics both for
the cache overall and the cache misses that then need to calculate the
result or request it from a backend.

::: {data-type="tip"}
###### Tip

With metrics for failures and total, it is easy to calculate the failure
ratio by division. []{#ch03.xhtml#idm45207116498608 primary="failures"
secondary="metrics for total and failures"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116497616 primary="metrics"
secondary="total and failures, not success and failures"
data-type="indexterm"}With success and failures this is
trickier,^[12](#ch03.xhtml#idm45207116496576){#ch03.xhtml#idm45207116496576-marker
data-type="noteref"}^ as you first need to calculate the total.

Similarly, for caches it is best to have either hits and total requests,
or failures and total requests. All of total, hits, and misses works
fine too.
:::

It is beneficial to add metrics for any errors that occur and anywhere
that you have logging. You might only keep your debug logs for a few
days due to their volume, but with a metric you can still have a good of
idea of the frequency of that log line over time.

Thread and worker pools should be instrumented similarly to
offline-serving systems. You will want to have metrics for the queue
size, active threads, any limit on the number of threads, and errors
encountered.[]{#ch03.xhtml#idm45207116494416
primary="thread and worker pools, instrumentation"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116493616
primary="worker pools, instrumentation" data-type="indexterm"}

Background maintenance tasks that run no more than a few times an hour
are effectively batch jobs, and you should have similar metrics for
these tasks.
:::
:::
:::
:::

::: {.section pdf-bookmark="How Much Should I Instrument?" data-type="sect2"}
::: {#ch03.xhtml#instrumentation_scale .sect2}
## How Much Should I Instrument?

While Prometheus is extremely efficient, there are limits to how many
metrics it can handle. []{#ch03.xhtml#idm45207116490080
primary="metrics" secondary="limits on Prometheus' handling of"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116489040
primary="instrumentation" secondary="deciding how much to instrument"
data-type="indexterm"}At some point the operational and resource costs
outweigh the benefits for certain instrumentation strategies.

The good news is that most of the time you don't need to worry about
this. Let's say that you had a Prometheus server that could handle 10
million
metrics^[13](#ch03.xhtml#idm45207116487440){#ch03.xhtml#idm45207116487440-marker
data-type="noteref"}^ and 1,000 application instances. A single new
metric on each of these instances would use 0.01% of your resources,
making it effectively
free.^[14](#ch03.xhtml#idm45207116486592){#ch03.xhtml#idm45207116486592-marker
data-type="noteref"}^ This means you are able to add individual metrics
by hand where it is useful.

Where you need to be careful is when things get industrial. If you
automatically add a metric for the duration of every function, that can
add up fast (it is classic profiling, after all).
[]{#ch03.xhtml#idm45207116485552 primary="durations"
secondary="not adding metric for duration of every function"
data-type="indexterm"}If you have metrics broken out by request type and
HTTP
URL,^[15](#ch03.xhtml#idm45207116484480){#ch03.xhtml#idm45207116484480-marker
data-type="noteref"}^ all the potential combinations can easily take up
a significant chunk of your resources. Histogram buckets expand that
again.[]{#ch03.xhtml#idm45207116483120 primary="buckets (in histograms)"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116482480
primary="histograms" secondary="buckets" data-type="indexterm"} A metric
with a cardinality of a hundred on each instance would take up 1% of
your Prometheus server's resources, which is a less clear win and
certainly not free. We discuss this further in
["Cardinality"](#ch05.xhtml#cardinality_section){data-type="xref"}.

It is common for the 10 biggest metrics in a Prometheus instance to
constitute over half of its resource usage. If you are trying to manage
the resource usage of your Prometheus, you will get a better return for
your efforts by focusing on the 10 biggest metrics.

As a rule of thumb, a simple []{#ch03.xhtml#idm45207116479776
primary="caches" data-type="indexterm"}service like a cache might have a
hundred metrics in total, while a complex and well-instrumented service
might have a thousand.
:::
:::

::: {.section pdf-bookmark="What Should I Name My Metrics?" data-type="sect2"}
::: {#ch03.xhtml#metric_naming .sect2}
## What Should I Name My Metrics?

The naming of metrics is more of an art than a
science.[]{#ch03.xhtml#ix_instrnmmtr primary="instrumentation"
secondary="naming metrics"
data-type="indexterm"}[]{#ch03.xhtml#ix_mtrcname primary="metrics"
secondary="naming" data-type="indexterm"} There are some simple rules
you can follow to avoid the more obvious pitfalls, and also general
guidelines to construct your metric names.

::: {.warning data-type="warning"}
###### Warning

Renaming metrics can make it difficult to track and analyze data
accurately over time, as it can break existing queries and dashboards.
Complex workarounds such as editing PromQL queries may be required to
maintain the integrity of your data when a metric gets renamed.
:::

The overall structure of a metric name is generally
*library_name_unit_suffix*.

::: {.section pdf-bookmark="Characters" data-type="sect3"}
::: {#ch03.xhtml#idm45207116471840 .sect3}
### Characters

Prometheus metric names should start with a letter, and can be followed
with any number of letters, numbers, and
underscores.[]{#ch03.xhtml#idm45207116470512
primary="characters (in metric names)" data-type="indexterm"}

While `[a-zA-Z_:][a-zA-Z0-9_:]*` is a regular expression for valid
metric names for Prometheus, you should avoid some of the valid values.
You should not use colons in instrumentation as they are reserved for
user use in recording rules, as discussed in ["Naming of Recording
Rules"](#ch17.xhtml#naming_rules){data-type="xref"}. Underscores at the
start of metric names are reserved for internal Prometheus use.
:::
:::

::: {.section pdf-bookmark="snake_case" data-type="sect3"}
::: {#ch03.xhtml#idm45207116467856 .sect3}
### snake_case

The convention with []{#ch03.xhtml#idm45207116466560
primary="snake case for metric names" data-type="indexterm"}Prometheus
is to use snake case for metric names; that is, each component of the
name should be lowercase and separated by an underscore.
:::
:::

::: {.section pdf-bookmark="Metric suffixes" data-type="sect3"}
::: {#ch03.xhtml#idm45207116465216 .sect3}
### Metric suffixes

The `_total`, `_count`, `_sum`, and `_bucket` suffixes are used by the
counter, summary, and histogram metrics.[]{#ch03.xhtml#idm45207116461904
primary="suffixes (metrics)" data-type="indexterm"} Aside from always
having a `_total` suffix on counters, you should avoid putting these
suffixes on the end of your metric names to avoid
[confusion]{.keep-together}.
:::
:::

::: {.section pdf-bookmark="Units" data-type="sect3"}
::: {#ch03.xhtml#idm45207116459760 .sect3}
### Units

You should prefer using unprefixed base units such as seconds, bytes,
and
ratios.^[16](#ch03.xhtml#idm45207116458400){#ch03.xhtml#idm45207116458400-marker
data-type="noteref"}^ []{#ch03.xhtml#idm45207116457600
primary="base units"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116456896 primary="units"
secondary="in metric names" secondary-sortas="metric"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116455680 primary="time"
data-type="indexterm"}This is because Prometheus uses seconds in
functions such as `time`, and it avoids ugliness such as
kilomicroseconds.

Using only one unit avoids confusion as to whether this particular
metric is seconds or
milliseconds,^[17](#ch03.xhtml#idm45207116453904){#ch03.xhtml#idm45207116453904-marker
data-type="noteref"}^ so you should always include the unit of your
metric in the name. For example, `mymetric_seconds_total` for a counter
with a unit of seconds.

There is not always an obvious unit for a metric, so don't worry if your
metric name is missing a unit. You should avoid `count` as a unit, as
aside from clashing with summaries and histograms, most metrics are
counts of something so it doesn't tell you anything. Similarly with
`total`.
:::
:::

::: {.section pdf-bookmark="Name" data-type="sect3"}
::: {#ch03.xhtml#idm45207116451008 .sect3}
### Name

The meat of a metric name is, um, the name.
[]{#ch03.xhtml#idm45207116449072 primary="name (metrics)"
data-type="indexterm"}The name of a metric should give someone who has
no knowledge of the subsystem the metric is from a good idea of what [it
means]{.keep-together}. `requests` is not very insightful,
`http_requests` is better, and `http_requests_authenticated` is better
again. The metric description can expand further, but often the user
will only have the metric name to go on.

As you can see from the preceding examples, a name may have several
underscore-separated components. Try to have the same prefix on related
metrics, so that it's easier to understand their relationship.
`queue_size` and `queue_limit` are more useful than `size_queue` and
`limit_queue`. You might even have `items` and `items_limit`. Names
generally go from less to more specific as you go from left to right.

Do not put what should be labels (covered in
[Chapter 5](#ch05.xhtml#labels_chapter){data-type="xref"}) in metric
names. When implementing direct instrumentation, you should never
procedurally generate metrics or metric
names.[]{#ch03.xhtml#idm45207116441520 primary="labels"
secondary="metric names and" data-type="indexterm"}

::: {.note data-type="note"}
###### Note

You should avoid putting the names of labels that a metric has into a
metric's name because it will be incorrect when that label is aggregated
away with PromQL.
:::
:::
:::

::: {.section pdf-bookmark="Library" data-type="sect3"}
::: {#ch03.xhtml#idm45207116439216 .sect3}
### Library

As metrics names are effectively a global namespace, it is important to
both try to avoid collisions between libraries and indicate where a
metric is coming from. []{#ch03.xhtml#idm45207116437648
primary="libraries" secondary="in metric names"
secondary-sortas="metric" data-type="indexterm"}A metric name is
ultimately pointing you to a specific line of code in a specific file in
a specific library. A library could be a stereotypical library that you
have pulled in as a dependency, a subsystem in your application, or even
the main function of the application itself.

You should provide sufficient distinction in the library part of the
metric name to avoid confusion, but there's no need to include complete
organization names and paths in source control. There is a balance
between succinctness and full [qualification]{.keep-together}.

For example, Cassandra is a well-established application so it would be
appropriate for it to use just `cassandra` as the library part of its
metric names. On the other hand, using `db` for a company's internal
database connection pool library would be unwise, as database libraries
and database connection pool libraries are both quite common. You might
even have several inside the same application.
`robustperception_db_pool` or `rp_db_pool` would be better choices
there.

Some library names are already
established.[]{#ch03.xhtml#idm45207116432160 primary="process library"
data-type="indexterm"} The `process` library exposes process-level
metrics such as CPU and memory usage, and is standardized across client
libraries. Thus you should not expose additional metrics with this
prefix. []{#ch03.xhtml#idm45207116430912 primary="client libraries"
secondary="metrics related to runtime"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116429872 primary="Go"
secondary="client library metrics for" data-type="indexterm"}Client
libraries also expose metrics relating to their runtime. Python metrics
use `python`, Java Virtual Machine (JVM) metrics use `jvm`, and Go uses
`go`.

Combining these steps produces metric names like
`go_memstats_heap_inuse_bytes`. This is from the `go_memstats` library,
memory statistics from the Go runtime. `heap_inuse` indicates the metric
is related to the amount of heap being used, and `bytes` tells us that
it is measured in bytes. From just the name you can tell that it is the
amount of the heap
memory^[18](#ch03.xhtml#idm45207116424704){#ch03.xhtml#idm45207116424704-marker
data-type="noteref"}^ that Go is currently using. While the meaning of a
metric will not always be this obvious from the name, it is something to
strive for.

::: {data-type="caution"}
###### Caution

You should not prefix all metric names coming from an application with
the name of the application.
`process_cpu_seconds_​`[`total`]{.keep-together} is
`process_cpu_seconds_total` no matter which application exposes it.
[]{#ch03.xhtml#idm45207116420624
primary="applications, metric names coming from"
data-type="indexterm"}The way to distinguish metrics from different
applications is with target labels, not metric names. See ["Target
Labels"](#ch08.xhtml#target_labels){data-type="xref"}.
:::

Now that you have instrumented your application, let's look at how you
can expose those metrics to Prometheus.[]{#ch03.xhtml#idm45207116418208
primary="instrumentation" secondary="naming metrics"
startref="ix_instrnmmtr"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116416960 primary="metrics"
secondary="naming" startref="ix_mtrcname"
data-type="indexterm"}[]{#ch03.xhtml#idm45207116415744
primary="instrumentation" startref="ix_instr" data-type="indexterm"}
:::
:::
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch03.xhtml#idm45207117179600-marker)^ Unfortunately, not all
client libraries can have this happen automatically for various
technical reasons.

^[2](#ch03.xhtml#idm45207117171568-marker)^ It may increase by two due
to your browser also hitting the */favicon.ico* endpoint.

^[3](#ch03.xhtml#idm45207117331136-marker)^ While this is a gauge, it is
best exposed using a counter. You can convert a requests over time
counter to a gauge in PromQL with the `rate` function.

^[4](#ch03.xhtml#idm45207117327696-marker)^ Unlike counters, gauges can
decrease, so it is fine to pass negative numbers to a gauge's `inc`
method.

^[5](#ch03.xhtml#idm45207113696864-marker)^ Seconds
[]{#ch03.xhtml#idm45207113696304 primary="units"
secondary="base units in Prometheus"
data-type="indexterm"}[]{#ch03.xhtml#idm45207113695328
primary="base units" secondary="seconds as base unit for time"
data-type="indexterm"}are the base unit for time, and thus preferred in
Prometheus to other time units such as minutes, hours, days,
milliseconds, microseconds, and nanoseconds.

^[6](#ch03.xhtml#idm45207113574528-marker)^ In practice, there is not
much need for such a metric. The `timestamp` PromQL function will return
the timestamp of a sample, and the `time` PromQL function will return
the query evaluation time.

^[7](#ch03.xhtml#idm45207119139280-marker)^ System time can go backward
if the date is manually set in the kernel, or if a daemon is trying to
keep things in sync with the Network Time Protocol (NTP).

^[8](#ch03.xhtml#idm45207115781024-marker)^ The `+Inf` bucket is
required, and should never be dropped.

^[9](#ch03.xhtml#idm45207115776752-marker)^ Particularly if the
histogram has labels.

^[10](#ch03.xhtml#idm45207115727136-marker)^ However, it is not correct
to average a set of averages. For example, if you had 3 events with an
average of 5, and 4 events with an average of 6, the overall average
would not be 5 + 6 / 2 = 5.5, but rather (3 \* 5 + 4 \* 6) / [(3 +
4)]{.keep-together} = 5.57.

^[11](#ch03.xhtml#idm45207115716256-marker)^ Categories of logs were
mentioned in ["Logging"](#ch01.xhtml#logging){data-type="xref"}.

^[12](#ch03.xhtml#idm45207116496576-marker)^ You should not try dividing
the failures by the successes.

^[13](#ch03.xhtml#idm45207116487440-marker)^ This was roughly the
performance limit of Prometheus 1.x.

^[14](#ch03.xhtml#idm45207116486592-marker)^ This calculation is valid
for a Counter or a Gauge without labels.

^[15](#ch03.xhtml#idm45207116484480-marker)^
[Chapter 5](#ch05.xhtml#labels_chapter){data-type="xref"} looks at
labels, which are a powerful feature of Prometheus that make this
possible.

^[16](#ch03.xhtml#idm45207116458400-marker)^ As a general rule, ratios
typically go from 0...​1 and percentages go from 0...​100.

^[17](#ch03.xhtml#idm45207116453904-marker)^ At one point Prometheus
itself was using seconds, milliseconds, microseconds, and nanoseconds
for metrics.

^[18](#ch03.xhtml#idm45207116424704-marker)^ The heap is the memory of
your process that is dynamically allocated. It is used for memory
allocation by functions such as `malloc`.
:::
:::
:::

[]{#ch04.xhtml}

::: {#ch04.xhtml#sbo-rt-content}
::: {#ch04.xhtml#exposition_chapter .chapter}
# [Chapter 4. ]{.label}Exposition

In [Chapter 3](#ch03.xhtml#instrumentation_chapter){data-type="xref"} we
mainly focused on adding instrumentation to your code. But all the
instrumentation in the world isn't much use if the metrics produced
don't end up in your monitoring system. []{#ch04.xhtml#ix_expo
primary="exposition"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116411168 primary="metrics"
secondary="exposition to Prometheus" data-type="indexterm"}The process
of making metrics available to Prometheus is known as *exposition*.

Exposition to Prometheus is done over HTTP.
[]{#ch04.xhtml#idm45207116409296 primary="HTTP"
secondary="exposition to Prometheus over" data-type="indexterm"}Usually
you expose metrics under the */metrics* path, and the request is handled
for you by a client library. Prometheus supports two human-readable text
formats: the Prometheus text format and OpenMetrics.
[]{#ch04.xhtml#idm45207116407792 primary="exposition formats"
secondary="supported by Prometheus" data-type="indexterm"}You have the
option of producing the exposition format by hand, in which case it will
be easier with the Prometheus text format, which is less strict. You may
choose to do this if there is no suitable library for your language, but
it is recommended you use a library as it'll get all the little details
like escaping correct. Most of the libraries will also provide the
ability to produce metrics using both the OpenMetrics and Prometheus
text format.

Exposition is typically done either in your main function or another
top-level function and only needs to be configured once per application.

Metrics are usually registered with the *default registry* when you
define them. []{#ch04.xhtml#idm45207116405248 primary="default registry"
seealso="registry"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116404240
primary="registry"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116403568
primary="instrumentation"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116402896
primary="libraries" secondary="instrumentation" data-type="indexterm"}If
one of the libraries you are depending on has Prometheus
instrumentation, the metrics will be in the default registry and you
will gain the benefit of that additional instrumentation without having
to do anything. Some users prefer to explicitly pass a registry all the
way down from the main function, so you'd have to rely on every library
between your application's main function and the Prometheus
instrumentation being aware of the instrumentation. This presumes that
every library in the dependency chain cares about instrumentation and
agrees on the choice of instrumentation [libraries]{.keep-together}.

This design allows for instrumentation for Prometheus metrics with no
exposition at
all.^[1](#ch04.xhtml#idm45207116400608){#ch04.xhtml#idm45207116400608-marker
data-type="noteref"}^ In that case, aside from still paying the (tiny)
resource cost of instrumentation, there is no impact on your
application. If you are the one writing a library, you can add
instrumentation for your users using Prometheus without requiring extra
effort for your users who don't monitor. To better support this use
case, the instrumentation parts of client libraries try to minimize
their dependencies.

Let's take a look at exposition in some of the popular client libraries.
[]{#ch04.xhtml#idm45207116399536 primary="client libraries"
secondary="exposition in" see="exposition" data-type="indexterm"}We are
going to presume here that you know how to install the client libraries
and any other required dependencies.[]{#ch04.xhtml#ix_expoPy
primary="exposition" secondary="from Python client libraries"
secondary-sortas="Python" data-type="indexterm"}[]{#ch04.xhtml#ix_Pyexpo
primary="Python" secondary="exposition in client libraries"
data-type="indexterm"}

::: {.section pdf-bookmark="Python" data-type="sect1"}
::: {#ch04.xhtml#idm45207116395120 .sect1}
# Python

You have already seen `start_http_server` in
[Chapter 3](#ch03.xhtml#instrumentation_chapter){data-type="xref"}. It
starts up a background thread with an HTTP server that only serves
Prometheus metrics, as follows:

``` {code-language="python" data-type="programlisting"}
from prometheus_client import start_http_server

if __name__ == '__main__':
    start_http_server(8000)
    // Your code goes here.
```

`start_http_server` is very convenient to get up and running quickly.
[]{#ch04.xhtml#idm45207116385744 primary="start_http_server"
data-type="indexterm"}But it is likely that you already have an HTTP
server in your application that you would like your metrics to be served
from.

In Python there are various ways this can be done depending on which
frameworks you are using.

::: {.section pdf-bookmark="WSGI" data-type="sect2"}
::: {#ch04.xhtml#idm45207116352416 .sect2}
## WSGI

[Web Server Gateway Interface (WSGI)](https://oreil.ly/5B1tz) is a
Python standard for web applications.[]{#ch04.xhtml#idm45207116350480
primary="Python" secondary="exposition in client libraries"
tertiary="WSGI" data-type="indexterm"}[]{#ch04.xhtml#idm45207116349264
primary="WSGI (Web Server Gateway Interface)"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116348624
primary="Web Server Gateway Interface (WSGI)" data-type="indexterm"} The
Python client provides a WSGI app that you can use with your existing
WSGI code. In
[Example 4-1](#ch04.xhtml#exposition_wsgi){data-type="xref"}, the
`metrics_app` is delegated to by `my_app` if the */metrics* path is
requested; otherwise, it performs its usual
logic.[]{#ch04.xhtml#idm45207116375824 primary="metrics_app"
data-type="indexterm"} By chaining WSGI applications, you can add
middleware such as authentication, which client libraries do not offer
out of the box.

::: {#ch04.xhtml#exposition_wsgi data-type="example"}
##### [Example 4-1. ]{.label}Exposition using WSGI in Python

``` {code-language="python" data-type="programlisting"}
from prometheus_client import make_wsgi_app
from wsgiref.simple_server import make_server

metrics_app = make_wsgi_app()

def my_app(environ, start_fn):
    if environ['PATH_INFO'] == '/metrics':
        return metrics_app(environ, start_fn)
    start_fn('200 OK', [])
    return [b'Hello World']

if __name__ == '__main__':
    httpd = make_server('', 8000, my_app)
    httpd.serve_forever()
```
:::

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch04.xhtml#idm45207116323136 .sidebar}
# Does It Have to Be */metrics*?

*/metrics* is the HTTP path where Prometheus metrics are served by
convention, but it's just a convention, so you can put the metrics on
other paths.[]{#ch04.xhtml#idm45207116244304 primary="metrics"
secondary="/metrics path" secondary-sortas="metrics"
data-type="indexterm"} For example, if [*/metrics*]{.keep-together} is
already in use in your application or you want to put administrative
endpoints under an */admin/* prefix.

Even if it is on another path, it is still common to refer to such an
endpoint as [*your /metrics*]{.keep-together}.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Twisted" data-type="sect2"}
::: {#ch04.xhtml#idm45207116239904 .sect2}
## Twisted

[Twisted](https://twisted.org) is a Python event-driven network
engine.[]{#ch04.xhtml#idm45207116238016 primary="Twisted"
data-type="indexterm"}[]{#ch04.xhtml#idm45207116237312 primary="Python"
secondary="exposition in client libraries" tertiary="Twisted"
data-type="indexterm"} It supports WSGI so you can plug in
`make_wsgi_app`, as shown in
[Example 4-2](#ch04.xhtml#exposition_twisted){data-type="xref"}.

::: {#ch04.xhtml#exposition_twisted data-type="example"}
##### [Example 4-2. ]{.label}Exposition using Twisted in Python

``` {code-language="python" data-type="programlisting"}
from prometheus_client import make_wsgi_app
from twisted.web.server import Site
from twisted.web.wsgi import WSGIResource
from twisted.web.resource import Resource
from twisted.internet import reactor

metrics_resource = WSGIResource(
        reactor, reactor.getThreadPool(), make_wsgi_app())

class HelloWorld(Resource):
      isLeaf = False
      def render_GET(self, request):
          return b"Hello World"

root = HelloWorld()
root.putChild(b'metrics', metrics_resource)

reactor.listenTCP(8000, Site(root))
reactor.run()
```
:::
:::
:::

::: {.section pdf-bookmark="Multiprocess with Gunicorn" data-type="sect2"}
::: {#ch04.xhtml#multiprocess_gunicorn .sect2}
## Multiprocess with Gunicorn

Prometheus assumes that the applications it is monitoring are long-lived
and multithreaded.[]{#ch04.xhtml#ix_prcmlti primary="processes"
secondary="multiprocess with Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#ix_mltiprc
primary="multiprocess mode with Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#ix_Guni primary="Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#ix_Pyexpomlti primary="Python"
secondary="exposition in client libraries"
tertiary="multiprocess with Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#idm45207118325008 primary="CPython"
data-type="indexterm"} But this can fall apart a little with runtimes
such as
CPython.^[2](#ch04.xhtml#idm45207118324208){#ch04.xhtml#idm45207118324208-marker
data-type="noteref"}^ CPython is effectively limited to one processor
core due to the Global Interpreter Lock (GIL). To work around this, some
users spread the workload across multiple processes using a tool such as
[Gunicorn](https://gunicorn.org).

If you were to use the Python client library in the usual fashion, each
worker would track its own metrics. Each time Prometheus went to scrape
the application, it would randomly get the metrics from only one of the
workers, which would be only a fraction of the information and would
also have issues such as counters appearing to be going backward.
Workers can also be relatively short-lived.

The solution to this problem offered by the Python client is to have
each worker track its own metrics. At exposition time all the metrics of
all the workers are combined in a way that provides the semantics you
would get from a multithreaded application. There are some limitations
to the approach used: the `process_` metrics and custom collectors will
not be exposed, and the Pushgateway cannot be
used.^[3](#ch04.xhtml#idm45207118321744){#ch04.xhtml#idm45207118321744-marker
data-type="noteref"}^

Using Gunicorn, you need to let the client library know when a worker
process
exits.^[4](#ch04.xhtml#idm45207118320688){#ch04.xhtml#idm45207118320688-marker
data-type="noteref"}^ This is done in a config file like the one in
[Example 4-3](#ch04.xhtml#exposition_multiproc_config){data-type="xref"}.

::: {#ch04.xhtml#exposition_multiproc_config data-type="example"}
##### [Example 4-3. ]{.label}Gunicorn *config.py* to handle worker processes exiting

``` {code-language="python" data-type="programlisting"}
from prometheus_client import multiprocess

def child_exit(server, worker):
    multiprocess.mark_process_dead(worker.pid)
```
:::

You will also need an application to serve the
metrics.[]{#ch04.xhtml#idm45207118293824 primary="registry"
secondary="custom registry for multiprocess exposition with Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#idm45207118292976
primary="MultiProcessCollector"
data-type="indexterm"}[]{#ch04.xhtml#idm45207118272400
primary="custom registries" data-type="indexterm"} Gunicorn uses WSGI,
so you can use `make_wsgi_app`. You must create a *custom registry*
containing only a `MultiProcessCollector` for exposition, so that it
does not include both the multiprocess metrics and metrics from the
local default registry
([Example 4-4](#ch04.xhtml#exposition_multiproc_code){data-type="xref"}).

::: {#ch04.xhtml#exposition_multiproc_code data-type="example"}
##### [Example 4-4. ]{.label}Gunicorn application in *app.py*

``` {code-language="python" data-type="programlisting"}
from prometheus_client import multiprocess, make_wsgi_app, CollectorRegistry
from prometheus_client import Counter, Gauge

REQUESTS = Counter("http_requests_total", "HTTP requests")
IN_PROGRESS = Gauge("http_requests_inprogress", "Inprogress HTTP requests",
        multiprocess_mode='livesum')

@IN_PROGRESS.track_inprogress()
def app(environ, start_fn):
    REQUESTS.inc()
    if environ['PATH_INFO'] == '/metrics':
        registry = CollectorRegistry()
        multiprocess.MultiProcessCollector(registry)
        metrics_app = make_wsgi_app(registry)
        return metrics_app(environ, start_fn)
    start_fn('200 OK', [])
    return [b'Hello World']
```
:::

As you can see in
[Example 4-4](#ch04.xhtml#exposition_multiproc_code){data-type="xref"},
counters work normally, as do summaries and histograms. For gauges there
is additional optional configuration using `multiprocess_mode`.
[]{#ch04.xhtml#idm45207118202224
primary="multiprocess_mode configuration (gauges)"
data-type="indexterm"}[]{#ch04.xhtml#idm45207118201616 primary="gauges"
secondary="multiprocess_mode configuration" data-type="indexterm"}You
can configure the gauge based on how you intended to use it, as follows:

`all`

:   The default, which returns a time series from each process, whether
    it is alive or dead. This allows you to aggregate the series as you
    wish in PromQL. They will be distinguished by a `pid` label.

`liveall`

:   Returns a time series from each alive process.

`livesum`

:   Returns a single time series that is the sum of the value from each
    alive process. You would use this for things like in-progress
    requests or resource usage across all processes. A process might
    have aborted with a nonzero value, so dead processes are excluded.

`max`

:   Returns a single time series that is the maximum of the value from
    each alive or dead process. This is useful if you want to track the
    last time something happened, such as a request being processed,
    which could have been in a process that is now dead.

`min`

:   Returns a single time series that is the minimum of the value from
    each alive or dead process.

There is a small bit of setup before you can run Gunicorn, as shown in
[Example 4-5](#ch04.xhtml#exposition_gunicorn_run){data-type="xref"}.
You must set an environment variable called `prometheus_multiproc_dir`.
[]{#ch04.xhtml#idm45207118190640
primary="prometheus_multiproc_dir environment variable"
data-type="indexterm"}This points to an empty directory the client
library uses for tracking metrics. Before starting the application, you
should always wipe this directory to handle any potential changes to
your instrumentation.

::: {#ch04.xhtml#exposition_gunicorn_run data-type="example"}
##### [Example 4-5. ]{.label}Preparing the environment before starting Gunicorn with two workers

``` {data-type="programlisting"}
hostname $ export prometheus_multiproc_dir=$PWD/multiproc
hostname $ rm -rf $prometheus_multiproc_dir
hostname $ mkdir -p $prometheus_multiproc_dir
hostname $ gunicorn -w 2 -c config.py app:app
[2018-01-07 19:05:30 +0000] [9634] [INFO] Starting gunicorn 19.7.1
[2018-01-07 19:05:30 +0000] [9634] [INFO] Listening at: http://127.0.0.1:8000 (9634)
[2018-01-07 19:05:30 +0000] [9634] [INFO] Using worker: sync
[2018-01-07 19:05:30 +0000] [9639] [INFO] Booting worker with pid: 9639
[2018-01-07 19:05:30 +0000] [9640] [INFO] Booting worker with pid: 9640
```
:::

When you look at the */metrics* path, you will see the two defined
metrics, but `python_info` and the `process_` metrics will not be there.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch04.xhtml#idm45207114249536 .sidebar}
# Multiprocess Mode Under the Covers

Performance is vital for client libraries.
[]{#ch04.xhtml#idm45207114248128 primary="performance"
secondary="importance for client libraries" data-type="indexterm"}This
excludes designs where work processes send UDP packets or any other use
of networks, due to the system call overhead it would involve. What is
needed is something that is about as fast as normal instrumentation,
which means something that is as fast as local process memory but can be
accessed by other processes.

The approach taken is to use `mmap`. []{#ch04.xhtml#idm45207114246160
primary="mmap utility" data-type="indexterm"}Each process has its own
set of mmaped files where it tracks its own metrics. At exposition time
all the files are read and the metrics combined. There is no locking
between the instrumentation writing to the files and the exposition
reading it to ensure isolation metric values are aligned in memory and a
two-phase write is used when adding a new time series.

Counters (including summaries and histograms) must not go backward, so
files relating to counters are kept after a worker
exits.[]{#ch04.xhtml#idm45207114244912 primary="counters"
secondary="multiprocess mode and" data-type="indexterm"} Whether this
makes sense for a gauge depends on how it is used. For a metric like
in-progress requests, you only want it from live processes, whereas for
the last time a request was processed, you want the maximum across both
live and dead processes. This can be configured on a per-gauge basis.
:::

```{=html}
</aside>
```
::: {data-type="tip"}
###### Tip

Each process creates several files that must be read at exposition time
in `prometheus_multiproc_dir`. If your workers stop and start a lot,
this can make exposition slow when you have thousands of
files.[]{#ch04.xhtml#idm45207114242128
primary="prometheus_multiproc_dir environment variable"
data-type="indexterm"}

It is not safe to delete individual files as that could cause counters
to incorrectly go backward, but you can either try to reduce the churn
(for example, by increasing or removing a limit on the number of
requests workers handle before
exiting^[5](#ch04.xhtml#idm45207114240976){#ch04.xhtml#idm45207114240976-marker
data-type="noteref"}^), or regularly restarting the application and
wiping the files.
:::

These steps are for Gunicorn. The same approach also works with other
Python multiprocess setups, such as using the `multiprocessing` module.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch04.xhtml#idm45207114238608 .sidebar}
# OpenMetrics Support

The Python client library natively produces an OpenMetrics format.
Prometheus always prefers OpenMetrics when it is
available.[]{#ch04.xhtml#idm45207114237024 primary="OpenMetrics"
secondary="support for format by Python client library"
data-type="indexterm"} Prometheus uses an `Accept` HTTP Header to
indicate that it supports scraping the OpenMetrics
format.[]{#ch04.xhtml#idm45207114235472 primary="cURL utility"
data-type="indexterm"} You can simulate this behavior with the `-H`
option when using `curl`:

``` {data-type="programlisting"}
curl -v -H 'Accept: application/openmetrics-text; version=1.0.0;
            charset=utf-8' http://127.0.0.1:8000/metrics
```
:::

```{=html}
</aside>
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Go" data-type="sect1"}
::: {#ch04.xhtml#go_exposition .sect1}
# Go

In Go, `http.Handler` is the standard interface for providing HTTP
handlers, and `promhttp.Handler` provides that interface for the Go
client library.[]{#ch04.xhtml#idm45207114229936
primary="promhttp.Handler"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114229232
primary="processes" secondary="multiprocess with Gunicorn"
startref="ix_prcmlti"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114228048
primary="multiprocess mode with Gunicorn" startref="ix_mltiprc"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114227088
primary="Gunicorn" startref="ix_Guni"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114226144 primary="Python"
secondary="exposition in client libraries" startref="ix_Pyexpomlti"
tertiary="multiprocess with Gunicorn"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114224624
primary="exposition" secondary="from Python client libraries"
secondary-sortas="Python" startref="ix_expoPy"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114223120 primary="Python"
secondary="exposition in client libraries" startref="ix_Pyexpo"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114221888 primary="Go"
secondary="exposition from client libraries"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114220928
primary="exposition" secondary="from Go client libraries"
secondary-sortas="Go"
data-type="indexterm"}[]{#ch04.xhtml#idm45207114219696
primary="instrumentation" secondary="Go program demonstrating"
data-type="indexterm"} To demonstrate how this works, place the code in
[Example 4-6](#ch04.xhtml#exposition_go){data-type="xref"} in a file
called *example.go*.

::: {#ch04.xhtml#exposition_go data-type="example"}
##### [Example 4-6. ]{.label}A simple Go program demonstrating instrumentation and exposition

``` {code-language="go" data-type="programlisting"}
package main

import (
  "log"
  "net/http"

  "github.com/prometheus/client_golang/prometheus"
  "github.com/prometheus/client_golang/prometheus/promauto"
  "github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
  requests = promauto.NewCounter(
    prometheus.CounterOpts{
      Name: "hello_worlds_total",
      Help: "Hello Worlds requested.",
    })
)

func handler(w http.ResponseWriter, r *http.Request) {
  requests.Inc()
  w.Write([]byte("Hello World"))
}

func main() {
  http.HandleFunc("/", handler)
  http.Handle("/metrics", promhttp.Handler())
  log.Fatal(http.ListenAndServe(":8000", nil))
}
```
:::

You can fetch dependencies and run this code in the usual way:

``` {data-type="programlisting"}
hostname $ go get -d -u github.com/prometheus/client_golang/prometheus
hostname $ go run example.go
```

This example uses `promauto`, which will automatically register your
metric with the default registry. []{#ch04.xhtml#idm45207103404576
primary="promauto"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103403872
primary="registry" secondary="registration with Go client library"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103402960
primary="MustRegister function"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103402288
primary="NewCounter" data-type="indexterm"}If you do not wish to do so,
you can use `prometheus.NewCounter` instead and then use `MustRegister`
in an `init` function:

``` {data-type="programlisting"}
func init() {
  prometheus.MustRegister(requests)
}
```

This is a bit more fragile, as it is easy for you to create and use the
metric but forget the `MustRegister` call.
:::
:::

::: {.section pdf-bookmark="Java" data-type="sect1"}
::: {#ch04.xhtml#idm45207103398576 .sect1}
# Java

The Java client library is also known as the *simpleclient*.
[]{#ch04.xhtml#idm45207103396576 primary="simpleclient (Java)"
data-type="indexterm"}[]{#ch04.xhtml#ix_expoJava primary="exposition"
secondary="from Java client libraries" secondary-sortas="Java"
data-type="indexterm"}[]{#ch04.xhtml#ix_Javaexpo primary="Java"
secondary="exposition in client libraries" data-type="indexterm"}It
replaced the *original client*, which was developed before many of the
current practices and guidelines around how to write a client library
were established. The Java client should be used for any instrumentation
for languages running on a Java Virtual Machine
(JVM).[]{#ch04.xhtml#idm45207103392656 primary="instrumentation"
secondary="for languages running on JVM" secondary-sortas="languages"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103391440
primary="JVM (Java Virtual Machine)" data-type="indexterm"}

::: {.section .less_space .pagebreak-before pdf-bookmark="HTTPServer" data-type="sect2"}
::: {#ch04.xhtml#idm45207103390800 .sect2}
## HTTPServer

Similar to `start_http_server` in Python, the `HTTPServer`
class[]{#ch04.xhtml#idm45207103388128 primary="HttpServer class"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103387392 primary="Java"
secondary="exposition in client libraries" tertiary="HttpServer class"
data-type="indexterm"} in the Java client gives you an easy way to get
up and running
([Example 4-7](#ch04.xhtml#exposition_java_httpserver){data-type="xref"}).

::: {#ch04.xhtml#exposition_java_httpserver data-type="example"}
##### [Example 4-7. ]{.label}A simple Java program demonstrating instrumentation and exposition

``` {code-language="java" data-type="programlisting"}
import io.prometheus.client.Counter;
import io.prometheus.client.hotspot.DefaultExports;
import io.prometheus.client.exporter.HTTPServer;

public class Example {
  private static final Counter myCounter = Counter.build()
      .name("my_counter_total")
      .help("An example counter.").register();

  public static void main(String[] args) throws Exception {
    DefaultExports.initialize();
    HTTPServer server = new HTTPServer(8000);
    while (true) {
      myCounter.inc();
      Thread.sleep(1000);
    }
  }
}
```
:::

You should generally have Java metrics as class static fields, so that
they are only registered once.[]{#ch04.xhtml#idm45207103295376
primary="metrics" secondary="from Java client libraries"
secondary-sortas="Java" data-type="indexterm"}

The call to `DefaultExports.initialize` is needed for the various
`process` and `jvm` metrics to work. You should generally call it once
in all of your Java applications, such as in the main function.
[]{#ch04.xhtml#idm45207103214704 primary="DefaultExports.initialize"
data-type="indexterm"}However, `DefaultExports.initialize` is idempotent
and thread safe, so additional calls are harmless.

In order to run the code in
[Example 4-7](#ch04.xhtml#exposition_java_httpserver){data-type="xref"},
you will need the simpleclient dependencies.
[]{#ch04.xhtml#idm45207103212160 primary="dependencies"
secondary="simpleclient in Java" data-type="indexterm"}If you are using
Maven,
[Example 4-8](#ch04.xhtml#exposition_java_httpserver_pom){data-type="xref"}
is what the `dependencies` in your *pom.xml* should look like.

::: {#ch04.xhtml#exposition_java_httpserver_pom .less_space .pagebreak-before data-type="example"}
##### [Example 4-8. ]{.label}*pom.xml* dependencies for [Example 4-7](#ch04.xhtml#exposition_java_httpserver){data-type="xref"}

``` {code-language="xml" data-type="programlisting"}
  <dependencies>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient</artifactId>
      <version>0.16.0</version>
    </dependency>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient_hotspot</artifactId>
      <version>0.16.0</version>
    </dependency>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient_httpserver</artifactId>
      <version>0.16.0</version>
    </dependency>
  </dependencies>
```
:::
:::
:::

::: {.section pdf-bookmark="Servlet" data-type="sect2"}
::: {#ch04.xhtml#idm45207103119920 .sect2}
## Servlet

Many Java and JVM frameworks support using subclasses of *HttpServlet*
in their HTTP servers and middleware.[]{#ch04.xhtml#idm45207103131264
primary="servlets"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103130560 primary="Java"
secondary="exposition in client libraries" tertiary="servlet"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103129376
primary="HttpServlet class"
data-type="indexterm"}[]{#ch04.xhtml#idm45207103128704
primary="MetricsServlet class" data-type="indexterm"} Jetty is one such
server, and you can see how to use the Java client's `MetricsServlet` in
[Example 4-9](#ch04.xhtml#exposition_java_servlet){data-type="xref"}.

::: {#ch04.xhtml#exposition_java_servlet data-type="example"}
##### [Example 4-9. ]{.label}A Java program demonstrating exposition using `MetricsServlet` and Jetty

``` {code-language="java" data-type="programlisting"}
import io.prometheus.client.Counter;
import io.prometheus.client.exporter.MetricsServlet;
import io.prometheus.client.hotspot.DefaultExports;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
import javax.servlet.ServletException;
import org.eclipse.jetty.server.Server;
import org.eclipse.jetty.servlet.ServletContextHandler;
import org.eclipse.jetty.servlet.ServletHolder;
import java.io.IOException;


public class Example {
  static class ExampleServlet extends HttpServlet {
    private static final Counter requests = Counter.build()
        .name("hello_worlds_total")
        .help("Hello Worlds requested.").register();

    @Override
    protected void doGet(final HttpServletRequest req,
        final HttpServletResponse resp)
        throws ServletException, IOException {
      requests.inc();
      resp.getWriter().println("Hello World");
    }
  }

  public static void main(String[] args) throws Exception {
      DefaultExports.initialize();

      Server server = new Server(8000);
      ServletContextHandler context = new ServletContextHandler();
      context.setContextPath("/");
      server.setHandler(context);
      context.addServlet(new ServletHolder(new ExampleServlet()), "/");
      context.addServlet(new ServletHolder(new MetricsServlet()), "/metrics");

      server.start();
      server.join();
  }
}
```
:::

You will also need to specify the Java client as a
dependency.[]{#ch04.xhtml#idm45207102950352 primary="dependencies"
secondary="Java servlet client library" data-type="indexterm"} If you
are using Maven, this will look like
[Example 4-10](#ch04.xhtml#exposition_java_servlet_pom){data-type="xref"}.

::: {#ch04.xhtml#exposition_java_servlet_pom data-type="example"}
##### [Example 4-10. ]{.label}*pom.xml* dependencies for [Example 4-9](#ch04.xhtml#exposition_java_servlet){data-type="xref"}

``` {code-language="xml" data-type="programlisting"}
  <dependencies>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient</artifactId>
      <version>0.16.0</version>
    </dependency>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient_hotspot</artifactId>
      <version>0.16.0</version>
    </dependency>
    <dependency>
      <groupId>io.prometheus</groupId>
      <artifactId>simpleclient_servlet</artifactId>
      <version>0.16.0</version>
    </dependency>
    <dependency>
      <groupId>org.eclipse.jetty</groupId>
      <artifactId>jetty-servlet</artifactId>
      <version>11.0.11</version>
    </dependency>
  </dependencies>
```
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Pushgateway" data-type="sect1"}
::: {#ch04.xhtml#pushgateway .sect1}
# Pushgateway

Batch jobs are typically run on a regular schedule, such as hourly or
daily. They start up, do some work, and then
exit.[]{#ch04.xhtml#idm45207102622848 primary="exposition"
secondary="from Java client libraries" secondary-sortas="Java"
startref="ix_expoJava"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102613120 primary="Java"
secondary="exposition in client libraries" startref="ix_Javaexpo"
data-type="indexterm"}[]{#ch04.xhtml#ix_pshgt primary="Pushgateway"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102610992
primary="batch jobs" data-type="indexterm"}[]{#ch04.xhtml#ix_expobatch
primary="exposition" secondary="from batch jobs using Pushgateway"
secondary-sortas="batch" data-type="indexterm"} As they are not
continuously running, Prometheus can't exactly scrape
them.^[6](#ch04.xhtml#idm45207102608736){#ch04.xhtml#idm45207102608736-marker
data-type="noteref"}^ This is where the *Pushgateway* comes in.

The
Pushgateway^[7](#ch04.xhtml#idm45207102607216){#ch04.xhtml#idm45207102607216-marker
data-type="noteref"}^ is a metrics cache for service-level batch jobs.
[]{#ch04.xhtml#idm45207102606080 primary="batch jobs"
secondary="service-level, Pushgateway metrics cache for"
data-type="indexterm"}Its architecture is shown in
[Figure 4-1](#ch04.xhtml#pgw_architecture_diagram){data-type="xref"}. It
remembers only the last push that you make to it for each batch
job.[]{#ch04.xhtml#idm45207102572720 primary="pgw" see="Pushgateway"
data-type="indexterm"} You use it by having your batch jobs push their
metrics just before they exit. Prometheus scrapes these metrics from
your Pushgateway and you can then alert and graph them. Usually you run
a Pushgateway beside a Prometheus.

<figure>
<div id="ch04.xhtml#pgw_architecture_diagram" class="figure">
<img src="assets/pur2_0401.png" width="600" height="71"
alt="Pushgateway architecture diagram" />
<h6><span class="label">Figure 4-1. </span>The Pushgateway
architecture</h6>
</div>
</figure>

A service-level batch job is one where there isn't really an `instance`
label to apply to it. []{#ch04.xhtml#idm45207102569232
primary="instance labels" secondary="service-level batch jobs and"
data-type="indexterm"}That is to say it applies to all of one of your
services, rather than being innately tied to one machine or process
instance.^[8](#ch04.xhtml#idm45207102568256){#ch04.xhtml#idm45207102568256-marker
data-type="noteref"}^ If you don't particularly care where a batch job
runs but do care that it happens (even if it happens to currently be set
up to run via cron on one machine), it is a service-level batch job.
Examples include a per-datacenter batch job to check for bad machines,
or one that performs garbage collection across a whole service.

::: {.note data-type="note"}
###### Note

The Pushgateway is not a way to convert Prometheus from pull to push.
If, for example, there are several pushes between one Prometheus scrape
and the next, the Pushgateway will only return the last push for that
batch job. This is discussed further in ["Networks and
Authentication"](#ch21.xhtml#prometheus_network){data-type="xref"}.
:::

You can download the Pushgateway from the [Prometheus download
page](https://oreil.ly/hoXpK). It is an exporter that runs by default on
port 9091, and Prometheus should be set up to scrape it. However, you
should also provide the `honor_labels: true` setting in the *scrape
config*, as shown in
[Example 4-11](#ch04.xhtml#pushgateway_scrape_config){data-type="xref"}.
This is because the metrics you push to the Pushgateway should not have
an `instance` label, and you do not want the Pushgateway's own
`instance` target label to end up on the metrics when Prometheus scrapes
them.^[9](#ch04.xhtml#idm45207102561296){#ch04.xhtml#idm45207102561296-marker
data-type="noteref"}^ `honor_labels` is discussed in ["Label Clashes and
honor_labels"](#ch08.xhtml#honor_labels){data-type="xref"}.

::: {#ch04.xhtml#pushgateway_scrape_config data-type="example"}
##### [Example 4-11. ]{.label}*prometheus.yml* scrape config for a local Pushgateway

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: pushgateway
   honor_labels: true
   static_configs:
    - targets:
      - localhost:9091
```
:::

You can use client libraries to push to the Pushgateway.
[Example 4-12](#ch04.xhtml#pushgateway_python){data-type="xref"} shows
the structure you would use for a Python batch
job.[]{#ch04.xhtml#idm45207102522848 primary="Python"
secondary="exposition from batch jobs using Pushgateway"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102522000
primary="registry" secondary="custom registry for Python client library"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102521120
primary="custom registries" data-type="indexterm"} A *custom registry*
is created so that only the specific metrics you choose are pushed. The
duration of the batch job is always
pushed,^[10](#ch04.xhtml#idm45207102519776){#ch04.xhtml#idm45207102519776-marker
data-type="noteref"}^ and the time it ended is pushed only if the job is
successful.[]{#ch04.xhtml#idm45207102518672 primary="push_to_gateway"
data-type="indexterm"}

There are three different ways you can write to the Pushgateway. In
Python these are the `push_to_gateway`, `pushadd_to_gateway`, and
`delete_from_gateway` functions:

`push`

:   Any existing metrics for this job are removed and the pushed metrics
    added. This uses the PUT HTTP method under the covers.

`pushadd`

:   The pushed metrics override existing metrics with the same metric
    names for this job.[]{#ch04.xhtml#idm45207102512992
    primary="pushadd_to_gateway" data-type="indexterm"} Any metrics that
    previously existed with different metric names remain unchanged.
    This uses the POST HTTP method under the covers.

`delete`

:   The metrics for this job are removed. This uses the DELETE HTTP
    method under the covers.[]{#ch04.xhtml#idm45207102510640
    primary="delete_from_gateway" data-type="indexterm"}

As [Example 4-12](#ch04.xhtml#pushgateway_python){data-type="xref"} is
using `pushadd_to_gateway`, the value of `my_job_duration_seconds` will
always get replaced. However, `my_job_last_success_seconds`\# will only
get replaced if there are no exceptions; it is added to the registry and
then pushed.[]{#ch04.xhtml#idm45207102507168 primary="instrumentation"
secondary="Python batch job and pushing its metrics to Pushgateway"
data-type="indexterm"}

::: {#ch04.xhtml#pushgateway_python data-type="example"}
##### [Example 4-12. ]{.label}Instrumenting a batch job and pushing its metrics to a Pushgateway

``` {code-language="python" data-type="programlisting"}
from prometheus_client import CollectorRegistry, Gauge, pushadd_to_gateway

registry = CollectorRegistry()
duration = Gauge('my_job_duration_seconds',
        'Duration of my batch job in seconds', registry=registry)
try:
    with duration.time():
        # Your code here. 
        pass

    # This only runs if there wasn't an exception. 
    g = Gauge('my_job_last_success_seconds',
            'Last time my batch job successfully finished', registry=registry)
    g.set_to_current_time()
finally:
    pushadd_to_gateway('localhost:9091', job='batch', registry=registry)
```
:::

You can see pushed data on the status page, as
[Figure 4-2](#ch04.xhtml#pushgateway_status){data-type="xref"} shows. An
additional metric `push_time_seconds` has been added by the Pushgateway
because Prometheus will always use the time at which it scrapes as the
timestamp of the Pushgateway metrics. `push_time_seconds` gives you a
way to know the actual time the data was last
pushed.[]{#ch04.xhtml#idm45207102420432 primary="metrics"
secondary="Pushgateway showing from Python batch job"
data-type="indexterm"} Another metric, `push_failure_time_seconds`, has
been introduced, which represents the last time when an update to this
group in the Pushgateway failed.

<figure>
<div id="ch04.xhtml#pushgateway_status" class="figure">
<img src="assets/pur2_0402.png" width="600" height="337"
alt="Pushgateway Status page." />
<h6><span class="label">Figure 4-2. </span>The Pushgateway status page
showing metrics from a push</h6>
</div>
</figure>

You might have noticed in
[Figure 4-2](#ch04.xhtml#pushgateway_status){data-type="xref"} that the
push is referred to as a *group*. []{#ch04.xhtml#idm45207102361328
primary="groups" data-type="indexterm"}[]{#ch04.xhtml#idm45207102360720
primary="job labels"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102360112 primary="labels"
secondary="grouping key for metrics from push to Pushgateway"
data-type="indexterm"}You can provide labels in addition to the `job`
label when pushing, and all of these labels are known as the *grouping
key*. []{#ch04.xhtml#idm45207102358368 primary="grouping keys"
data-type="indexterm"}In Python this can be provided with the
`grouping_key` keyword argument. You would use this if a batch job was
sharded or split up somehow. For example, if you have 30 database shards
and each had its own batch job, you might distinguish them with a
`shard` label.[]{#ch04.xhtml#idm45207102356672 primary="shard labels"
data-type="indexterm"}

::: {data-type="tip"}
###### Tip

Once pushed, groups stay forever in the Pushgateway. You should avoid
using grouping keys that vary from one batch job run to the next, as
this will make the metrics difficult to work with and cause performance
issues. When decommissioning a batch job, don't forget to delete its
metrics from the Pushgateway.[]{#ch04.xhtml#idm45207102355120
primary="exposition" secondary="from batch jobs using Pushgateway"
startref="ix_expobatch"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102353904
primary="Pushgateway" startref="ix_pshgt" data-type="indexterm"}
:::
:::
:::

::: {.section pdf-bookmark="Bridges" data-type="sect1"}
::: {#ch04.xhtml#idm45207102624288 .sect1}
# Bridges

Prometheus client libraries are not limited to outputting metrics in the
Prometheus format. []{#ch04.xhtml#idm45207102351312 primary="bridges"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102350608
primary="exposition" secondary="using bridges"
data-type="indexterm"}There is a separation of concerns between
instrumentation and exposition so that you can process the metrics in
any way you like.

For example, the Go, Python, and Java clients each include a *Graphite
bridge*. []{#ch04.xhtml#idm45207102348704 primary="Graphite"
secondary="bridge" data-type="indexterm"}A bridge takes metrics output
from the client library registry and outputs it to something other than
Prometheus. So the Graphite bridge will convert the metrics into a form
that Graphite can
understand^[11](#ch04.xhtml#idm45207102347568){#ch04.xhtml#idm45207102347568-marker
data-type="noteref"}^ and write them out to Graphite, as shown in
[[Example 4-13](#ch04.xhtml#python_graphite_bridge){data-type="xref"}]{.keep-together}.

::: {#ch04.xhtml#python_graphite_bridge data-type="example"}
##### [Example 4-13. ]{.label}Using the Python `GraphiteBridge` to push to Graphite every 10 seconds

``` {code-language="python" data-type="programlisting"}
import time
from prometheus_client.bridge.graphite import GraphiteBridge

gb = GraphiteBridge(['graphite.your.org', 2003])
gb.start(10)
while True:
    time.sleep(1)
```
:::

This works because the registry has a method that allows you to get a
snapshot of all the current metrics.[]{#ch04.xhtml#idm45207102310432
primary="registry" secondary="metrics pushed to Graphite using a bridge"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102309584
primary="CollectorRegistry.metricFamilySamples"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102308976 primary="Java"
secondary="CollectorRegistry.metricFamilySamples"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102308128
primary="CollectorRegistry.collect" data-type="indexterm"} This is
`CollectorRegistry.collect` in Python,
`CollectorRegistry.metricFamilySamples` in Java, and `Registry.Gather`
in Go. This is the method that HTTP exposition uses, and you can use it
too.[]{#ch04.xhtml#idm45207102306112 primary="Gather method"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102305408
primary="Registry.Gather"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102304736 primary="Go"
secondary="Registry.Gather"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102303792
primary="instrumentation"
secondary="feeding data into non-Prometheus library"
data-type="indexterm"} For example, you could use this method to feed
data into another non-Prometheus instrumentation
library.^[12](#ch04.xhtml#idm45207102302624){#ch04.xhtml#idm45207102302624-marker
data-type="noteref"}^

::: {data-type="tip"}
###### Tip

If you ever want to hook into direct instrumentation, you should instead
use the metrics output by a registry. Wanting to know every time a
counter is incremented does not make sense in terms of a metrics-based
monitoring system. However, the count of increments is already provided
for you by `CollectorRegistry.collect` and works for custom collectors.
:::
:::
:::

::: {.section pdf-bookmark="Parsers" data-type="sect1"}
::: {#ch04.xhtml#idm45207102299616 .sect1}
# Parsers

In addition to a client library's registry allowing you to access metric
output, the
Go^[13](#ch04.xhtml#idm45207102298208){#ch04.xhtml#idm45207102298208-marker
data-type="noteref"}^ and Python clients also feature a parser for the
Prometheus and OpenMetrics exposition
formats.[]{#ch04.xhtml#idm45207102297408 primary="exposition formats"
secondary="parsers in Python client libraries"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102296448 primary="parsers"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102295776
primary="exposition" secondary="from Python client libraries"
secondary-sortas="Python" tertiary="parsers"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102294272
primary="text format (Prometheus)" secondary="parsers for"
data-type="indexterm"}
[Example 4-14](#ch04.xhtml#python_parser){data-type="xref"} only prints
the samples, but you could feed Prometheus metrics into other monitoring
systems or into your local tooling.

::: {#ch04.xhtml#python_parser data-type="example"}
##### [Example 4-14. ]{.label}Parsing the Prometheus text format with the Python client

``` {code-language="python" data-type="programlisting"}
from prometheus_client.parser import text_string_to_metric_families

for family in text_string_to_metric_families(u"counter_total 1.0\n"):
  for sample in family.samples:
    print("Name: {0} Labels: {1} Value: {2}".format(*sample))
```
:::

DataDog, InfluxDB, Sensu, and
Metricbeat^[14](#ch04.xhtml#idm45207102219840){#ch04.xhtml#idm45207102219840-marker
data-type="noteref"}^ are some of the monitoring systems that have
components that can parse the text
format.[]{#ch04.xhtml#idm45207102219104
primary="monitoring systems (other)"
secondary="having parsers for Prometheus text format"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102218256
primary="Elasticsearch" data-type="indexterm"} Using one of these
monitoring systems, you could take advantage of the Prometheus ecosystem
without ever running the Prometheus server. We believe that this is a
good thing, as there is currently a lot of duplication of effort between
the various monitoring systems. Each of them has to write similar code
to support the myriad custom metric outputs provided by the most
commonly used software.
:::
:::

::: {.section pdf-bookmark="Text Exposition Format" data-type="sect1"}
::: {#ch04.xhtml#exposition_format .sect1}
# Text Exposition Format

The Prometheus text exposition format is relatively easy to produce and
parse.[]{#ch04.xhtml#ix_expfmtPrm primary="exposition formats"
secondary="Prometheus text format"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102214048
primary="text format (Prometheus)" seealso="exposition formats"
data-type="indexterm"} Although you should almost always rely on a
client library to handle it for you, there are cases such as with the
Node Exporter textfile collector (discussed in ["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}) where you
may have to produce it yourself.

We will be showing you version 0.0.4 of the text format, which has the
content type header:

``` {data-type="programlisting"}
Content-Type: text/plain; version=0.0.4; charset=utf-8
```

In the simplest cases, the text format is just the name of the metric
followed by a 64-bit floating-point number. Each line is terminated with
a line-feed character (`\n`):

``` {data-type="programlisting"}
my_counter_total 14
a_small_gauge 8.3e-96
```

::: {.section pdf-bookmark="Metric Types" data-type="sect2"}
::: {#ch04.xhtml#idm45207102183040 .sect2}
## Metric Types

More complete Prometheus text format output would
[]{#ch04.xhtml#idm45207102181648 primary="exposition formats"
secondary="Prometheus text format" tertiary="metric types"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102180048 primary="metrics"
secondary="types in Prometheus text format"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102179136
primary="HELP (metrics)"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102178464
primary="TYPE (metrics)" data-type="indexterm"}include the `HELP` and
`TYPE` of the metrics, as shown in
[Example 4-15](#ch04.xhtml#format_example){data-type="xref"}. `HELP` is
a description of what the metric is, and should not generally change
from scrape to scrape. `TYPE` is one of `counter`, `gauge`, `summary`,
`histogram`, or `untyped`. `untyped` is used when you do not know the
type of the metric, and is the default if no type is
specified.[]{#ch04.xhtml#idm45207102172224
primary="untyped (metric type)" data-type="indexterm"} It is invalid for
you to have a duplicate metric, so make sure all the time series that
belong to a metric are grouped together.[]{#ch04.xhtml#idm45207102171392
primary="gauges" secondary="text exposition format for"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102170480
primary="counters" secondary="text exposition format for"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102169568 primary="summary"
secondary="text exposition format for"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102168656
primary="histograms" secondary="text exposition format for"
data-type="indexterm"}

::: {#ch04.xhtml#format_example data-type="example"}
##### [Example 4-15. ]{.label}Exposition format for a gauge, counter, summary, and histogram

``` {data-type="programlisting"}
# HELP example_gauge An example gauge
# TYPE example_gauge gauge
example_gauge -0.7
# HELP my_counter_total An example counter
# TYPE my_counter_total counter
my_counter_total 14
# HELP my_summary An example summary
# TYPE my_summary summary
my_summary_sum 0.6
my_summary_count 19
# HELP latency_seconds An example histogram
# TYPE latency_seconds histogram
latency_seconds_bucket{le="0.1"} 7 
latency_seconds_bucket{le="0.2"} 18
latency_seconds_bucket{le="0.4"} 24
latency_seconds_bucket{le="0.8"} 28
latency_seconds_bucket{le="+Inf"} 29
latency_seconds_sum 0.6
latency_seconds_count 29 
```
:::

[![1](assets/1.png){height="12" width="12"}](#ch04.xhtml#co_exposition_CO1-1){#ch04.xhtml#callout_exposition_CO1-1 .co}

:   For histograms, the `le` labels have floating-point values and must
    be sorted. You should note how the histogram buckets are cumulative,
    as `le` stands for less than or equal
    to.[]{#ch04.xhtml#idm45207102158864 primary="le labels"
    data-type="indexterm"}

[![2](assets/2.png){height="12" width="12"}](#ch04.xhtml#co_exposition_CO1-2){#ch04.xhtml#callout_exposition_CO1-2 .co}

:   The `_count` must match the `+Inf` bucket, and the `+Inf` bucket
    must always be present. Buckets should not change from scrape to
    scrape, as this will cause problems for PromQL's
    `histogram_quantile` function.[]{#ch04.xhtml#idm45207102153920
    primary="histogram_quantile function" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Labels" data-type="sect2"}
::: {#ch04.xhtml#idm45207102152784 .sect2}
## Labels

The histogram in the preceding example also shows how labels are
represented. Multiple labels are separated by commas, and it is OK to
have a trailing comma before the closing
brace.[]{#ch04.xhtml#idm45207102151168 primary="exposition formats"
secondary="Prometheus text format" tertiary="labels"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102149920 primary="labels"
secondary="in Prometheus text exposition format"
secondary-sortas="Prometheus" data-type="indexterm"}

The ordering of labels does not matter, but it is a good idea to have
the ordering consistent from scrape to scrape. This will make writing
your unit tests easier, and consistent ordering ensures the best
ingestion performance in Prometheus.

Here is an example[]{#ch04.xhtml#idm45207102147888 primary="summary"
secondary="example in Prometheus text exposition format"
data-type="indexterm"} of a summary in text format:

``` {data-type="programlisting"}
# HELP my_summary An example summary
# TYPE my_summary summary
my_summary_sum{foo="bar",baz="quu"} 1.8
my_summary_count{foo="bar",baz="quu"} 453
my_summary_sum{foo="blaa",baz=""} 0
my_summary_count{foo="blaa",baz="quu"} 0
```

It is possible to have a metric with no time series, if no children have
been initialized, as discussed in
["Child"](#ch05.xhtml#child){data-type="xref"}:

``` {data-type="programlisting"}
# HELP a_counter_total An example counter
# TYPE a_counter_total counter
```
:::
:::

::: {.section pdf-bookmark="Escaping" data-type="sect2"}
::: {#ch04.xhtml#idm45207102143680 .sect2}
## Escaping

The text exposition format is encoded in UTF-8, and full
UTF-8^[15](#ch04.xhtml#idm45207102142128){#ch04.xhtml#idm45207102142128-marker
data-type="noteref"}^ is permitted in both `HELP` and label values.
[]{#ch04.xhtml#idm45207102141024 primary="UTF-8 encoding"
data-type="indexterm"}Thus you need to use backslashes to escape
characters that would cause issues using
backslashes.[]{#ch04.xhtml#idm45207102140048
primary="exposition formats" secondary="Prometheus text format"
tertiary="escaping characters in"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102138832
primary="escaping characters in exposition format"
data-type="indexterm"} For `HELP` this is line feeds and backslashes.
[]{#ch04.xhtml#idm45207102137520 primary="labels"
secondary="exposition format, escaping characters in values"
data-type="indexterm"}For label values this is line feeds, backslashes,
and double
quotes.^[16](#ch04.xhtml#idm45207102136384){#ch04.xhtml#idm45207102136384-marker
data-type="noteref"}^ The format ignores extra whitespace.

Here is an example demonstrating escaping in the text exposition format:

``` {data-type="programlisting"}
# HELP escaping A newline \\n and backslash \\ escaped
# TYPE escaping gauge
escaping{foo="newline \\n backslash \\ double quote \" "} 1
```
:::
:::

::: {.section pdf-bookmark="Timestamps" data-type="sect2"}
::: {#ch04.xhtml#format_timestamps .sect2}
## Timestamps

It is possible to specify a timestamp on a time
series.[]{#ch04.xhtml#idm45207102132256 primary="timestamps"
secondary="in Prometheus text exposition format"
secondary-sortas="Prometheus"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102130992
primary="exposition formats" secondary="Prometheus text format"
tertiary="timestamps"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102129776 primary="Unix"
secondary="epoch" data-type="indexterm"} It is an integer value in
milliseconds since the Unix
epoch,^[17](#ch04.xhtml#idm45207102128704){#ch04.xhtml#idm45207102128704-marker
data-type="noteref"}^ and it goes after the value. Timestamps in the
exposition format should generally be avoided as they are only
applicable in certain limited use cases (such as federation) and come
with limitations. Timestamps for scrapes are usually applied
automatically by Prometheus. It is not defined as to what happens if you
specify multiple lines with the same name and labels but different
timestamps.

This gauge has[]{#ch04.xhtml#idm45207102127664 primary="gauges"
secondary="timestamp in text exposition format" data-type="indexterm"} a
timestamp:

``` {data-type="programlisting"}
# HELP foo I'm trapped in a client library
# TYPE foo gauge
foo 1 15100992000000
```

::: {.warning data-type="warning"}
###### Warning

Timestamps are expressed in *milliseconds* since epoch in the Prometheus
text format, while in OpenMetrics they are expressed in *seconds* since
epoch.[]{#ch04.xhtml#idm45207102123936 primary="OpenMetrics"
secondary="timestamps" data-type="indexterm"}
:::
:::
:::

::: {.section pdf-bookmark="check metrics" data-type="sect2"}
::: {#ch04.xhtml#idm45207102122672 .sect2}
## check metrics

Prometheus 2.0 uses a custom parser for
efficiency.[]{#ch04.xhtml#idm45207102121168 primary="exposition formats"
secondary="Prometheus text format"
tertiary="using promtool check metrics"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102119856
primary="check metrics" data-type="indexterm"} So, just because a
*/metrics* endpoint can be scraped doesn't mean that the metrics are
compliant with the format.[]{#ch04.xhtml#idm45207102118640
primary="promtool" secondary="check metrics" data-type="indexterm"}

*Promtool* is a utility included with Prometheus that among other things
can verify that your metric output is valid and perform lint checks:

``` {data-type="programlisting"}
curl http://localhost:8000/metrics | promtool check metrics
```

Common mistakes include forgetting the line feed on the last line, using
carriage return and line feed rather than just line
feed,^[18](#ch04.xhtml#idm45207102115888){#ch04.xhtml#idm45207102115888-marker
data-type="noteref"}^ and invalid metric or label names. As a brief
reminder, metric and label names cannot contain hyphens, and cannot
start with a number.

You now have a working knowledge of the text format. You can find the
full specification in the official [Prometheus
documentation](https://oreil.ly/20X3R).
:::
:::
:::
:::

::: {.section pdf-bookmark="OpenMetrics" data-type="sect1"}
::: {#ch04.xhtml#idm45207102216672 .sect1}
# OpenMetrics

The OpenMetrics format is similar to the Prometheus text exposition
format but contains several incompatible changes with the Prometheus
text format.[]{#ch04.xhtml#idm45207102111280
primary="exposition formats" secondary="Prometheus text format"
startref="ix_expfmtPrm" data-type="indexterm"}[]{#ch04.xhtml#ix_expfmtOM
primary="exposition formats" secondary="OpenMetrics"
data-type="indexterm"}[]{#ch04.xhtml#ix_OpnM primary="OpenMetrics"
data-type="indexterm"} Even if they look similar, for a given set of
metrics, the output they generate would generally be different.

We will be showing you version 1.0.0 of the OpenMetrics format, which
has the content type header:

``` {data-type="programlisting"}
Content-Type: application/openmetrics-text; version=1.0.0; charset=utf-8
```

In the simplest cases, the text format is just the name of the metric
followed by a 64-bit floating-point number. Each line is terminated with
a line-feed character (`\n`). The file is terminated by `# EOF`:

``` {data-type="programlisting"}
my_counter_total 14
a_small_gauge 8.3e-96
# EOF
```

::: {.section pdf-bookmark="Metric Types" data-type="sect2"}
::: {#ch04.xhtml#idm45207102104000 .sect2}
## Metric Types

The metric types supported by the Prometheus text exposition format are
also supported in OpenMetrics.[]{#ch04.xhtml#idm45207102102208
primary="OpenMetrics" secondary="metric types"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102101232
primary="exposition formats" secondary="OpenMetrics"
tertiary="metric types"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102100016 primary="metrics"
secondary="types in OpenMetrics format" data-type="indexterm"} In
addition to counters, gauges, summaries, and histograms, specific types
have been added: StateSet, GaugeHistograms, and
Info.[]{#ch04.xhtml#idm45207102098880 primary="StateSets"
data-type="indexterm"}

StateSets represent a series of related boolean values, also called a
bitset. A value of `1` means `true` and `0` means `false`.

GaugeHistograms measure current distributions. The difference with
histograms is that buckets values and sum can go up and
down.[]{#ch04.xhtml#idm45207102095504 primary="GaugeHistograms"
data-type="indexterm"}

Info metrics are used to expose textual information that does not change
during process lifetime.[]{#ch04.xhtml#idm45207102094304
primary="info metrics" data-type="indexterm"} An application's version,
revision control commit, and the version of a compiler are good
candidates. The value of these metrics is always
1.[]{#ch04.xhtml#idm45207102093472 primary="HELP (metrics)"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102092800
primary="TYPE (metrics)"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102092128
primary="UNIT metadata (metrics)" data-type="indexterm"}

In addition to `HELP` and `TYPE`, metric families in OpenMetrics have an
optional `UNIT` metadata that specifies a metric's unit.

All the types are demonstrated in
[Example 4-16](#ch04.xhtml#omformat_example){data-type="xref"}.

::: {#ch04.xhtml#omformat_example data-type="example"}
##### [Example 4-16. ]{.label}Exposition format for different types of metrics

``` {data-type="programlisting"}
# HELP example_gauge An example gauge
# TYPE example_gauge gauge
example_gauge -0.7
# HELP my_counter An example counter
# TYPE my_counter counter
my_counter_total 14
my_counter_created 1.640991600123e+09
# HELP my_summary An example summary
# TYPE my_summary summary
my_summary_sum 0.6
my_summary_count 19
# HELP latency_seconds An example histogram
# TYPE latency_seconds histogram
# UNIT latency_seconds seconds
latency_seconds_bucket{le="0.1"} 7
latency_seconds_bucket{le="0.2"} 18
latency_seconds_bucket{le="0.4"} 24
latency_seconds_bucket{le="0.8"} 28
latency_seconds_bucket{le="+Inf"} 29
latency_seconds_sum 0.6
latency_seconds_count 29
# TYPE my_build_info info
my_build_info{branch="HEAD",version="0.16.0rc1"} 1.0
# TYPE my_stateset stateset
# HELP my_stateset An example stateset
my_stateset{feature="a"} 1
my_stateset{feature="b"} 0
# TYPE my_gaugehistogram gaugehistogram
# HELP my_gaugehistogram An example gaugehistogram
my_gaugehistogram_bucket{le="1.0"} 0
my_gaugehistogram_bucket{le="+Inf"} 3
my_gaugehistogram_gcount 3
my_gaugehistogram_gsum 2
# EOF
```
:::

In OpenMetrics, as shown in
[Example 4-16](#ch04.xhtml#omformat_example){data-type="xref"},
GaugeHistograms use distinct `_gcount` and `_gsum` suffixes for counts
and sums, differentiating them from Histograms' `_count` and `_sum`.
:::
:::

::: {.section pdf-bookmark="Labels" data-type="sect2"}
::: {#ch04.xhtml#idm45207102082080 .sect2}
## Labels

The Histogram and GaugeHistogram in the preceding example also showed
how labels are represented.[]{#ch04.xhtml#idm45207102080640
primary="labels" secondary="in OpenMetrics format"
secondary-sortas="OpenMetrics"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102079392
primary="exposition formats" secondary="OpenMetrics" tertiary="labels"
data-type="indexterm"} Multiple labels are separated by commas, but
unlike in the Prometheus wire format, commas before the closing brace
are not allowed in [OpenMetrics]{.keep-together}.
:::
:::

::: {.section pdf-bookmark="Timestamps" data-type="sect2"}
::: {#ch04.xhtml#idm45207102077008 .sect2}
## Timestamps

It is possible to specify a timestamp on a time
series.[]{#ch04.xhtml#idm45207102075408 primary="timestamps"
secondary="in OpenMetrics format" secondary-sortas="OpenMetrics"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102074160
primary="OpenMetrics" secondary="timestamps"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102073216
primary="exposition formats" secondary="OpenMetrics"
tertiary="timestamps" data-type="indexterm"} It is a float value in
seconds []{#ch04.xhtml#idm45207102071872 primary="Unix"
secondary="epoch" data-type="indexterm"}since the Unix
epoch,^[19](#ch04.xhtml#idm45207102070800){#ch04.xhtml#idm45207102070800-marker
data-type="noteref"}^ and it goes after the value, as shown in this
example:

``` {data-type="programlisting"}
# HELP foo I'm trapped in a client library
# TYPE foo gauge
foo 1 1.5100992e9
```

::: {.warning data-type="warning"}
###### Warning

Timestamps are expressed in *seconds* since epoch in OpenMetrics, while
in the Prometheus text format they are expressed in *milliseconds* since
epoch.
:::

You now have a working knowledge of the OpenMetrics format. You can find
the full specification in [the OpenMetrics GitHub
repository](https://oreil.ly/EUEZa).

We have mentioned labels a few times now. In the following chapter
you'll learn what they are in detail.[]{#ch04.xhtml#idm45207102065344
primary="OpenMetrics" startref="ix_OpnM"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102064368
primary="exposition formats" secondary="OpenMetrics"
startref="ix_expfmtOM"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102063152
primary="exposition" startref="ix_expo" data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch04.xhtml#idm45207116400608-marker)^ No exposition means that the
metrics are not scraped by a Prometheus server.

^[2](#ch04.xhtml#idm45207118324208-marker)^ CPython is the official name
of the standard Python implementation. Do not confuse it with Cython,
which can be used to write C extensions in Python.

^[3](#ch04.xhtml#idm45207118321744-marker)^ The Pushgateway is not
suitable for this use case, so this is not a problem in practice.

^[4](#ch04.xhtml#idm45207118320688-marker)^ `child_exit` was added in
Gunicorn version 19.7 released in March 2017.

^[5](#ch04.xhtml#idm45207114240976-marker)^ Gunicorn's `--max-requests`
flag is one example of such a limit.

^[6](#ch04.xhtml#idm45207102608736-marker)^ Though for batch jobs that
take more than a few minutes to run, it may also make sense to scrape
them normally over HTTP to help debug performance issues.

^[7](#ch04.xhtml#idm45207102607216-marker)^ You may see it referenced as
*pgw* in informal contexts.

^[8](#ch04.xhtml#idm45207102568256-marker)^ For batch jobs such as
database backups that are tied to a machine's lifecycle, the Node
Exporter textfile collector is a better choice. This is discussed in
["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}.

^[9](#ch04.xhtml#idm45207102561296-marker)^ The Pushgateway explicitly
exports empty `instance` labels for metrics without an `instance` label.
Combined with `honor_labels: true`, this results in Prometheus not
applying an `instance` label to these metrics. Usually, empty labels and
missing labels are the same thing in Prometheus, but this is the
exception.

^[10](#ch04.xhtml#idm45207102519776-marker)^ Just like summaries and
histograms, gauges have a *time* function decorator and context manager.
It is intended only for use in batch jobs.

^[11](#ch04.xhtml#idm45207102347568-marker)^ The labels are flattened
into the metric name.[]{#ch04.xhtml#idm45207102346976 primary="Python"
secondary="exposition using Graphite bridge"
data-type="indexterm"}[]{#ch04.xhtml#idm45207102346032 primary="labels"
secondary="Graphite bridge" data-type="indexterm"} Tag (i.e., label)
support for Graphite was only recently added in 1.1.0.

^[12](#ch04.xhtml#idm45207102302624-marker)^ This works both ways. Other
instrumentation libraries with an equivalent feature can have their
metrics fed into a Prometheus client library. This is discussed in
["Custom Collectors"](#ch12.xhtml#custom_collectors){data-type="xref"}.

^[13](#ch04.xhtml#idm45207102298208-marker)^ The Go client's parser is
the reference implementation.

^[14](#ch04.xhtml#idm45207102219840-marker)^ Part of the Elasticsearch
stack.

^[15](#ch04.xhtml#idm45207102142128-marker)^ The null byte is a valid
UTF-8 character.

^[16](#ch04.xhtml#idm45207102136384-marker)^ Yes, there are two
different sets of escaping rules within the text format. In OpenMetrics,
this has been unified to just one rule, as double quotes must be escaped
in `HELP` as well.

^[17](#ch04.xhtml#idm45207102128704-marker)^ Midnight January 1st 1970
UTC.

^[18](#ch04.xhtml#idm45207102115888-marker)^ `\r\n` is the line ending
on Windows, while on Unix, `\n` is used. Prometheus has a Unix heritage,
so it uses `\n`.

^[19](#ch04.xhtml#idm45207102070800-marker)^ Midnight January 1st 1970
UTC.
:::
:::
:::

[]{#ch05.xhtml}

::: {#ch05.xhtml#sbo-rt-content}
::: {#ch05.xhtml#labels_chapter .chapter}
# [Chapter 5. ]{.label}Labels

Labels are a key part of Prometheus, and one of the things that make it
powerful.[]{#ch05.xhtml#ix_lbls primary="labels" data-type="indexterm"}
In this chapter you will learn what labels are, where they come from,
and how you can add them to your own metrics.

::: {.section pdf-bookmark="What Are Labels?" data-type="sect1"}
::: {#ch05.xhtml#idm45207102059200 .sect1}
# What Are Labels?

Labels are key-value pairs associated with time series that, in addition
to the metric name, uniquely identify
them.[]{#ch05.xhtml#idm45207102057600 primary="labels" secondary="about"
data-type="indexterm"} That's a bit of a mouthful, so let's look at an
example.

If you had a metric for HTTP []{#ch05.xhtml#idm45207102056032
primary="HTTP requests" secondary="metrics on broken out by path"
data-type="indexterm"}[]{#ch05.xhtml#idm45207102054992 primary="paths"
secondary="HTTP requests broken out by" data-type="indexterm"}requests
that was broken out by path, you might try putting the path in the
metric name, such as is common in
Graphite:^[1](#ch05.xhtml#idm45207102053776){#ch05.xhtml#idm45207102053776-marker
data-type="noteref"}^

``` {data-type="programlisting"}
http_requests_login_total
http_requests_logout_total
http_requests_adduser_total
http_requests_comment_total
http_requests_view_total
```

These metrics would be difficult for you to work with in PromQL. In
order to calculate the total requests, you would either need to know
every possible HTTP path or do some form of potentially expensive
matching across all metric names. Accordingly, this is an antipattern
you should avoid. Instead, to handle this common use case, Prometheus
has labels.[]{#ch05.xhtml#idm45207102051968 primary="path labels"
data-type="indexterm"} In the preceding case you might use a `path`
label:

``` {data-type="programlisting"}
http_requests_total{path="/login"}
http_requests_total{path="/logout"}
http_requests_total{path="/adduser"}
http_requests_total{path="/comment"}
http_requests_total{path="/view"}
```

You can then work with the `http_requests_total` metric with all its
`path` labels as one. With PromQL you could get an overall aggregated
request rate, the rate of just one of the paths, or what proportion each
request is of the whole.

You can also have metrics with more than one label. There is no ordering
on labels, so you can aggregate by any given label while ignoring the
others, or even aggregate by several of the labels at once.
:::
:::

::: {.section pdf-bookmark="Instrumentation and Target Labels" data-type="sect1"}
::: {#ch05.xhtml#idm45207102048032 .sect1}
# Instrumentation and Target Labels

Labels come from two sources, *instrumentation labels* and *target
labels*.[]{#ch05.xhtml#idm45207102045600 primary="labels"
secondary="instrumentation and target" data-type="indexterm"} When you
are working in PromQL there is no difference between the two, but it's
important to distinguish between them in order to get the most benefits
from labels.

Instrumentation labels, as the name indicates, come from your
instrumentation. They are about things that are known inside your
application or library, such as the type of HTTP requests it receives,
which databases it talks to, and other internal specifics.

Target labels identify a specific monitoring target; that is, a target
that Prometheus scrapes. []{#ch05.xhtml#idm45207102043488
primary="target labels" data-type="indexterm"}A target label relates
more to your architecture and may include which application it is, what
datacenter it lives in, if it is in a development or production
environment, which team owns it, and of course, which exact instance of
the application it is. Target labels are attached by Prometheus as part
of the process of scraping metrics.

Different Prometheus servers run by different teams may have different
views of what a "team," "region," or "service" is, so an instrumented
application should not try to expose such labels itself. Accordingly,
you will not find any features in client libraries to add
labels^[2](#ch05.xhtml#idm45207102042272){#ch05.xhtml#idm45207102042272-marker
data-type="noteref"}^ across all metrics of a target. Target labels come
from service discovery and
relabeling^[3](#ch05.xhtml#idm45207102041472){#ch05.xhtml#idm45207102041472-marker
data-type="noteref"}^ and are discussed further in
[Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="Instrumentation" data-type="sect1"}
::: {#ch05.xhtml#idm45207102038320 .sect1}
# Instrumentation

Let's extend
[Example 3-3](#ch03.xhtml#simple_program_one_counter){data-type="xref"}
to use a label. []{#ch05.xhtml#ix_lblsinstr primary="labels"
secondary="instrumentation"
data-type="indexterm"}[]{#ch05.xhtml#ix_instrlbl
primary="instrumentation labels" data-type="indexterm"}In
[Example 5-1](#ch05.xhtml#simple_labels){data-type="xref"} you can see
`labelnames=['path']` in the
definition,^[4](#ch05.xhtml#idm45207102031872){#ch05.xhtml#idm45207102031872-marker
data-type="noteref"}^ indicating that your metric has a single label
called `path`. When using the metric in instrumentation you must add a
call to the `labels` method with []{#ch05.xhtml#idm45207102029264
primary="Python" secondary="application using label for counter metric"
data-type="indexterm"}[]{#ch05.xhtml#idm45207102028160
primary="labels method"
data-type="indexterm"}[]{#ch05.xhtml#idm45207102027488 primary="Go"
secondary="WithLabelValues"
data-type="indexterm"}[]{#ch05.xhtml#idm45207102026544
primary="WithLabelValues"
data-type="indexterm"}[]{#ch05.xhtml#idm45207102025872 primary="Java"
secondary="labels method" data-type="indexterm"}an argument for the
label
value.^[5](#ch05.xhtml#idm45207102024752){#ch05.xhtml#idm45207102024752-marker
data-type="noteref"}^

::: {#ch05.xhtml#simple_labels data-type="example"}
##### [Example 5-1. ]{.label}A Python application using a label for a counter metric

``` {code-language="python" data-type="programlisting"}
import http.server
from prometheus_client import start_http_server, Counter

REQUESTS = Counter('hello_worlds_total',
        'Hello Worlds requested.',
        labelnames=['path'])

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        REQUESTS.labels(self.path).inc()
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")

if __name__ == "__main__":
    start_http_server(8000)
    server = http.server.HTTPServer(('localhost', 8001), MyHandler)
    server.serve_forever()
```
:::

If you visit *http://localhost:8001/* and *http://localhost:8001/foo*,
then on the */metrics* page at *http://localhost:8000/metrics* you will
see the time series for each of the paths:

``` {data-type="programlisting"}
# HELP hello_worlds_total Hello Worlds requested.
# TYPE hello_worlds_total counter
hello_worlds_total{path="/favicon.ico"} 6.0
hello_worlds_total{path="/"} 4.0
hello_worlds_total{path="/foo"} 1.0
```

Label names are limited in terms of what characters you can use.
[]{#ch05.xhtml#idm45207101961360 primary="labels" secondary="naming"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101960384
primary="naming labels" data-type="indexterm"}They should begin with a
letter (a--z or A--Z) and be followed with letters, numbers, and
underscores. This is the same as for metric names, except without
colons.

Unlike metric names, label names are not generally namespaced. However,
you should take care when defining instrumentation labels to avoid
labels likely to be used as target labels, such as `env`, `cluster`,
`service`, `team`, `zone`, and `region`. We also recommend avoiding
`type` as a label name, as it is very generic. Snake case is the
convention for label names.

The `instance` and `job` label names are used natively by Prometheus, so
we don't recommend them either, as they will collide with the target
labels.

Label values can be any UTF-8
characters.[]{#ch05.xhtml#idm45207101868432 primary="UTF-8 encoding"
secondary="label values" data-type="indexterm"} You can also have an
empty label value, but this can be a little confusing in the Prometheus
server as at first glance it looks the same as not having that label.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch05.xhtml#name_label .sidebar}
# Reserved Labels and \_\_name\_\_

Labels can start with underscores, but you should avoid such labels.
Label names beginning with a double underscore `__` are reserved.

Internally in Prometheus the metric name is just another label called
`__name__`.^[6](#ch05.xhtml#idm45207101864256){#ch05.xhtml#idm45207101864256-marker
data-type="noteref"}^ The expression `up` is syntactic sugar for
`{__name__="up"}`, and there are also special semantics with PromQL
operators, as discussed in ["Vector
Matching"](#ch15.xhtml#vector_matching){data-type="xref"}.
:::

```{=html}
</aside>
```
::: {.section pdf-bookmark="Metric" data-type="sect2"}
::: {#ch05.xhtml#idm45207101861328 .sect2}
## Metric

As you may have noticed, the word *metric* is a bit ambiguous and means
different things depending on context.[]{#ch05.xhtml#idm45207101859408
primary="instrumentation labels" secondary="metric"
data-type="indexterm"} It could refer to a
[]{#ch05.xhtml#idm45207101858304 primary="metric family"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101857600
primary="time series"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101856928
primary="child metrics" data-type="indexterm"}metric family, a child, or
a time series:

``` {data-type="programlisting"}
# HELP latency_seconds Latency in seconds.
# TYPE latency_seconds summary
latency_seconds_sum{path="/foo"} 1.0
latency_seconds_count{path="/foo"} 2.0
latency_seconds_sum{path="/bar"} 3.0
latency_seconds_count{path="/bar"} 4.0
```

`latency_seconds_sum{path="/bar"}` is a time series, distinguished by a
name and labels. This is what PromQL works with.

`latency_seconds{path="/bar"}` is a child, and is what the return value
of `labels()` in the Python client represents. For a summary it contains
both the `_sum` and `_count` time series with those labels.

`latency_seconds` is a metric family. It is only the metric name and its
associated type. This is the metric definition when using a client
library.

For a gauge metric with no labels, the metric family, child, and time
series are the same.
:::
:::

::: {.section pdf-bookmark="Multiple Labels" data-type="sect2"}
::: {#ch05.xhtml#idm45207101851328 .sect2}
## Multiple Labels

You can specify any number of labels when
[]{#ch05.xhtml#idm45207101849760 primary="instrumentation labels"
secondary="multiple labels for a metric"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101848768
primary="multiple labels for a metric" data-type="indexterm"}defining a
metric, and then the values in the same order in the `labels` call
([Example 5-2](#ch05.xhtml#labels_python_multi){data-type="xref"}).

::: {#ch05.xhtml#labels_python_multi .less_space data-type="example"}
##### [Example 5-2. ]{.label}`hello_worlds_total` has `path` and `method` labels

``` {code-language="python" data-type="programlisting"}
REQUESTS = Counter('hello_worlds_total',
        'Hello Worlds requested.',
        labelnames=['path', 'method'])

class MyHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        REQUESTS.labels(self.path, self.command).inc()
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello World")
```
:::

Python and Go also allow you to supply a map with both label names and
values, though the label names must still match those in the metric
definitions. This can make it harder to mix up the order of your
arguments, but if that is a real risk, then you may have too many
labels.

It is not possible to have varying label names for a metric, and client
libraries will prevent it. When working with metrics it is important
that you know what labels you have in play, so you must know your label
names in advance when doing direct instrumentation. If you don't know
your labels, you probably want a logs-based monitoring tool for that
specific use case instead.
:::
:::

::: {.section pdf-bookmark="Child" data-type="sect2"}
::: {#ch05.xhtml#child .sect2}
## Child

The value returned to you by the `labels` method in Python is called a
*child*.[]{#ch05.xhtml#ix_instrchld primary="instrumentation labels"
secondary="child" data-type="indexterm"}[]{#ch05.xhtml#ix_chld
primary="child metrics"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101793616
primary="labels method" data-type="indexterm"} You can store this child
for later use, which saves you from having to look it up at each
instrumentation event, saving time in performance-critical code that is
called hundreds of thousands of times a second. In benchmarks with the
Java client, we have found that with no contention the child lookup took
30 ns, while the actual increment took [12
ns]{.keep-together}.^[7](#ch05.xhtml#idm45207101792160){#ch05.xhtml#idm45207101792160-marker
data-type="noteref"}^

A common pattern, when an object refers to only one child of a metric,
is to call `labels` once and then store that in the object, as shown in
[Example 5-3](#ch05.xhtml#labels_child_python){data-type="xref"}.

::: {#ch05.xhtml#labels_child_python data-type="example"}
##### [Example 5-3. ]{.label}A simple Python cache that stores the child in each named cache

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Counter

FETCHES = Counter('cache_fetches_total',
        'Fetches from the cache.',
        labelnames=['cache'])

class MyCache(object):
    def __init__(self, name):
        self._fetches = FETCHES.labels(name)
        self._cache = {}

    def fetch(self, item):
        self._fetches.inc()
        return self._cache.get(item)

    def store(self, item, value):
        self._cache[item] = value
```
:::

Another place where you will run into children is in initializing them.
Children only appear on the */metrics* page after you call
`labels`.^[8](#ch05.xhtml#idm45207101627728){#ch05.xhtml#idm45207101627728-marker
data-type="noteref"}^ This can cause issues in PromQL, as time series
that appear and disappear can be very challenging to work with.
Accordingly, where possible you should initialize children at startup,
such as in
[Example 5-4](#ch05.xhtml#labels_child_python_init){data-type="xref"},
although if you follow the pattern in
[Example 5-3](#ch05.xhtml#labels_child_python){data-type="xref"}, you
get this for free.

::: {#ch05.xhtml#labels_child_python_init data-type="example"}
##### [Example 5-4. ]{.label}Initializing children of a metric at application startup

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Counter

REQUESTS = Counter('http_requests_total',
        'HTTP requests.',
        labelnames=['path'])
REQUESTS.labels('/foo')
REQUESTS.labels('/bar')
```
:::

When using Python decorators, you may also use `labels` without
immediately calling a method on the return value, as shown in
[Example 5-5](#ch05.xhtml#labels_child_decorator_labels){data-type="xref"}.

::: {#ch05.xhtml#labels_child_decorator_labels data-type="example"}
##### [Example 5-5. ]{.label}Using a decorator with labels in Python

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Summary

LATENCY = Summary('http_requests_latency_seconds',
        'HTTP request latency.',
        labelnames=['path'])

foo = LATENCY.labels('/foo')
@foo.time()
def foo_handler(params):
    pass
```
:::

::: {.note data-type="note"}
###### Note

Client libraries usually offer methods to remove children from a metric.
You should only consider using these for unit tests. From a PromQL
semantic standpoint, once a child exists it should continue to exist
until the process dies, otherwise functions such as `rate` may return
undesirable results.[]{#ch05.xhtml#idm45207101499328
primary="child metrics" startref="ix_chld" data-type="indexterm"} These
methods also invalidate previous values returned from `labels`.
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Aggregating" data-type="sect1"}
::: {#ch05.xhtml#idm45207102037824 .sect1}
# Aggregating

Now that your instrumentation is bursting with labels, let's actually
use them in PromQL.[]{#ch05.xhtml#idm45207101496128
primary="instrumentation labels" secondary="child"
startref="ix_instrchld"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101494880 primary="labels"
secondary="instrumentation" startref="ix_lblsinstr"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101485824
primary="instrumentation labels" startref="ix_instrlbl"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101484976
primary="aggregation"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101484368 primary="labels"
secondary="aggregating with" data-type="indexterm"} We will be going
into more detail about aggregation operators in
[Chapter 14](#ch14.xhtml#promql_aggregation_chapter){data-type="xref"},
but want to give you a taste of the power of labels now.

In [Example 5-2](#ch05.xhtml#labels_python_multi){data-type="xref"},
`hello_worlds_total` has `path` and `method`
labels.[]{#ch05.xhtml#idm45207101480064 primary="method labels"
data-type="indexterm"} As `hello_worlds_total` is a counter, you must
first use the `rate` function.
[Table 5-1](#ch05.xhtml#labels_rate_output){data-type="xref"} is one
possible output, showing results for two application instances with
different HTTP paths and methods.

  ------------------------------------------------------------------------ ----
  {job=\"myjob",instance=\"localhost:1234",path=\"/foo",method=\"GET\"}    1
  {job=\"myjob",instance=\"localhost:1234",path=\"/foo",method=\"POST\"}   2
  {job=\"myjob",instance=\"localhost:1234",path=\"/bar",method=\"GET\"}    4
  {job=\"myjob",instance=\"localhost:5678",path=\"/foo",method=\"GET\"}    8
  {job=\"myjob",instance=\"localhost:5678",path=\"/foo",method=\"POST\"}   16
  {job=\"myjob",instance=\"localhost:5678",path=\"/bar",method=\"GET\"}    32
  ------------------------------------------------------------------------ ----

  : [Table 5-1. ]{.label}Output of `rate(hello_worlds_total[5m])`

This can be a little hard for you to consume, especially if you have far
more time series than in this simple example. Let's start by aggregating
away the `path` label.[]{#ch05.xhtml#idm45207101465056
primary="path labels" secondary="aggregating away"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101464048 primary="sum"
secondary="without clause"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101463104
primary="without clause" secondary="sum without" data-type="indexterm"}
You do this using the `sum` aggregation, as you want to add samples
together. The `without` clause indicates what label you want to remove.
This gives you the expression
`sum without(path)(rate(hello_worlds_total[5m]))` that produces the
output in
[Table 5-2](#ch05.xhtml#labels_without_path){data-type="xref"}.

  ----------------------------------------------------------- ----
  {job=\"myjob",instance=\"localhost:1234",method=\"GET\"}    5
  {job=\"myjob",instance=\"localhost:1234",method=\"POST\"}   2
  {job=\"myjob",instance=\"localhost:5678",method=\"GET\"}    40
  {job=\"myjob",instance=\"localhost:5678",method=\"POST\"}   16
  ----------------------------------------------------------- ----

  : [Table 5-2. ]{.label}Output of
  `sum without(path)(rate(hello_worlds_total[5m]))`

It is not uncommon for you to have tens or hundreds of instances, and in
our experience, looking at individual instances on dashboards breaks
down somewhere around three to five.[]{#ch05.xhtml#idm45207101450880
primary="instance labels" secondary="including in sum without"
data-type="indexterm"} You can expand the `without` clause to include
the `instance` label, which gives the output shown in
[Table 5-3](#ch05.xhtml#labels_without_path_instance){data-type="xref"}.
As you would expect from the values in
[Table 5-1](#ch05.xhtml#labels_rate_output){data-type="xref"}, 1 + 4 +
8 + 32 = 45 requests per second for `GET` and 2 + 16 = 18 requests per
second for `POST`.

  -------------------------------- ----
  {job=\"myjob",method=\"GET\"}    45
  {job=\"myjob",method=\"POST\"}   18
  -------------------------------- ----

  : [Table 5-3. ]{.label}Output of
  `sum without(path, instance)(rate(hello_worlds_total[5m]))`

Labels are not ordered[]{#ch05.xhtml#idm45207101440976
primary="method labels" secondary="removing using sum without"
data-type="indexterm"} in any way, so just as you can remove `path` you
can also remove `method`, as seen in
[Table 5-4](#ch05.xhtml#labels_without_method_instance){data-type="xref"}.

  ------------------------------ ----
  {job=\"myjob",path=\"/foo\"}   27
  {job=\"myjob",path=\"/bar\"}   36
  ------------------------------ ----

  : [Table 5-4. ]{.label}Output of
  `sum without(method, instance)(rate(hello_worlds_total[5m]))`

::: {.note data-type="note"}
###### Note

There is also a `by` clause that keeps only the labels you
specify.[]{#ch05.xhtml#idm45207101431344 primary="by clause"
secondary="sum by"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101430336 primary="sum"
secondary="by clause" data-type="indexterm"} `without` is preferred
because if there are additional labels such as `env` or `region` across
all of a job, they will not be lost. This helps when you are sharing
your rules with others.
:::
:::
:::

::: {.section pdf-bookmark="Label Patterns" data-type="sect1"}
::: {#ch05.xhtml#label_patterns .sect1}
# Label Patterns

Prometheus only supports 64-bit floating-point numbers as time series
values, not any other data types such as strings.
[]{#ch05.xhtml#ix_lblspatt primary="labels" secondary="patterns in"
data-type="indexterm"}But label values are strings, and there are
certain limited use cases where it is OK to (ab)use them without getting
too far into logs-based monitoring.

::: {.section pdf-bookmark="Enum" data-type="sect2"}
::: {#ch05.xhtml#enum_metrics .sect2}
## Enum

The first common case for strings is
*enums*.[]{#ch05.xhtml#idm45207101421936 primary="labels"
secondary="patterns in" tertiary="enum"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101420656 primary="enums"
data-type="indexterm"} For example, you may have a resource that could
be in exactly one of the states of `STARTING`, `RUNNING`, `STOPPING`, or
[`TERMINATED`]{.keep-together}.

You could expose this as a gauge with `STARTING` being 0, `RUNNING`
being 1, `STOPPING` being 2, and `TERMINATED` being
3.^[9](#ch05.xhtml#idm45207101415424){#ch05.xhtml#idm45207101415424-marker
data-type="noteref"}^ []{#ch05.xhtml#idm45207101414640 primary="gauges"
secondary="used as enum, custom collector for" data-type="indexterm"}But
this is a bit tricky to work with in PromQL. The numbers 0--3 are a bit
opaque, and there is not a single expression you can write to tell you
what proportion of the time your resource spent `STARTING`.

The solution to this is to add a label for the state to the gauge, with
each potential state becoming a child. When exposing a boolean value in
Prometheus, you should use 1 for true and 0 for
false.[]{#ch05.xhtml#idm45207101412848 primary="boolean values"
data-type="indexterm"} Accordingly, one of the children will have the
value 1 and all the others 0, which would produce metrics like those in
[Example 5-6](#ch05.xhtml#labels_enum_output){data-type="xref"}.

::: {#ch05.xhtml#labels_enum_output data-type="example"}
##### [Example 5-6. ]{.label}An enum example; the `blaa` resource is in the `RUNNING` state

``` {data-type="programlisting"}
# HELP gauge The current state of resources.
# TYPE gauge resource_state
resource_state{resource_state="STARTING",resource="blaa"} 0
resource_state{resource_state="RUNNING",resource="blaa"} 1
resource_state{resource_state="STOPPING",resource="blaa"} 0
resource_state{resource_state="TERMINATED",resource="blaa"} 0
```
:::

Because the 0s are always present, the PromQL expression
`avg_over_time​(resource_state[1h])` would give you the proportion of
time spent in each state. You could also aggregate by `resource_state`
using `sum without(resource)(resource_state)` to see how many resources
are in each state.[]{#ch05.xhtml#idm45207101406480
primary="avg_over_time"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101405808 primary="sum"
secondary="without clause" data-type="indexterm"}

To produce such metrics you could use `set` on a gauge, but that would
bring with it race conditions. []{#ch05.xhtml#idm45207101404016
primary="sets" data-type="indexterm"}A scrape might see a 1 on either
zero or two of the states, depending on when exactly it happened. You
need some isolation so that the gauge isn't exposed in the middle of an
update.[]{#ch05.xhtml#idm45207101403104 primary="custom collectors"
see="collectors" data-type="indexterm"}

The solution to this is to use a *custom collector*, which will be
discussed further in ["Custom
Collectors"](#ch12.xhtml#custom_collectors){data-type="xref"}.
[]{#ch05.xhtml#idm45207101400432 primary="collectors" secondary="custom"
data-type="indexterm"}To give you an idea of how to go about this, you
can find a basic implementation in
[Example 5-7](#ch05.xhtml#labels_enum_python){data-type="xref"}. In
reality you would usually add code like this into an existing class
rather than having a standalone
class.^[10](#ch05.xhtml#idm45207101398400){#ch05.xhtml#idm45207101398400-marker
data-type="noteref"}^

::: {#ch05.xhtml#labels_enum_python data-type="example"}
##### [Example 5-7. ]{.label}A custom collector for a gauge used as an enum

``` {code-language="python" data-type="programlisting"}
from threading import Lock
from prometheus_client.core import GaugeMetricFamily, REGISTRY

class StateMetric(object):
    def __init__(self):
        self._resource_states = {}
        self._STATES = ["STARTING", "RUNNING", "STOPPING", "TERMINATED",]
        self._mutex = Lock()

    def set_state(self, resource, state):
        with self._mutex:
            self._resource_states[resource] = state

    def collect(self):
        family = GaugeMetricFamily("resource_state",
                "The current state of resources.",
                labels=["resource_state", "resource"])
        with self._mutex:
            for resource, state in self._resource_states.items():
                for s in self._STATES:
                    family.add_metric([s, resource], 1 if s == state else 0)
        yield family

sm = StateMetric()
REGISTRY.register(sm)

# Use the StateMetric.
sm.set_state("blaa", "RUNNING")
```
:::

Enum gauges are normal gauges that follow all the usual gauge semantics,
so no special metric suffix is needed.

Note that there are limits to this technique that you should be aware
of. If your number of states combined with the number of other labels
gets too high, performance issues due to the volume of samples and time
series can result. You could try combining similar states together, but
in the worst case you may have to fall back to using a gauge with values
such as 0--3 to represent the enum, and deal with the complexity that
brings to PromQL. This is discussed further in
["Cardinality"](#ch05.xhtml#cardinality_section){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="Info" data-type="sect2"}
::: {#ch05.xhtml#info_metrics .sect2}
## Info

The second common case for strings are *info metrics*, which
[]{#ch05.xhtml#idm45207101160768 primary="info metrics"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101160032
primary="machine roles approach"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101159360 primary="labels"
secondary="patterns in" tertiary="info" data-type="indexterm"}you may
also find called the *machine roles approach* for historical
reasons.^[11](#ch05.xhtml#idm45207101157632){#ch05.xhtml#idm45207101157632-marker
data-type="noteref"}^ Info metrics are useful for annotations such as
version numbers and other build information that would be useful to
query on, but it doesn't make sense to use them as target labels, which
apply to all metrics from a target (discussed in ["Target
Labels"](#ch08.xhtml#target_labels){data-type="xref"}) that applies to
every metric from a target.

The convention that has emerged is to use a gauge with the value 1 and
all the strings you'd like to have annotating the target as
labels.[]{#ch05.xhtml#idm45207101155104 primary="python_info expression"
data-type="indexterm"} The gauge should have the suffix `_info`. This
was shown in
[Figure 3-2](#ch03.xhtml#prometheus_eb_python_info){data-type="xref"}
with the `python_info` metric, which would look something like
[Example 5-8](#ch05.xhtml#labels_info_python_info){data-type="xref"}
when exposed.

::: {#ch05.xhtml#labels_info_python_info data-type="example"}
##### [Example 5-8. ]{.label}The `python_info` metric the Python client exposes by default

``` {data-type="programlisting"}
# HELP python_info Python platform information
# TYPE python_info gauge
python_info{implementation="CPython",major="3",minor="5",patchlevel="2",
        version="3.5.2"} 1.0
```
:::

To produce this in Python you could use either direct instrumentation or
a custom collector.
[Example 5-9](#ch05.xhtml#labels_info_python_code){data-type="xref"}
takes the direct instrumentation route, and also takes advantage of the
ability to pass in labels as keyword arguments with the Python
client.[]{#ch05.xhtml#idm45207101148256 primary="instrumentation"
secondary="direct, use by info metric" data-type="indexterm"}

::: {#ch05.xhtml#labels_info_python_code data-type="example"}
##### [Example 5-9. ]{.label}An info metric using direct instrumentation

``` {code-language="python" data-type="programlisting"}
from prometheus_client import Info

version_info = {
    "implementation": "CPython",
    "major": "3",
    "minor": "5",
    "patchlevel": "2",
    "version": "3.5.2",
}

INFO = Info("my_python", "Python platform information")
INFO.labels(version_info)
```
:::

An info metric can be joined to any other metric using the
multiplication operator and the `group_left` modifier.
[]{#ch05.xhtml#idm45207101102032 primary="info metrics"
secondary="joining to another metric"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101101184
primary="group_left" data-type="indexterm"}Any operator can be used to
join the metrics, but as the value of the info metric is 1,
multiplication won't change the value of the other
[metric]{.keep-together}.^[12](#ch05.xhtml#idm45207101099632){#ch05.xhtml#idm45207101099632-marker
data-type="noteref"}^

The value of 1 is inferred by the type of metric, in this case Info.

To add the `version` label from `python_info` to all `up` metrics,
you[]{#ch05.xhtml#idm45207101096896 primary="up"
secondary="adding version label from python_info to all"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101095920
primary="version labels" data-type="indexterm"} would use the PromQL
expression:

``` {data-type="programlisting"}
  up
* on (instance, job) group_left(version)
  python_info
```

The `group_left(version)` indicates that this is a many-to-one
match^[13](#ch05.xhtml#idm45207101093376){#ch05.xhtml#idm45207101093376-marker
data-type="noteref"}^ and that the `version` label should be copied over
from `python_info` into all `up` metrics that have the same `job` and
`instance` labels. We will look at `group_left` in more detail in
["Many-to-One and
group_left"](#ch15.xhtml#group_left){data-type="xref"}.

You can tell from looking at this expression that the output will have
the labels of the `up` metric, with a `version` label added. Adding all
the labels from `python_info` is not possible, as you could potentially
have unknown labels from both sides of the
expression,^[14](#ch05.xhtml#idm45207101054272){#ch05.xhtml#idm45207101054272-marker
data-type="noteref"}^ which is not workable semantically. It is
important to always know what labels are in play.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch05.xhtml#idm45207101052416 .sidebar}
# Breaking Changes and Labels

If you add or remove a label from instrumentation, it is always a
breaking change. []{#ch05.xhtml#idm45207101051120 primary="labels"
secondary="patterns in" tertiary="breaking changes and labels"
data-type="indexterm"}Removing a label removes a distinction a user may
have been depending on. Adding a label breaks aggregation that uses the
`without` clause.

The one exception to this is for info metrics. For those, the PromQL
expressions are constructed such that extra labels aren't a problem, so
it's safe for you to add labels to info metrics.
:::

```{=html}
</aside>
```
Info metrics also have a value of 1 so it is easy to calculate how many
time series have each label value using `sum`.
[]{#ch05.xhtml#idm45207101047632 primary="sum"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101046896 primary="count"
data-type="indexterm"}The number of application instances running each
version of Python would be `sum by (version)(python_info)`. If it were a
different value such as 0, a mix of `sum` and `count` would be required
in your aggregation hierarchy, which would be both more complicated and
error prone.[]{#ch05.xhtml#idm45207101044704 primary="labels"
secondary="patterns in" startref="ix_lblspatt" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="When to Use Labels" data-type="sect1"}
::: {#ch05.xhtml#when_to_use_labels .sect1}
# When to Use Labels

For a metric to be useful, you need to be able to aggregate it
somehow.[]{#ch05.xhtml#ix_lblsuse primary="labels"
secondary="when to use" data-type="indexterm"} The rule of thumb is that
either summing or averaging across a metric should produce a meaningful
result. For a counter of HTTP requests split by path and method, the sum
is the total number of requests. For a queue, combining the items in it
and its size limit into one metric would not make sense, as neither
summing nor averaging it produces anything useful.

One hint that an instrumentation label is not useful is if any time you
use the metric you find yourself needing to specify that label in
PromQL.^[15](#ch05.xhtml#idm45207101039888){#ch05.xhtml#idm45207101039888-marker
data-type="noteref"}^ In such a case you should probably move the label
to be in the metric name instead.

Another []{#ch05.xhtml#idm45207101038816 primary="time series"
secondary="total of rest of metric, avoiding"
data-type="indexterm"}thing to avoid is having a time series that is a
total of the rest of the metric, such as:

``` {data-type="programlisting"}
some_metric{label="foo"} 7
some_metric{label="bar"} 13
some_metric{label="total"} 20
```

or:

``` {data-type="programlisting"}
some_metric{label="foo"} 7
some_metric{label="bar"} 13
some_metric{} 20
```

Both of these break aggregation with `sum` in PromQL as you'd be double
counting. PromQL already provides you with the ability to calculate this
aggregate.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch05.xhtml#table_exception .sidebar}
# Table Exception

Astute readers probably noticed that summary metric quantiles break the
rule about the sum or average being meaningful because you can't do math
on quantiles.[]{#ch05.xhtml#idm45207101032944 primary="table exceptions"
data-type="indexterm"}

This is what we call the *table exception*, where even though you can't
do math across a metric, it's better to (ab)use a label than to have to
do regexes against a metric name. Regexes on metric names are a very bad
smell, and should never be used in graphs or alerts.

For you, this exception should only ever come up when writing exporters,
never for direct instrumentation. For example, you might have an unknown
mix of voltages, fan speeds, and temperatures coming from hardware
sensors. As you lack the information needed to split them into different
metrics, the only thing you can really do is shove them all into one
metric and leave it to the person consuming the metric to interpret it.
:::

```{=html}
</aside>
```
::: {.warning data-type="warning"}
###### Warning

The label names used for a metric should not change during the lifetime
of an application process. If you feel the need for this, you probably
want a logs-based monitoring solution for that use case.
:::

::: {.section pdf-bookmark="Cardinality" data-type="sect2"}
::: {#ch05.xhtml#cardinality_section .sect2}
## Cardinality

Don't go too far when using labels. []{#ch05.xhtml#idm45207101027648
primary="cardinality" data-type="indexterm"}Monitoring is a means to an
end, so more time series and more monitoring aren't always better. For a
monitoring system, whether you run it yourself on-premises or pay a
company to run it for you in the cloud, every time series and sample has
both a resource cost and a human cost in terms of ongoing operations to
keep the monitoring system up and running.

In this context we would like to talk about cardinality, which in
Prometheus is the number of time series you
have.[]{#ch05.xhtml#idm45207101026304 primary="time series"
secondary="number of, or cardinality" data-type="indexterm"} If your
Prometheus is provisioned to handle, say, 10 million time series, how
would you best spend those? At what point do you move certain use cases
to logs-based monitoring instead?

The way we look at it is to assume that someone running your code has a
large setup with a thousand instances of a particular
application.^[16](#ch05.xhtml#idm45207101024592){#ch05.xhtml#idm45207101024592-marker
data-type="noteref"}^ Adding a simple counter metric to an obscure
subsystem will add a thousand time series to your Prometheus, which is
0.01% of its capacity. That is basically free, and it might help you
debug a weird problem one day. Across all of the application and its
libraries, you might have a hundred of these obscure metrics, which
total to 1% of your monitoring capacity and is still quite cheap even
given the rarity that you'll likely use any one of them.

Now consider a []{#ch05.xhtml#idm45207101023520 primary="histograms"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101022784 primary="sum"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101022112 primary="count"
data-type="indexterm"}metric with a label with 10 values and in addition
was a histogram that by default has 12 time
series.^[17](#ch05.xhtml#idm45207101021200){#ch05.xhtml#idm45207101021200-marker
data-type="noteref"}^ That is a 120 series, or 1.2% of your monitoring
capacity. That this is a good trade-off is less clear. It might be OK to
have a handful of these, but you might also consider switching to a
quantile-less summary metric
instead.^[18](#ch05.xhtml#idm45207101019488){#ch05.xhtml#idm45207101019488-marker
data-type="noteref"}^

The next stage is where things get a little troublesome. If a label
already has a cardinality of 10, there is a good chance that it will
only increase over time as new features are introduced to your
application. A cardinality of 10 today might be 15 next year, and 200
might change to 300. Increased traffic from users usually means more
application instances. If you have more than one of these expanding
labels on a metric, the impact is compounded, resulting in a
combinatorial explosion of time series. And this is just one of the
ever-growing applications that Prometheus is monitoring.

In this way cardinality can sneak up on you. It is usually obvious that
email addresses, customers, and IP addresses are poor choices for label
values on cardinality grounds. It is less clear that the HTTP path is
going to be a problem. Given that the HTTP request metric is regularly
used, removing labels from it, switching away from a histogram, or even
reducing the number of buckets in the histogram can be challenging
politically.

The rule of thumb we use is that the cardinality of an arbitrary metric
on one application instance should be kept below 10. It is also OK to
have a handful of metrics that have a cardinality around 100, but you
should be prepared to reduce metric cardinality and rely on logs as that
cardinality grows.

::: {.note data-type="note"}
###### Note

The handful of a hundred cardinality metrics per Prometheus presumes a
thousand instances exposing such cardinality. If you are 100% certain
that you will not reach these numbers, such as with applications you
will run exactly one of, you can adjust the rule of thumb accordingly.
:::

There is a common pattern that we have seen when Prometheus is
introduced to an organization. It is common for organizations to
experience a learning curve when introducing Prometheus. At some point
it clicks, and they start to grasp the power of labels. It usually
follows quickly that your Prometheus has performance issues due to label
cardinality. We advise talking about the limitations of cardinality with
your users early on, and also consider using `sample_limit` as an
emergency safety valve (see ["Reducing
Load"](#ch21.xhtml#reducing_load_chap_twenty){data-type="xref"}).

The 10 biggest metrics in a Prometheus commonly constitute over half of
its resource usage, and this is almost always due to label cardinality.
There is sometimes confusion that if the issue is the number of label
values, wouldn't moving the label value into the metric name fix the
problem? As the underlying resource constraint is actually time series
cardinality (which manifests due to label values), moving label values
to the metric name doesn't change the cardinality, it just makes the
metrics harder to
use.^[19](#ch05.xhtml#idm45207101013376){#ch05.xhtml#idm45207101013376-marker
data-type="noteref"}^

Now that you can add metrics to your applications and know some basic
PromQL expressions, in the following chapter we will show you how you
can create dashboards in Grafana.[]{#ch05.xhtml#idm45207101012128
primary="labels" secondary="when to use" startref="ix_lblsuse"
data-type="indexterm"}[]{#ch05.xhtml#idm45207101010880 primary="labels"
startref="ix_lbls" data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch05.xhtml#idm45207102053776-marker)^ Graphite would use periods
rather than underscores.

^[2](#ch05.xhtml#idm45207102042272-marker)^ Or prefixes to metric names.

^[3](#ch05.xhtml#idm45207102041472-marker)^ When using the Pushgateway,
target labels may come from the application, as each Pushgateway group
is in a way a monitoring target.[]{#ch05.xhtml#idm45207102040800
primary="Pushgateway" secondary="target labels" data-type="indexterm"}
Depending on who you ask, this is either a feature or a limitation of
push-based monitoring.

^[4](#ch05.xhtml#idm45207102031872-marker)^ In Python be careful not to
do `labelnames='path'`, which is the same as
`labelnames=['p', 'a', 't', 'h']`. This is one of the more common
gotchas in Python.

^[5](#ch05.xhtml#idm45207102024752-marker)^ In Java the method is also
`labels`, and the Go equivalent is `WithLabelValues`.

^[6](#ch05.xhtml#idm45207101864256-marker)^ This is different from the
`__name__` in the Python code examples.

^[7](#ch05.xhtml#idm45207101792160-marker)^ For this reason you should
also resist the temptation to write a facade or wrapper around a
Prometheus client library that takes the metric name as an argument, as
that would also incur this lookup cost. It is cheaper, simpler, and
better semantically to have a file-level variable track the address of
the metric object rather than having to look it up all the time.

^[8](#ch05.xhtml#idm45207101627728-marker)^ This happens automatically
for metrics with no labels.

^[9](#ch05.xhtml#idm45207101415424-marker)^ Which is how an enum in a
language like C works.

^[10](#ch05.xhtml#idm45207101398400-marker)^ It is likely that future
versions of the client libraries will offer you utilities to make
working with enums easier. OpenMetrics, for example, currently plans on
having a *state set* type, of which enums are a special case.

^[11](#ch05.xhtml#idm45207101157632-marker)^ [Brian's
article](https://oreil.ly/eu_jZ) was the first place this technique was
documented.

^[12](#ch05.xhtml#idm45207101099632-marker)^ More formally, 1 is the
identity element for multiplication.

^[13](#ch05.xhtml#idm45207101093376-marker)^ In this case it is only
one-to-one as there is only one `up` time series per `python_info`;
however, you could use same expression for metrics with multiple time
series per target.

^[14](#ch05.xhtml#idm45207101054272-marker)^ Target labels for `up` and
any additional instrumentation labels added to `python_info` in the
future.

^[15](#ch05.xhtml#idm45207101039888-marker)^ Unless the label is a
target label.

^[16](#ch05.xhtml#idm45207101024592-marker)^ It is possible to have
more, but it's a reasonably conservative upper bound.

^[17](#ch05.xhtml#idm45207101021200-marker)^ So 10 buckets, plus the
`_sum` and `_count`.

^[18](#ch05.xhtml#idm45207101019488-marker)^ With only the `_sum` and
`_count` time series, quantileless summary metrics are a very cheap way
to get an idea of latency.

^[19](#ch05.xhtml#idm45207101013376-marker)^ It would also make it
harder to pinpoint the metrics responsible for your resource usage.
:::
:::
:::

[]{#ch06.xhtml}

::: {#ch06.xhtml#sbo-rt-content}
::: {#ch06.xhtml#grafana_chapter .chapter}
# [Chapter 6. ]{.label}Dashboarding with Grafana

When you get an alert or want to check on the current performance of
your systems, dashboards will be your first port of call.
[]{#ch06.xhtml#idm45207101008240 primary="dashboards"
data-type="indexterm"}The expression browser that you have seen up to
now is fine for ad hoc graphing and when you need to debug your PromQL,
but it's not designed to be used as a dashboard.

What do we mean by dashboard? A set of graphs, tables, and other
visualizations of your systems. You might have a dashboard for global
traffic, which services are getting how much traffic, and with what
latency. For each of those services you would likely have a dashboard of
its latency, errors, request rate, instance count, CPU usage, memory
usage, and service-specific metrics. Drilling down, you could have a
dashboard for particular subsystems or each service, or a garbage
collection dashboard that can be used with any Java
application.[]{#ch06.xhtml#ix_Grfn primary="Grafana"
data-type="indexterm"}

Grafana is a popular tool with which you can build such dashboards for
many different monitoring and nonmonitoring systems, including Graphite,
InfluxDB, Jaeger, Elasticsearch, and PostgreSQL. It is the recommended
tool for you to create dashboards when using Prometheus, and is
continuously improving its Prometheus support.

In this chapter we introduce using Grafana with Prometheus, extending
the Prometheus and Node Exporter you set up in
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"}.

```{=html}
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar">
```
::: {#ch06.xhtml#idm45207101004048 .sidebar}
# Promdash and Console Templates

Originally the Prometheus project had its own dashboarding tool called
*Promdash*. []{#ch06.xhtml#idm45207101002048 primary="dashboards"
secondary="Promdash and console templates" data-type="indexterm"}Even
though Promdash was better at the time for Prometheus use cases, the
Prometheus developers decided in 2016 to rally around Grafana rather
than have to continue to work on their own dashboarding solution. These
days, Prometheus is a first-class plug-in in Grafana, and also one of
the most
popular.^[1](#ch06.xhtml#idm45207101000848){#ch06.xhtml#idm45207101000848-marker
data-type="noteref"}^

There is a feature included with Prometheus called *console templates*
that can be used for dashboards.[]{#ch06.xhtml#idm45207100997008
primary="console templates" data-type="indexterm"} Unlike Promdash and
Grafana, which store dashboards in relational databases, it is built
right into Prometheus and is configured from the filesystem.
[]{#ch06.xhtml#idm45207100996016 primary="Go"
secondary="templating language"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100995072
primary="templating" secondary="Go templating language"
data-type="indexterm"}It allows you to render web pages using Go's
templating
language^[2](#ch06.xhtml#idm45207100994000){#ch06.xhtml#idm45207100994000-marker
data-type="noteref"}^ and easily keep your dashboards in source control.
Console templates are a very raw feature upon which you could build a
dashboard system, and as such it is recommended only for niche use cases
and advanced users.
:::

```{=html}
</aside>
```
::: {.section pdf-bookmark="Installation" data-type="sect1"}
::: {#ch06.xhtml#idm45207100993088 .sect1}
# Installation

You can download Grafana from the [Grafana
website](https://oreil.ly/ANoWC). []{#ch06.xhtml#ix_Grfninst
primary="Grafana" secondary="installing"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100989104 primary="Docker"
secondary="installing Grafana with" data-type="indexterm"}The site
includes installation instructions, but if you're using Docker, for
example, you would use:

``` {data-type="programlisting"}
docker run -d --name=grafana --net=host grafana/grafana:9.1.6
```

Note that this doesn't use a volume
mount,^[3](#ch06.xhtml#idm45207100986784){#ch06.xhtml#idm45207100986784-marker
data-type="noteref"}^ so it will store all state inside the container.

We use Grafana 9.1.6 here. You can use a newer version but be aware that
what you see will likely differ slightly.

Once Grafana is running you should be able to access it in your browser
at *http://localhost:3000/*, and you will see a login screen like the
one in [Figure 6-1](#ch06.xhtml#grafana_login){data-type="xref"}.

<figure>
<div id="ch06.xhtml#grafana_login" class="figure">
<img src="assets/pur2_0601.png" width="600" height="712"
alt="The Grafana login page" />
<h6><span class="label">Figure 6-1. </span>Grafana login screen</h6>
</div>
</figure>

Log in with the default username of **`admin`** and the default
password, which is also **`admin`**. You will be prompted to change your
password, which we recommend you to do.

You should then see the Home Dashboard, as shown in
[Figure 6-2](#ch06.xhtml#grafana_home){data-type="xref"}. We have
switched to the Light theme in the Org Settings in order to make things
easier to see in our screenshots.[]{#ch06.xhtml#idm45207100977648
primary="Home Dashboard (Grafana)" data-type="indexterm"}

<figure>
<div id="ch06.xhtml#grafana_home" class="figure">
<img src="assets/pur2_0602.png" width="600" height="362"
alt="Grafana Home Dashboard." />
<h6><span class="label">Figure 6-2. </span>Grafana Home Dashboard on a
fresh install</h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="Data Source" data-type="sect1"}
::: {#ch06.xhtml#idm45207100992496 .sect1}
# Data Source

Grafana connects to Prometheus through *data sources* to fetch
information used for graphs.[]{#ch06.xhtml#idm45207100973088
primary="Grafana" secondary="installing" startref="ix_Grfninst"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100971840 primary="Grafana"
secondary="data source"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100970896
primary="data sources" data-type="indexterm"} A variety of data source
types are supported out of the box, including InfluxDB, PostgreSQL, and
of course, Prometheus. You can have many data sources of the same type,
and usually you would have one per running Prometheus. A Grafana
dashboard can have graphs from a variety of sources, and you can even
mix sources in a time series panel.

More recent versions of Grafana make it easy to add your first data
source. Click "Add your first data source" and add a data source with a
Name of **`Prometheus`**, a Type of Prometheus, and a URL of
*http://localhost:9090* (or whatever other URL your Prometheus from
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"} is
listening on). The form should look like
[Figure 6-3](#ch06.xhtml#grafana_data_source){data-type="xref"}. Leave
all other settings at their defaults, and finally click Save & Test at
the bottom of the form. Depending on your screen size, you might need to
scroll to see the buttons. If it works, you will get a message that the
data source is working. If you don't, check that the Prometheus is
indeed running and that it is accessible from
Grafana.^[4](#ch06.xhtml#idm45207100966784){#ch06.xhtml#idm45207100966784-marker
data-type="noteref"}^

<figure>
<div id="ch06.xhtml#grafana_data_source" class="figure">
<img src="assets/pur2_0603.png" width="600" height="513"
alt="Grafana Add data source page for a Prometheus data source on http://localhost:9090." />
<h6><span class="label">Figure 6-3. </span>Adding a Prometheus data
source to Grafana</h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="Dashboards and Panels" data-type="sect1"}
::: {#ch06.xhtml#idm45207100964032 .sect1}
# Dashboards and Panels

Go again to *http://localhost:3000/* in your browser, and this time
click "Create your first dashboard," which will bring you to a page that
looks like
[Figure 6-4](#ch06.xhtml#grafana_new_dashboard){data-type="xref"}.[]{#ch06.xhtml#idm45207100961072
primary="dashboards" secondary="new Grafana dashboard"
data-type="indexterm"}[]{#ch06.xhtml#ix_Grfndspa primary="Grafana"
secondary="dashboards and panels" data-type="indexterm"}

<figure>
<div id="ch06.xhtml#grafana_new_dashboard" class="figure">
<img src="assets/pur2_0604.png" width="600" height="382"
alt="A new Grafana dashboard with one row and no panels." />
<h6><span class="label">Figure 6-4. </span>A new Grafana dashboard</h6>
</div>
</figure>

From here you can click "Add a new panel" and select the first panel
you'd like to add. Panels are rectangular areas containing a graph,
table, or other visual information. You can add new panels beyond the
first with the "Add panel" button, which is the button on the top row
with the orange plus sign. Panels are organized within a grid system,
and can be rearranged using drag-and-drop.

::: {.note data-type="note"}
###### Note

After making any changes to a dashboard or panels, if you want them to
be remembered you must explicitly save them. You can do this with the
save button at the top of the page or using the Ctrl-S keyboard
shortcut.
:::

You can access the dashboard settings, such as its name, using the gear
icon at the top. From the settings menu you can also duplicate
dashboards using Save As, which is handy when you want to experiment
with a dashboard.

::: {.section .less_space .pagebreak-before pdf-bookmark="Avoiding the Wall of Graphs" data-type="sect2"}
::: {#ch06.xhtml#idm45207100954720 .sect2}
## Avoiding the Wall of Graphs

It is not unusual to end up with multiple dashboards per service you
run.[]{#ch06.xhtml#idm45207100952960 primary="dashboards"
secondary="avoiding wall of graphs"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100951984 primary="Grafana"
secondary="dashboards and panels" tertiary="avoiding wall of graphs"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100950768
primary="wall of graphs, avoiding" data-type="indexterm"} It is easy for
dashboards to gradually get bloated with too many graphs, making it
challenging for you to interpret what is actually going on. To mitigate
this you should try to avoid dashboards that serve more than one team or
purpose, and instead give them a dashboard each.

The more high-level a dashboard is, the fewer rows and panels it should
have. A global overview should fit on one screen and be understandable
at a distance. Dashboards commonly used for on call might have a row or
two more than that, whereas a dashboard for in-depth performance tuning
by experts might run to several screens.

Why do we recommend that you limit the amount of graphs on each of your
dashboards?[]{#ch06.xhtml#idm45207100949104 primary="graphs"
secondary="limiting number on a dashboard" data-type="indexterm"} The
answer is that every graph, line, and number on a dashboard makes it
harder to understand, due to cognitive load to understand everything you
are looking at. This is particularly relevant when you are on call and
handling alerts. When you are stressed, need to act quickly, and are
possibly only half awake, having to remember the subtler points of what
each graph on your dashboard means is not going to aid you in terms of
either response time or taking an appropriate action.

To give an extreme example, one service Brian worked on had a dashboard
(singular) with over 600
graphs.^[5](#ch06.xhtml#idm45207100947472){#ch06.xhtml#idm45207100947472-marker
data-type="noteref"}^ This was hailed as superb monitoring, due to the
vast wealth of data on display. The sheer volume of data meant he was
never able to get his head around that dashboard, plus it took rather a
long time to load. He likes to call this style of dashboarding the Wall
of Graphs antipattern.

You should not confuse having lots of graphs with having good
monitoring. Monitoring is ultimately about outcomes, such as faster
incident resolution and better engineering decisions, not pretty
graphs.[]{#ch06.xhtml#idm45207100946304 primary="Grafana"
secondary="dashboards and panels" startref="ix_Grfndspa"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Time Series Panel" data-type="sect1"}
::: {#ch06.xhtml#idm45207100944928 .sect1}
# Time Series Panel

The Time series panel is the main panel you will be
using.[]{#ch06.xhtml#ix_GrfnTSP primary="Grafana"
secondary="Time series panel"
data-type="indexterm"}[]{#ch06.xhtml#ix_tmserpnl
primary="Time series panel" data-type="indexterm"} As the name
indicates, it displays time series. As shown in
[Figure 6-4](#ch06.xhtml#grafana_new_dashboard){data-type="xref"}, click
the "Add a new panel" button to add a Time series
panel.[]{#ch06.xhtml#idm45207100939488 primary="graphs"
secondary="graph editor in Grafana" data-type="indexterm"} You are
directly entering editing mode for this new panel. To configure it again
later, click Panel Title and then Edit, as shown in
[Figure 6-5](#ch06.xhtml#grafana_panel_editor){data-type="xref"}.^[6](#ch06.xhtml#idm45207100937536){#ch06.xhtml#idm45207100937536-marker
data-type="noteref"}^

<figure>
<div id="ch06.xhtml#grafana_panel_editor" class="figure">
<img src="assets/pur2_0605.png" width="583" height="297"
alt="A blank Grafana time series panel with the edit popup showing." />
<h6><span class="label">Figure 6-5. </span>Opening the editor for a Time
series panel</h6>
</div>
</figure>

The panel editor will open on the Query tab. In the text box beside
A,^[7](#ch06.xhtml#idm45207100933968){#ch06.xhtml#idm45207100933968-marker
data-type="noteref"}^ enter
**`process_resident_memory_​`[`bytes`]{.keep-together}** for the query
expression, as shown in
[Figure 6-6](#ch06.xhtml#grafana_query_rss){data-type="xref"}, and then
click "Run queries." You will see a graph of memory usage similar to
what [Figure 2-7](#ch02.xhtml#prometheus_eb_rss_graph){data-type="xref"}
showed when the same expression was used in the expression
browser.[]{#ch06.xhtml#idm45207100930256 primary="memory"
secondary="memory usage graph in Grafana"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100929216
primary="process_resident_memory_bytes"
secondary="in Grafana graph editor" secondary-sortas="Grafana"
data-type="indexterm"}

<figure>
<div id="ch06.xhtml#grafana_query_rss" class="figure">
<img src="assets/pur2_0606.png" width="600" height="294"
alt="Grafana graph editor." />
<h6><span class="label">Figure 6-6. </span>The expression
<code>process_resident_memory_bytes</code> in the graph editor</h6>
</div>
</figure>

Grafana offers more than the expression browser. You can configure the
legend to display something other than the full-time series name. Select
Custom in the Legend dropdown and type **`{{job}}`** in the text box. On
the right side, under "Standard options," change the unit to "data/bytes
(IEC)." Under "Panel options," change the Title to **`Memory Usage`**.
The graph will now look something like
[Figure 6-7](#ch06.xhtml#grafana_graph_rss){data-type="xref"}, with a
more useful legend, appropriate units on the axis, and a title.

<figure>
<div id="ch06.xhtml#grafana_graph_rss" class="figure">
<img src="assets/pur2_0607.png" width="600" height="178"
alt="Memory usage graph in Grafana." />
<h6><span class="label">Figure 6-7. </span>Memory Usage graph with
custom legend, title, and axis units configured</h6>
</div>
</figure>

These are the settings you will want to configure on virtually all of
your graphs, but this is only a small taste of what you can do with
graphs in Grafana. You can configure colors, draw style, tool tips,
stacking, filling, and even include metrics from multiple data sources.

Don't forget to save the panel and dashboard before continuing! Click
Apply, then save the dashboard. `New dashboard` is a special dashboard
name for Grafana, so you should choose something more
memorable.[]{#ch06.xhtml#idm45207100919328
primary="New dashboard (Grafana)" data-type="indexterm"}

::: {.section pdf-bookmark="Time Controls" data-type="sect2"}
::: {#ch06.xhtml#idm45207100918496 .sect2}
## Time Controls

You may have noticed Grafana's time controls on the top right of the
page.[]{#ch06.xhtml#idm45207100916848 primary="Time series panel"
secondary="time controls"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100915872 primary="time"
secondary="time controls in Grafana" data-type="indexterm"} By default,
it should say "Last 6 hours." Clicking the time controls will show the
page in
[Figure 6-8](#ch06.xhtml#grafana_time_controls){data-type="xref"} where
you can choose a time range. The dropdown next to the circular arrow in
[Figure 6-9](#ch06.xhtml#refresh_dropdown){data-type="xref"} is where
you can choose how often to refresh.[]{#ch06.xhtml#idm45207100912912
primary="refresh interval menu (Grafana)" data-type="indexterm"} The
time controls apply to an entire dashboard at once, though you can also
configure some overrides on a per-panel basis.

<figure>
<div id="ch06.xhtml#grafana_time_controls" class="figure">
<img src="assets/pur2_0608.png" width="600" height="426"
alt="Grafana&#39;s time control menu." />
<h6><span class="label">Figure 6-8. </span>Grafana’s time control
menu</h6>
</div>
</figure>

<figure>
<div id="ch06.xhtml#refresh_dropdown" class="figure">
<img src="assets/pur2_0609.png" width="462" height="430"
alt="Grafana&#39;s refresh interval menu." />
<h6><span class="label">Figure 6-9. </span>Grafana’s refresh interval
menu</h6>
</div>
</figure>

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch06.xhtml#idm45207100907888 .sidebar}
# Aliasing

As your graphs refresh you may notice that the shape can change, even
though the underlying data hasn't
changed.[]{#ch06.xhtml#idm45207100906256 primary="aliasing"
data-type="indexterm"} This is a signal processing effect called
*aliasing*. You may already be familiar with aliasing from the graphics
in first-person video games, where the rendering artifacts of a distant
object change and may seem to flicker as you walk toward it.

The same thing is happening here. []{#ch06.xhtml#idm45207100904512
primary="rate function" data-type="indexterm"}Each rendering of the data
is at a slightly different time, so functions such as `rate` will
calculate slightly different results. None of these results are wrong,
they're just different approximations of what is going on.

This is a fundamental limitation of metrics-based monitoring, and any
other system that takes samples, and is related to the *Nyquist-Shannon
sampling theorem*. []{#ch06.xhtml#idm45207100902016 primary="sampling"
secondary="fundamental limitation of"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100900976
primary="Nyquist-Shannon sampling theorem" data-type="indexterm"}You can
mitigate aliasing by increasing the frequency of your scrapes and
evaluations, but ultimately to get a 100% accurate view of what is going
on you will need logs, as logs have an exact record of every single
event.

Note that recent versions of Grafana implement solutions that tweak
queries to Prometheus to limit this result as much as
possible.[]{#ch06.xhtml#idm45207100899632 primary="Grafana"
secondary="Time series panel" startref="ix_GrfnTSP"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100898384
primary="Time series panel" startref="ix_tmserpnl"
data-type="indexterm"}
:::

```{=html}
</aside>
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Stat Panel" data-type="sect1"}
::: {#ch06.xhtml#grafana_stat .sect1}
# Stat Panel

The Stat panel displays single values of a time series.
[]{#ch06.xhtml#ix_Statpnl primary="Stat panel"
data-type="indexterm"}[]{#ch06.xhtml#ix_GrfnStat primary="Grafana"
secondary="Stat panel" data-type="indexterm"}It can also show a
Prometheus label value.

We will start this example by adding a time series value. Click Apply
(the back arrow on the top right) to return from the Time series panel
to the dashboard view. Click the "Add panel" button and select "Stat
panel" from the dropdown on the
right.^[8](#ch06.xhtml#idm45207100892720){#ch06.xhtml#idm45207100892720-marker
data-type="noteref"}^ For the query expression on the Metrics tab, use
**`prometheus_tsdb_head_series`**, which is (roughly speaking) the
number of different time series Prometheus is ingesting. By default the
Stat panel will calculate the last value of the time series over the
dashboard's time range. The default text can be a bit small, so change
the Font Size to 200%. Under Panel options, change the Title to
**`Prometheus Time Series`**. Under Thresholds, click the trash bin
image next to the predefined threshold at 80 to remove the
threshold.[]{#ch06.xhtml#idm45207100890720 primary="dashboards"
secondary="with graph and Stat panels in Grafana"
secondary-sortas="graph" data-type="indexterm"} Finally, click Apply and
you should see something like
[Figure 6-10](#ch06.xhtml#grafana_two_panels){data-type="xref"}.

<figure>
<div id="ch06.xhtml#grafana_two_panels" class="figure">
<img src="assets/pur2_0610.png" width="600" height="486"
alt="Dashboard with a graph and stat panel." />
<h6><span class="label">Figure 6-10. </span>Dashboard with a graph and
Stat panel</h6>
</div>
</figure>

Displaying label[]{#ch06.xhtml#idm45207100886080 primary="labels"
secondary="displaying label values in Grafana" data-type="indexterm"}
values is handy for software versions on your graphs. Add another Stat
panel; this time you will use the query expression
**`node_uname_info`**, which contains the same information as the
`uname -a` command. Set "Format as" to Table, and under "Value options,"
set the Fields to "release." Under "Panel options," the Title should be
**`Kernel version`**. After clicking "Back to dashboard" and rearranging
the panels using drag-and-drop, you should see something like
[Figure 6-11](#ch06.xhtml#grafana_three_panels){data-type="xref"}.

The Stat panel has further features, including different colors
depending on the time series value, and displaying sparklines behind the
value.[]{#ch06.xhtml#idm45207100881664 primary="Grafana"
secondary="Stat panel" startref="ix_GrfnStat"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100880384
primary="Stat panel" startref="ix_Statpnl" data-type="indexterm"}

<figure>
<div id="ch06.xhtml#grafana_three_panels" class="figure">
<img src="assets/pur2_0611.png" width="600" height="338"
alt="Dashboard with a graph and two stat panels." />
<h6><span class="label">Figure 6-11. </span>Dashboard with a graph and
two Stat panels, one numeric and one text</h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="Table Panel" data-type="sect1"}
::: {#ch06.xhtml#idm45207100877280 .sect1}
# Table Panel

While the Stat panel can display multiple time series, each unique time
series takes quite a lot of space.[]{#ch06.xhtml#idm45207100875568
primary="Table panel"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100874864 primary="Grafana"
secondary="Table panel" data-type="indexterm"} The Table panel allows
you to display multiple time series in a more concise way, and offers
advanced features like pagination. Table panels tend to require more
configuration than other panels, and all the text can look cluttered on
your dashboards.

Add a new panel, this time a Table panel. As before, click "Add panel"
and then "Add a new panel." Select Table in the dropdown on the right.
Use the query expression
**`rate(node_network_receive_bytes_total[1m])`** on the Metrics tab, and
change the Type from Range to Instant. Change the Format to Table.

There are more columns that you need here. Go to the Transform tab, and
click "Organize fields." Select the fields you want to hide by clicking
the eye icon, as in
[Figure 6-12](#ch06.xhtml#fields_organized){data-type="xref"}.

<figure>
<div id="ch06.xhtml#fields_organized" class="figure">
<img src="assets/pur2_0612.png" width="600" height="262"
alt="A transformation to hide certain fields." />
<h6><span class="label">Figure 6-12. </span>A transformation to hide
certain fields</h6>
</div>
</figure>

In the sidebar, under "Standard options," set the unit to "bytes/sec
(IEC)" under "data rate." Finally, under "Panel options," set the title
to **`Network Traffic Received`**. []{#ch06.xhtml#idm45207100868000
primary="dashboards" secondary="with Stat panels and Table panel"
secondary-sortas="Stat" data-type="indexterm"}After all that, if you go
back to the dashboard and rearrange the panels, you should see a
dashboard like the one in
[Figure 6-13](#ch06.xhtml#grafana_four_panels){data-type="xref"}.

<figure>
<div id="ch06.xhtml#grafana_four_panels" class="figure">
<img src="assets/pur2_0613.png" width="600" height="308"
alt="Dashboard with a graph, two stat and one table panel." />
<h6><span class="label">Figure 6-13. </span>Dashboard with several
panels, including a table for per-device network traffic</h6>
</div>
</figure>
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="State Timeline Panel" data-type="sect1"}
::: {#ch06.xhtml#idm45207100863504 .sect1}
# State Timeline Panel

When visualizing metrics that represent a state, such as the `up`
metrics, the State timeline panel comes in handy. It shows how discrete
state changes over time.[]{#ch06.xhtml#idm45207100861104
primary="State timeline panel"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100860400 primary="Grafana"
secondary="State timeline panel" data-type="indexterm"}

Let's use it to display our `up`
metrics.[]{#ch06.xhtml#idm45207100858656 primary="up"
secondary="displaying up metrics in Grafana State timeline panel"
data-type="indexterm"}

Let's add a State timeline panel. As before, Click "Add panel" and then
"Add a new panel." Select "State Timeline" in the dropdown on the right.
Use the query expression **`up`** on the Metrics tab. Set the legend to
custom: `{{job}} / {{instance}}`.

In the sidebar, under "Standard options," set "Color scheme" to "Single
Color." Under "Value mappings," click "Add value mappings" and add two
value mappings: Value `1` to display the text `UP`, with a green color,
and Value `2` to display `DOWN`, with a red color as seen in
[Figure 6-14](#ch06.xhtml#adding_two_value_mappings){data-type="xref"}.

<figure>
<div id="ch06.xhtml#adding_two_value_mappings" class="figure">
<img src="assets/pur2_0614.png" width="600" height="214"
alt="The value mappings for our state timeline panel." />
<h6><span class="label">Figure 6-14. </span>Adding two value mappings to
the State timeline panel</h6>
</div>
</figure>

You see the finished State timeline panel in
[Figure 6-15](#ch06.xhtml#finished_state_timeline){data-type="xref"}.

<figure>
<div id="ch06.xhtml#finished_state_timeline" class="figure">
<img src="assets/pur2_0615.png" width="600" height="177"
alt="A state timeline panel that shows the status of the targets." />
<h6><span class="label">Figure 6-15. </span>The finished State timeline
panel</h6>
</div>
</figure>
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="Template Variables" data-type="sect1"}
::: {#ch06.xhtml#idm45207100847456 .sect1}
# Template Variables

All the dashboard examples we have shown you so far have applied to a
single Prometheus and a single Node Exporter.
[]{#ch06.xhtml#idm45207100845040 primary="Grafana"
secondary="template variables"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100844064
primary="dashboards"
secondary="creating using Grafana template variables"
data-type="indexterm"}This is fine for demonstration of the basics, but
not great when you have hundreds or even tens of machines to monitor.
The good news is that you don't have to create a dashboard for every
individual machine.[]{#ch06.xhtml#idm45207100842896 primary="templating"
secondary="template variables in Grafana" data-type="indexterm"} You can
use Grafana's templating feature.

You only have monitoring for one machine set up, so for this example we
will template based on network devices, as you should have at least two
of
those.^[9](#ch06.xhtml#idm45207100841424){#ch06.xhtml#idm45207100841424-marker
data-type="noteref"}^

To start with, create a new dashboard by hovering on the four squares
icon in the sidebar and then clicking "+New dashboard," as you can see
in
[Figure 6-16](#ch06.xhtml#grafana_2nd_new_dashboard){data-type="xref"}.

<figure>
<div id="ch06.xhtml#grafana_2nd_new_dashboard" class="figure">
<img src="assets/pur2_0616.png" width="600" height="324"
alt="Dashboards menu." />
<h6><span class="label">Figure 6-16. </span>Dashboards menu, including a
button to create new dashboards</h6>
</div>
</figure>

Click the gear icon up top and then
Variables.^[10](#ch06.xhtml#idm45207100836240){#ch06.xhtml#idm45207100836240-marker
data-type="noteref"}^ Click "+Add variable" to add a template variable.
The Name should be **`Device`**, and the "Data source" is Prometheus
with a Refresh of "On time range change." The Query you will use is
**`label_values(node_network_receive_bytes_total, device)`**.^[11](#ch06.xhtml#idm45207100834192){#ch06.xhtml#idm45207100834192-marker
data-type="noteref"}^ The page should look like
[Figure 6-17](#ch06.xhtml#grafana_template_device){data-type="xref"}.
Click Update to add the variable.

<figure>
<div id="ch06.xhtml#grafana_template_device" class="figure">
<img src="assets/pur2_0617.png" width="600" height="452"
alt="Grafana template variable edit page." />
<h6><span class="label">Figure 6-17. </span>Adding a Device template
variable to a Grafana dashboard</h6>
</div>
</figure>

When you click the arrow to go back to the dashboard, a dropdown for the
variable will now be available, as shown in
[Figure 6-18](#ch06.xhtml#grafana_blank_dashboard_template){data-type="xref"}.

<figure>
<div id="ch06.xhtml#grafana_blank_dashboard_template" class="figure">
<img src="assets/pur2_0618.png" width="600" height="500"
alt="Grafana template variable edit page." />
<h6><span class="label">Figure 6-18. </span>The dropdown for the Device
template variable is visible</h6>
</div>
</figure>

You now need to use the variable. Click the X to close the Templating
section, then click the three dots, and add a new Time series panel.
Configure the query expression to be
**`rate(node_network_receive_bytes_total​{device="$Device"}[$__rate_interval])`**,
and `$Device` will be substituted with the value of the template
variable. If you're using the Multi-value option, you would use
`device=~"$Device"` as the variable would be a regular expression in
that case. Regexes should also be used in case the value is complex, as
Grafana would try to escape them anyway. Set the Legend Format to Custom
then **`{{device}}`**, the Title to **`Bytes Received`**, and the Unit
to "bytes/sec" under "data rate."

::: {.note data-type="note"}
###### Note

As you have seen, we are using `$__rate_interval` in our PromQL
expression. This is a Grafana feature that selects the best interval
depending on the scrape interval set in the datasource configuration,
and other parameters such as the step used in the panel. If you look at
24 hours of data, the value of `$__rate_interval` would be greater than
if you only look at the last hour.
:::

Click Apply and click the panel title, and this time click More and then
Duplicate. This will create a copy of the existing panel. Alter the
settings on this new panel to use the expression
**`rate(node_network_transmit_bytes_total​`[`{device=~"$Device"})[$__rate_interval]`]{.keep-together}**,
and set the Title to **`Bytes Transmitted`**. The dashboard will now
have panels for bytes sent in both directions, as shown in
[Figure 6-19](#ch06.xhtml#grafana_network_dashboard){data-type="xref"},
and you can look at each network device by selecting it in the dropdown.

<figure>
<div id="ch06.xhtml#grafana_network_dashboard" class="figure">
<img src="assets/pur2_0619.png" width="600" height="209"
alt="Grafana dashboard with one template variable." />
<h6><span class="label">Figure 6-19. </span>A basic network traffic
dashboard using a template variable</h6>
</div>
</figure>

In the real world you would probably template based on the `instance`
label and display all the network-related metrics for one machine at
once. You might even have multiple variables for one dashboard. This is
how a generic dashboard for Java garbage collection might work: one
variable for the `job`, one for the `instance`, and one to select which
Prometheus data source to use.

You may have noticed that as you change the value of the variable, the
URL parameters change, and similarly if you use the time controls. This
allows you to share dashboard links, or have your alerts link to a
dashboard with just the right variable values, as shown in
["Notification
templates"](#ch19.xhtml#notification_templates){data-type="xref"}. There
is a "Share dashboard" icon at the top of the page you can use to create
the URLs and take snapshots of the data in the dashboard. Snapshots are
perfect for postmortems and outage reports, where you want to preserve
how the dashboard looked.[]{#ch06.xhtml#idm45207100812816
primary="Grafana" startref="ix_Grfn" data-type="indexterm"}

In the next chapter we will go into more detail on the Node Exporter and
some of the metrics it offers.
:::
:::

::: {data-type="footnotes"}
^[1](#ch06.xhtml#idm45207101000848-marker)^ Grafana by default reports
anonymous usage statistics.[]{#ch06.xhtml#idm45207101000256
primary="reporting_enabled setting"
data-type="indexterm"}[]{#ch06.xhtml#idm45207100999536 primary="Grafana"
secondary="reporting_enabled setting" data-type="indexterm"} This can be
disabled with the `reporting_enabled` setting in its configuration file.

^[2](#ch06.xhtml#idm45207100994000-marker)^ This is the same templating
language that is used for alert templating, with some minor differences
in available functionality.

^[3](#ch06.xhtml#idm45207100986784-marker)^ A way to have filesystems
shared across containers over time, as by default a Docker container's
storage is specific to that container. Volume mounts can be specified
with the `-v` flag to `docker run`.

^[4](#ch06.xhtml#idm45207100966784-marker)^ The Access server setting
has Grafana make the requests to your Prometheus. By contrast, the
direct setting has your browser make the request. Direct setting is
deprecated and will be removed in future Grafana releases.

^[5](#ch06.xhtml#idm45207100947472-marker)^ The worst case of this we
have heard of weighed in at over 1,000 graphs.

^[6](#ch06.xhtml#idm45207100937536-marker)^ You can also use the `e`
keyboard shortcut to open the editor while hovering over the panel. You
can press ? to view a full list of keyboard shortcuts.

^[7](#ch06.xhtml#idm45207100933968-marker)^ The A indicates that it is
the first query.

^[8](#ch06.xhtml#idm45207100892720-marker)^ The dropdown should display
Time series by default.

^[9](#ch06.xhtml#idm45207100841424-marker)^ Loopback and your wired
and/or WiFi device.

^[10](#ch06.xhtml#idm45207100836240-marker)^ This was called templating
in previous Grafana versions.

^[11](#ch06.xhtml#idm45207100834192-marker)^ Note that this is not a
PromQL query. `label_values` is specific to Grafana and used only for
templating.
:::
:::
:::

[]{#part03.xhtml}

::: {#part03.xhtml#sbo-rt-content}
::: {#part03.xhtml#part3 .part pdf-bookmark="Part III. Infrastructure Monitoring" data-type="part"}
# [Part III. ]{.label}Infrastructure Monitoring

The entire world does not (yet) revolve around Prometheus, nor provide
Prometheus metrics out of the box. Exporters are tools that let you
translate metrics from other systems into a format that Prometheus
understands.

In [Chapter 7](#ch07.xhtml#node_exporter_chapter){data-type="xref"} one
of the first exporters you will probably use, the Node Exporter, is
covered in detail.

In [Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}
you will learn how Prometheus knows what to pull metrics from and how to
do so.

[Chapter 9](#ch09.xhtml#containers_k8_chapter){data-type="xref"} dives
into monitoring of container technologies such as Docker and Kubernetes.

There are literally hundreds of exporters in the Prometheus ecosystem.
[Chapter 10](#ch10.xhtml#common_exporters_chapter){data-type="xref"}
shows you how to use various typical exporters.

As you may already have another metric-based monitoring system,
[Chapter 11](#ch11.xhtml#other_monitoring_systems_chapter){data-type="xref"}
looks at how you can integrate those into Prometheus.

Exporters don't appear from thin air. If the exporter you want doesn't
exist, you can use
[Chapter 12](#ch12.xhtml#writing_exporters_chapter){data-type="xref"} to
create one.
:::
:::

[]{#ch07.xhtml}

::: {#ch07.xhtml#sbo-rt-content}
::: {#ch07.xhtml#node_exporter_chapter .chapter}
# [Chapter 7. ]{.label}Node Exporter

The Node
Exporter^[1](#ch07.xhtml#idm45207100799344){#ch07.xhtml#idm45207100799344-marker
data-type="noteref"}^ is likely one of the first exporters you will use,
as already seen in
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"}.
[]{#ch07.xhtml#ix_NdE primary="Node Exporter" data-type="indexterm"}It
exposes machine-level metrics, largely from your operating system's
kernel, such as CPU, memory, disk space, disk I/O, network bandwidth,
and motherboard temperature.[]{#ch07.xhtml#idm45207100796368
primary="disk I/O"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100795696
primary="Windows Exporter" data-type="indexterm"} The Node Exporter is
used with Unix systems; Windows users should use the [Windows
Exporter](https://oreil.ly/mVsAX)
instead.^[2](#ch07.xhtml#idm45207100794144){#ch07.xhtml#idm45207100794144-marker
data-type="noteref"}^

The Node Exporter is intended only to monitor the machine itself, not
individual processes or services on it. Other monitoring systems often
have what we like to call an *uberagent*; that is, a single process that
monitors everything on the machine.[]{#ch07.xhtml#idm45207100792512
primary="uberagent" data-type="indexterm"} In the Prometheus
architecture each of your services will expose its own metrics, using an
exporter if needed, which is then directly scraped by Prometheus. This
avoids you ending up with uberagent as either an operational or
performance bottleneck, and enables you to think in terms more of
dynamic services rather than machines.

The guidelines to use when you are creating metrics with direct
instrumentation, such as those discussed in ["What Should I Name My
Metrics?"](#ch03.xhtml#metric_naming){data-type="xref"}, are relatively
clear. []{#ch07.xhtml#idm45207100790256 primary="metrics"
secondary="from exporters" secondary-sortas="exporters"
data-type="indexterm"}This is not the case with exporters, where by
definition the data is coming from a source not designed with the
Prometheus guidelines in mind. Depending on the volume and quality of
metrics, trade-offs have to be made by the exporter developers between
engineering effort and getting perfect metrics.

In the case of Linux, there are thousands of metrics on
offer.[]{#ch07.xhtml#idm45207100788528 primary="Linux"
secondary="metrics on offer" data-type="indexterm"} Some are well
documented and understood, such as CPU usage; others, like memory usage,
have varied from kernel version to kernel version as the implementation
has changed. You will even find metrics that are completely
undocumented, where you would have to read the kernel source code to try
to figure out what they do.

The Node Exporter is designed to be run as a nonroot user, and should be
run directly on the machine in the same way you run a system daemon like
sshd or cron.[]{#ch07.xhtml#idm45207100786960 primary="Docker"
secondary="running Node Exporter within"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100786016
primary="Node Exporter" secondary="running within Docker"
data-type="indexterm"}

::: {.warning data-type="warning"}
###### Warning

While running Node Exporter within
Docker^[3](#ch07.xhtml#idm45207100784032){#ch07.xhtml#idm45207100784032-marker
data-type="noteref"}^ is possible, you will need to use some volumes and
command-line parameters (`--path.procfs`, `--path.rootfs`,
`--path.sysfs`) to mount the host filesystem inside of the container. If
possible, run the Node Exporter as a service on the node, without
Docker. Docker attempts to isolate a container from the inner workings
of the machine, which doesn't work well with the Node Exporter trying to
get to those inner workings.
:::

Unlike most other exporters, due to the broad variety of metrics
available from operating systems, the Node Exporter allows you to
configure which categories of metrics it
fetches.[]{#ch07.xhtml#idm45207100781136 primary="metrics"
secondary="configuring types to collect with Node Exporter"
data-type="indexterm"} You can do this with command-line flags such as
`--collector.wifi`, which would enable the WiFi collector, and
`--no-collector.wifi`, which would disable it.
`--collector.disable-defaults` will disable all collectors except those
explicitly enabled as command-line flags. There are reasonable defaults
set, so this is not something you should worry about when starting out.

Different kernels expose different metrics, as, for example, Linux and
FreeBSD do things in different ways. Metrics may move between collectors
over time as the Node Exporter is refactored. If you are using a
different Unix system, you will find that the metrics and collectors on
offer vary.

In this chapter we explain some of the key metrics Node Exporter version
1.4.0 exposes with a 5.18.0 Linux kernel.
[]{#ch07.xhtml#idm45207100777216 primary="metrics"
secondary="exposed by Node Exporter"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100776176
primary="Node Exporter"
secondary="version 1.4.0  with 5.18.0 Linux kernel, metrics from"
data-type="indexterm"}This is not intended to be an exhaustive list of
available metrics. As with most exporters and applications, you will
want to look through the [*/metrics*]{.keep-together} path to see what
is available. You can try out the example PromQL expressions using your
setup from
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"}.

::: {.section pdf-bookmark="CPU Collector" data-type="sect1"}
::: {#ch07.xhtml#node_cpu .sect1}
# CPU Collector

The main metric from the CPU collector is `node_cpu_seconds_total`,
which []{#ch07.xhtml#idm45207100770224 primary="node_cpu_seconds_total"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100769488
primary="cpu collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100768816
primary="Node Exporter" secondary="cpu collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100767872
primary="collectors" seealso="Node Exporter"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100766928
primary="cpu labels"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100766256
primary="mode labels" data-type="indexterm"}is a counter indicating how
much time each CPU spent in each mode. The labels are `cpu` and `mode`:

``` {data-type="programlisting"}
# HELP node_cpu_seconds_total Seconds the CPUs spent in each mode.
# TYPE node_cpu_seconds_total counter
node_cpu_seconds_total{cpu="0",mode="idle"} 13024.48
node_cpu_seconds_total{cpu="0",mode="iowait"} 9.53
node_cpu_seconds_total{cpu="0",mode="irq"} 0
node_cpu_seconds_total{cpu="0",mode="nice"} 0.11
node_cpu_seconds_total{cpu="0",mode="softirq"} 109.74
node_cpu_seconds_total{cpu="0",mode="steal"} 0
node_cpu_seconds_total{cpu="0",mode="system"} 566.67
node_cpu_seconds_total{cpu="0",mode="user"} 1220.36
node_cpu_seconds_total{cpu="1",mode="idle"} 13501.28
node_cpu_seconds_total{cpu="1",mode="iowait"} 5.96
node_cpu_seconds_total{cpu="1",mode="irq"} 0
node_cpu_seconds_total{cpu="1",mode="nice"} 0.09
node_cpu_seconds_total{cpu="1",mode="softirq"} 23.74
node_cpu_seconds_total{cpu="1",mode="steal"} 0
node_cpu_seconds_total{cpu="1",mode="system"} 423.84
node_cpu_seconds_total{cpu="1",mode="user"} 936.05
```

For each CPU, the modes will in aggregate increase by one second per
second. This []{#ch07.xhtml#idm45207100763616
primary="avg without expression" data-type="indexterm"}allows you to
calculate the proportion of idle time across all CPUs using the PromQL
expression:

``` {data-type="programlisting"}
avg without(cpu, mode)(rate(node_cpu_seconds_total{mode="idle"}[1m]))
```

This works as it calculates the idle time per second per CPU and then
averages that across all the CPUs in the
machine.[]{#ch07.xhtml#idm45207100761392
primary="idle time per second per CPU" data-type="indexterm"}

You could generalize this to calculate the proportion of time spent in
each mode for a machine using:

``` {data-type="programlisting"}
avg without(cpu)(rate(node_cpu_seconds_total[1m]))
```

CPU usage by guests (i.e., virtual machines running under the kernel) is
already included in the `user` and `nice` modes.
[]{#ch07.xhtml#idm45207100757856 primary="node_cpu_guest_seconds_total"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100757152
primary="user mode"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100756480
primary="nice mode"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100755808
primary="guests, CPU usage by" data-type="indexterm"}You can see guest
time separately in the `node_cpu_guest_seconds_total` metric.
:::
:::

::: {.section pdf-bookmark="Filesystem Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100754432 .sect1}
# Filesystem Collector

The filesystem collector unsurprisingly collects metrics about your
*mounted* [filesystems]{.keep-together}, just as you would obtain from
the `df` command.[]{#ch07.xhtml#idm45207100750912 primary="df command"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100750176
primary="mounted filesystems, metrics on"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100749536
primary="filesystem collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100748864
primary="Node Exporter" secondary="filesystem collector"
data-type="indexterm"} The
[`--collector.`]{.keep-together}`filesystem.mount-points-exclude` and
[`--collector.filesystem.​`]{.keep-together}`fs-types-exclude` flags
allow restricting which filesystems are included (the defaults exclude
various pseudofilesystems). As you will not have Node Exporter running
as root, you will need to ensure that file permissions allow it to use
the `statfs` system call on mountpoints of interest to
you.[]{#ch07.xhtml#idm45207100745008 primary="statfs system call"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100744304
primary="node_filesystem prefix (metrics)"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100743568
primary="device labels"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100742896
primary="fstype labels"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100742224
primary="mountpoint labels" data-type="indexterm"}

All metrics from this collector are prefixed with `node_filesystem_` and
have `device`, `fstype`, and `mountpoint` labels:

``` {data-type="programlisting"}
# HELP node_filesystem_size_bytes Filesystem size in bytes.
# TYPE node_filesystem_size_bytes gauge
node_filesystem_size_bytes{device="/dev/sda5",fstype="ext4",mountpoint="/"} 9e+10
```

The filesystem metrics are largely self-evident. The one subtlety you
should be aware of is the difference between
`node_filesystem_avail_bytes` and `node_filesystem_free_bytes`.
[]{#ch07.xhtml#idm45207100736848
primary="node_filesystem_avail_bytes versus  node_filesystem_free_bytes"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100736096 primary="Unix"
secondary="filesystems"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100735152
primary="root user" data-type="indexterm"}On Unix filesystems some space
is reserved for the root user, so that they can still do things when
users fill up all available space. `node_filesystem_avail_bytes` is the
space available to users, and when trying to calculate used disk space
you should accordingly use:

``` {data-type="programlisting"}
  node_filesystem_avail_bytes
/
  node_filesystem_size_bytes
```

`node_filesystem_files` and `node_filesystem_files_free` indicate the
number of inodes and how many of them are free, which are roughly
speaking the number of files your filesystem has. You can also see this
with `df -i`.[]{#ch07.xhtml#idm45207100731536
primary="node_filesystem_files_free"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100730736
primary="node_filesystem_files" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Diskstats Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100729808 .sect1}
# Diskstats Collector

The diskstats collector exposes disk I/O metrics from
*/proc/diskstats*.[]{#ch07.xhtml#idm45207100727472 primary="disk I/O"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100726736
primary="I/O, disk"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100726064
primary="diskstats collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100725392
primary="Node Exporter" secondary="diskstats collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100724448
primary="--collector.diskstats.device-exclude flag"
primary-sortas="collector.diskstats" data-type="indexterm"} By default,
the `--collector.diskstats.device-exclude` flag attempts to exclude
things that are not real disks, such as partitions and loopback devices:

``` {data-type="programlisting"}
# HELP node_disk_io_now The number of I/Os currently in progress.
# TYPE node_disk_io_now gauge
node_disk_io_now{device="sda"} 0
```

All metrics have a `device` label, and
almost[]{#ch07.xhtml#idm45207100721248 primary="counters"
secondary="Node Exporter diskstat metrics" data-type="indexterm"} all
are counters, as follows:

`node_disk_io_now`

:   The number of I/Os in progress

`node_disk_io_time_seconds_total`

:   Incremented when I/O is in progress

`node_disk_read_bytes_total`

:   Bytes read by I/Os

```{=html}
<!-- -->
```

`node_disk_read_time_seconds_total`

:   The time taken by read I/Os

`node_disk_reads_completed_total`

:   The number of complete I/Os

`node_disk_written_bytes_total`

:   Bytes written by I/Os

`node_disk_write_time_seconds_total`

:   The time taken by write I/Os

`node_disk_writes_completed_total`

:   The number of complete write I/Os

These mostly mean what you think, but take a look at the [kernel
documentation](https://oreil.ly/xcAUs)^[4](#ch07.xhtml#idm45207100705824){#ch07.xhtml#idm45207100705824-marker
data-type="noteref"}^ for more details.

You can use `node_disk_io_time_seconds_total` to
[]{#ch07.xhtml#idm45207100702768
primary="node_disk_io_time_seconds_total"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100702000
primary="disk I/O"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100701328
primary="I/O, disk" data-type="indexterm"}calculate disk I/O
utilization, as would be shown by `iostat -x`:

``` {data-type="programlisting"}
rate(node_disk_io_time_seconds_total[1m])
```

You can calculate the average time for a read I/O with:

``` {data-type="programlisting"}
  rate(node_disk_read_time_seconds_total[1m])
/
  rate(node_disk_reads_completed_total[1m])
```
:::
:::

::: {.section pdf-bookmark="Netdev Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100729216 .sect1}
# Netdev Collector

The netdev collector exposes metrics
about[]{#ch07.xhtml#idm45207100696576 primary="netdev collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100695872
primary="Node Exporter" secondary="netdev collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100694928
primary="node_network prefix (metrics)"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100694240
primary="device labels" data-type="indexterm"} your network devices with
the prefix `node_network_` and a `device` label:

``` {data-type="programlisting"}
# HELP node_network_receive_bytes_total Network device statistic receive_bytes.
# TYPE node_network_receive_bytes_total counter
node_network_receive_bytes_total{device="lo"} 8.3213967e+07
node_network_receive_bytes_total{device="wlan0"} 7.0854462e+07
```

`node_network_receive_bytes_total` and
`node_network_transmit_bytes_total` are the main metrics you will care
about[]{#ch07.xhtml#idm45207100690608
primary="node_network_transmit_bytes_total"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100689872
primary="node_network_receive_bytes_total" data-type="indexterm"} as you
can calculate network bandwidth in and out with them:

``` {data-type="programlisting"}
rate(node_network_receive_bytes_total[1m])
```

You may also be interested in `node_network_receive_packets_total` and
`node_network_transmit_packets_total`, which track packets in and out,
respectively.
:::
:::

::: {.section pdf-bookmark="Meminfo Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100686720 .sect1}
# Meminfo Collector

The meminfo collector has all your standard memory-related metrics with
a `node_memory_` prefix. []{#ch07.xhtml#idm45207100684864
primary="meminfo collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100684128
primary="Node Exporter" secondary="meminfo collector"
data-type="indexterm"}These all come from your */proc/meminfo*, and this
is the first collector where semantics get a bit muddy. The collector
does convert kilobytes to preferred bytes, but beyond that it's up to
you to know enough from [the documentation](https://oreil.ly/F-0JW) and
experience with Linux internals to understand what these metrics mean:

``` {data-type="programlisting"}
# HELP node_memory_MemTotal_bytes Memory information field MemTotal.
# TYPE node_memory_MemTotal_bytes gauge
node_memory_MemTotal_bytes 3.285016576e+10
```

For example, `node_memory_MemTotal_bytes` is the
total^[5](#ch07.xhtml#idm45207100679888){#ch07.xhtml#idm45207100679888-marker
data-type="noteref"}^ amount of physical memory in the machine---nice
and obvious. But note that there is no used memory metric, so you have
to somehow calculate it and thus how much memory is not used from other
metrics.[]{#ch07.xhtml#idm45207100679200
primary="node_memory_MemFree_bytes"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100678432
primary="node_memory_MemTotal_bytes" data-type="indexterm"}

`node_memory_MemFree_bytes` is the amount of memory that isn't used by
anything, but that doesn't mean it is all the memory you have to
spare.[]{#ch07.xhtml#idm45207100676752
primary="node_memory_Buffers_bytes"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100676032
primary="node_memory_Cached_bytes" data-type="indexterm"} In theory your
page cache (`node_memory_Cached_bytes`) can be reclaimed, as can your
write buffers (`node_memory_Buffers_bytes`), but that could adversely
affect performance for some
applications.^[6](#ch07.xhtml#idm45207100674288){#ch07.xhtml#idm45207100674288-marker
data-type="noteref"}^ In addition, there are various other kernel
structures using memory such as slab and page tables.

`node_memory_MemAvailable` is a heuristic from the kernel for how much
memory is really available, but was only added in version 3.14 of Linux.
[]{#ch07.xhtml#idm45207100672736 primary="node_memory_MemAvailable"
data-type="indexterm"}If you are running a new enough kernel, this is a
metric you could use to detect memory exhaustion.
:::
:::

::: {.section pdf-bookmark="Hwmon Collector" data-type="sect1"}
::: {#ch07.xhtml#hwmon .sect1}
# Hwmon Collector

When on bare metal, the hwmon collector provides metrics such as
temperature and fan speeds with a `node_hwmon_` prefix.
[]{#ch07.xhtml#idm45207100669360 primary="node_hwmon prefix (metrics)"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100668608
primary="hwmon collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100667936
primary="sensors command"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100667264
primary="Node Exporter" secondary="hwmon collector"
data-type="indexterm"}This is the same information you can obtain with
the `sensors` command:

``` {data-type="programlisting"}
# HELP node_hwmon_sensor_label Label for given chip and sensor
# TYPE node_hwmon_sensor_label gauge
node_hwmon_sensor_label{chip="platform_coretemp_0",
    label="core_0",sensor="temp2"} 1
node_hwmon_sensor_label{chip="platform_coretemp_0",
    label="core_1",sensor="temp3"} 1
# HELP node_hwmon_temp_celsius Hardware monitor for temperature (input)
# TYPE node_hwmon_temp_celsius gauge
node_hwmon_temp_celsius{chip="platform_coretemp_0",sensor="temp1"} 42
node_hwmon_temp_celsius{chip="platform_coretemp_0",sensor="temp2"} 42
node_hwmon_temp_celsius{chip="platform_coretemp_0",sensor="temp3"} 41
```

`node_hwmon_temp_celsius` is the temperature of various of your
components, which may also have sensor
labels^[7](#ch07.xhtml#idm45207100664320){#ch07.xhtml#idm45207100664320-marker
data-type="noteref"}^ exposed in
`node_hwmon_sensor_label`.[]{#ch07.xhtml#idm45207100662128
primary="node_hwmon_temp_celsius"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100661392
primary="sensor labels"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100660720
primary="node_hwmon_sensor_label" data-type="indexterm"}

While it is not the case for all hardware, for
some^[8](#ch07.xhtml#idm45207100659600){#ch07.xhtml#idm45207100659600-marker
data-type="noteref"}^ you will need the sensor label to understand what
the sensor is. In the preceding metrics, `temp3` represents CPU core
number 1.[]{#ch07.xhtml#idm45207100658336 primary="group_left"
data-type="indexterm"}

You can join the `label` label from `node_hwmon_sensor_label` to
`node_​hwmon_temp_celsius` using `group_left`, which is further discussed
in ["Many-to-One and
group_left"](#ch15.xhtml#group_left){data-type="xref"}:

``` {data-type="programlisting"}
  node_hwmon_temp_celsius
* ignoring(label) group_left(label)
  node_hwmon_sensor_label
```
:::
:::

::: {.section pdf-bookmark="Stat Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100653696 .sect1}
# Stat Collector

The stat []{#ch07.xhtml#idm45207100652192 primary="stat collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100651456
primary="Node Exporter" secondary="stat collector"
data-type="indexterm"}collector is a bit of a mix, as it provides
metrics from
*/proc/stat*.^[9](#ch07.xhtml#idm45207100649920){#ch07.xhtml#idm45207100649920-marker
data-type="noteref"}^

`node_boot_time_seconds` is when the kernel started, from
[]{#ch07.xhtml#idm45207100648544 primary="node_boot_time_seconds"
data-type="indexterm"}which you can calculate how long the kernel has
been up:

``` {data-type="programlisting"}
time() - node_boot_time_seconds
```

`node_intr_total` indicates the number of hardware interrupts you have
had. It isn't called `node_interrupts_total`, as that is used by the
interrupts collector, which is disabled by default due to high
cardinality.[]{#ch07.xhtml#idm45207100645648 primary="node_intr_total"
data-type="indexterm"}

The other metrics relate to processes. `node_forks_total` is a counter
for the number of `fork` syscalls, `node_context_switches_total` is the
number of context switches, while `node_procs_blocked` and
`node_procs_running` indicate the number of processes that are blocked
or running.[]{#ch07.xhtml#idm45207100642064 primary="fork syscalls"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100641280
primary="context switches"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100640608
primary="processes" secondary="blocked or running, metrics on"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Uname Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100639600 .sect1}
# Uname Collector

The uname collector exposes a single metric, `node_uname_info`,
which[]{#ch07.xhtml#idm45207100637680 primary="node_uname_info"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100636944
primary="uname collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100636272
primary="Node Exporter" secondary="uname collector"
data-type="indexterm"} you already saw in ["Stat
Panel"](#ch06.xhtml#grafana_stat){data-type="xref"}:

``` {data-type="programlisting"}
# HELP node_uname_info Labeled system information as provided by the uname
    system call.
# TYPE node_uname_info gauge
node_uname_info{domainname="(none)",machine="x86_64",nodename="kozo",
    release="4.4.0-101-generic",sysname="Linux",
    version="#124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017"} 1
```

The `nodename` label is the hostname of the machine, which may differ
from the `instance` target label (see ["Target
Labels"](#ch08.xhtml#target_labels){data-type="xref"}) or any other
names, such as in DNS, that you may have for
it.[]{#ch07.xhtml#idm45207100631456 primary="nodename label"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100630704
primary="target labels" secondary="instance target label"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100629760
primary="instance labels" data-type="indexterm"}

To count []{#ch07.xhtml#idm45207100628704 primary="count"
secondary="count by" data-type="indexterm"}how many machines run which
kernel version, you could use:

``` {data-type="programlisting"}
count by(release)(node_uname_info)
```
:::
:::

::: {.section pdf-bookmark="OS Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100626704 .sect1}
# OS Collector

The OS collector[]{#ch07.xhtml#idm45207100625200 primary="OS collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100624464
primary="Node Exporter" secondary="OS collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100623520
primary="node_os_info"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100622848
primary="node_os_version" data-type="indexterm"} exposes two metrics,
`node_os_info` and `node_os_version`, which provide you with operating
system information:

``` {data-type="programlisting"}
# HELP node_os_info A metric with a constant '1' value labeled by
    build_id, id, id_like, image_id, image_version, name,
    pretty_name, variant, variant_id, version, version_codename,
    version_id.
# TYPE node_os_info gauge
node_os_info{build_id="22.05.20220912.bf014ca",id="nixos",
    id_like="",image_id="",image_version="",name="NixOS",
    pretty_name="NixOS 22.05 (Quokka)",variant="",
    variant_id="",version="22.05 (Quokka)",
    version_codename="quokka",version_id="22.05"} 1
# HELP node_os_version Metric containing the major.minor
    part of the OS version.
# TYPE node_os_version gauge
node_os_version{id="nixos",id_like="",name="NixOS"} 22.05
```

To count how many machines run which distro version, you could use:

``` {data-type="programlisting"}
count by(name, version)(node_os_info)
```
:::
:::

::: {.section pdf-bookmark="Loadavg Collector" data-type="sect1"}
::: {#ch07.xhtml#idm45207100619120 .sect1}
# Loadavg Collector

The loadavg collector provides the 1-, 5-, and 15-minute load averages
as `node_load1`, `node_load5`, and `node_load15`,
respectively.[]{#ch07.xhtml#idm45207100616208
primary="loadavg collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100615472
primary="Node Exporter" secondary="loadavg collector"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100614528
primary="average load" data-type="indexterm"}

The meaning of this metric varies across platforms, and may not mean
what you think it does. For example, on Linux it is not just the number
of processes waiting in the run queue, but also uninterruptible
processes such as those waiting for I/O.

::: {.note data-type="note"}
###### Note

If your kernel is recent enough, we recommend that you use the pressure
collector, as described in ["Pressure
Collector"](#ch07.xhtml#psi_collector){data-type="xref"}.
:::

Load averages can be useful for a quick idea if a machine has gotten
busier (for some definition of busier) recently, but they are not a good
choice to alert on. For a more detailed look we recommend Brendan
Gregg's blog, ["Linux Load Averages: Solving the
Mystery"](https://oreil.ly/JVKfd).

> Its a silly number but people think its important.
>
> A comment in the Linux loadavg.c
:::
:::

::: {.section pdf-bookmark="Pressure Collector" data-type="sect1"}
::: {#ch07.xhtml#psi_collector .sect1}
# Pressure Collector

Pressure Stall Information (PSI) was introduced in Linux kernel 4.20.
These metrics measure resource pressure for three resources: CPU,
memory, and I/O. It needs to be enabled in the kernel during compilation
time.[]{#ch07.xhtml#idm45207100606464
primary="Pressure Stall Information (PSI)"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100605792
primary="resource pressure for CPU, memory, and I/O"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100605024
primary="Node Exporter" secondary="pressure collector"
data-type="indexterm"}

Your kernel might be built with PSI support but it could be disabled by
default, in which case you can pass `psi=1` on the kernel command line
during boot to enable it.

Five different metrics are exposed by the PSI collector:

``` {data-type="programlisting"}
# HELP node_pressure_cpu_waiting_seconds_total
    Total time in seconds that processes have waited for CPU time
# TYPE node_pressure_cpu_waiting_seconds_total counter
node_pressure_cpu_waiting_seconds_total 113.6605130
# HELP node_pressure_io_stalled_seconds_total
    Total time in seconds no process could make progress due to IO congestion
# TYPE node_pressure_io_stalled_seconds_total counter
node_pressure_io_stalled_seconds_total 8.630361
# HELP node_pressure_io_waiting_seconds_total
    Total time in seconds that processes have waited due to IO congestion
# TYPE node_pressure_io_waiting_seconds_total counter
node_pressure_io_waiting_seconds_total 9.609997
# HELP node_pressure_memory_stalled_seconds_total
    Total time in seconds no process could make progress
# TYPE node_pressure_memory_stalled_seconds_total counter
node_pressure_memory_stalled_seconds_total 0
# HELP node_pressure_memory_waiting_seconds_total
    Total time in seconds that processes have waited for memory
# TYPE node_pressure_memory_waiting_seconds_total counter
node_pressure_memory_waiting_seconds_total 0
```

`waiting` metrics indicate the total amount of seconds that some tasks
have been waiting, and `stalled` means that all tasks were delayed by
lack of resources.[]{#ch07.xhtml#idm45207100599520
primary="waiting metrics"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100598736
primary="stalled metrics" data-type="indexterm"} Memory and I/O have
both `waiting` and `stalled` metrics, where CPU only has `waiting`. This
is because a CPU is always executing a process.

As those are[]{#ch07.xhtml#idm45207100596336 primary="rate function"
secondary="using to determine if resources are overloaded"
data-type="indexterm"} counters, you can use them with the `rate()`
function to determine whether some resources are overloaded:

``` {data-type="programlisting"}
rate(node_pressure_memory_waiting_seconds_total[1m])
```
:::
:::

::: {.section pdf-bookmark="Textfile Collector" data-type="sect1"}
::: {#ch07.xhtml#textfile_collector .sect1}
# Textfile Collector

The textfile collector is a bit different from the collectors we have
already shown you. It doesn't obtain metrics from the kernel, but rather
from files that you produce.[]{#ch07.xhtml#ix_NdEtxtfl
primary="Node Exporter" secondary="textfile collector"
data-type="indexterm"}[]{#ch07.xhtml#ix_txtfl
primary="textfile collector" data-type="indexterm"}

The Node Exporter is not meant to run as root, so metrics such as those
from
SMART^[10](#ch07.xhtml#idm45207100589024){#ch07.xhtml#idm45207100589024-marker
data-type="noteref"}^ require root privileges to run the `smartctl`
command.[]{#ch07.xhtml#idm45207100587712 primary="SMART metrics"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100586976
primary="smartctl command" data-type="indexterm"}

In addition to metrics that require root, you can only obtain some
information by running a command such as `iptables`. For reliability,
the Node Exporter does not start
processes.[]{#ch07.xhtml#idm45207100585376 primary="iptables command"
data-type="indexterm"}

To use the textfile collector you would create a cronjob
[]{#ch07.xhtml#idm45207100584208 primary="cronjobs"
data-type="indexterm"}that regularly runs commands such as `smartctl` or
`iptables`, converts its output into the Prometheus text exposition
format, and atomically writes it to a file in a specific directory. On
every scrape, the Node Exporter will read the files in that directory
and include their metrics in its output.

::: {.note data-type="note"}
###### Note

The Prometheus server cannot read text files directly, therefore you
need a software to expose the file as HTTP. While you could use any HTTP
server, the Node Exporter also checks that the metrics are correct and
is able to expose metrics coming from multiple files.
:::

You can use this collector to add in your own metrics via cronjobs, or
you could have more static information that comes from files written out
by your machine configuration management system to provide some info
metrics (discussed in
["Info"](#ch05.xhtml#info_metrics){data-type="xref"}), such as which
Chef^[11](#ch07.xhtml#idm45207100580032){#ch07.xhtml#idm45207100580032-marker
data-type="noteref"}^ roles it has, about the machine.

As with the Node Exporter generally, the textfile collector is intended
for metrics about a machine. For example, there might be some kernel
metric that the Node Exporter does not yet expose, or that requires root
to access. You might want to track more operating system-level metrics,
such as if there are pending package updates or a reboot due.
[]{#ch07.xhtml#idm45207100578720 primary="batch jobs"
secondary="recording when Cassandra backups completed"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100577648
primary="Cassandra" data-type="indexterm"}While it is technically a
service rather than an operating system metric, recording when batch
jobs such as backups last completed for the
Cassandra^[12](#ch07.xhtml#idm45207100576688){#ch07.xhtml#idm45207100576688-marker
data-type="noteref"}^ node running on the machine would also be a good
use of the textfile collector, as your interest in whether the backups
worked on that machine goes away when the machine does. That is to say
the Cassandra node has the same lifecycle as the
machine.^[13](#ch07.xhtml#idm45207100576032){#ch07.xhtml#idm45207100576032-marker
data-type="noteref"}^

The textfile collector should not be used to try to convert Prometheus
to push. Nor should you use the textfile collector as a way to take
metrics from other exporters and applications running on the machine and
expose them all on the Node Exporter's */metrics*, but rather have
Prometheus scrape each exporter and application
[individually]{.keep-together}.

::: {.section pdf-bookmark="Using the Textfile Collector" data-type="sect2"}
::: {#ch07.xhtml#idm45207100572752 .sect2}
## Using the Textfile Collector

The textfile collector is enabled by default, but you must provide the
[`--collector.textfile.directory`]{.keep-together} command-line flag to
the Node Exporter for it to work. []{#ch07.xhtml#idm45207100569968
primary="--collector.textfie.directory flag"
primary-sortas="collector.textfile"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100568912
primary="textfile collector" secondary="using"
data-type="indexterm"}This should point to a directory that you use
solely for this purpose to avoid mixups.

To try this out you should create a directory, write out a simple file
in the exposition format (as discussed in ["Text Exposition
Format"](#ch04.xhtml#exposition_format){data-type="xref"}), and start
the Node Exporter with it configured to use this directory, as shown in
[Example 7-1](#ch07.xhtml#textfile_simple){data-type="xref"}. The
textfile collector only looks at files with the *.prom* extension.

::: {#ch07.xhtml#textfile_simple data-type="example"}
##### [Example 7-1. ]{.label}Using the textfile collector with a simple example

``` {data-type="programlisting"}
hostname $ mkdir textfile
hostname $ echo example_metric 1 > textfile/example.prom
hostname $ ./node_exporter --collector.textfile.directory=$PWD/textfile
```
:::

[Example 7-2](#ch07.xhtml#textfile_simple2){data-type="xref"} shows the
content of the file created by
[Example 7-1](#ch07.xhtml#textfile_simple){data-type="xref"}.

::: {#ch07.xhtml#textfile_simple2 data-type="example"}
##### [Example 7-2. ]{.label}The content of *textfile/example.prom*

``` {data-type="programlisting"}
example_metric 1
```
:::

If you look at the Node Exporter's */metrics*, you will now see your
metric:

``` {data-type="programlisting"}
# HELP example_metric Metric read from /some/path/textfile/example.prom
# TYPE example_metric untyped
example_metric 1
```

::: {.warning data-type="warning"}
###### Warning

If no `HELP` is provided, the textfile collector will provide one for
you.[]{#ch07.xhtml#idm45207100553936 primary="HELP (metrics)"
data-type="indexterm"} If you are putting the same metric in multiple
files (with different labels of course), you need to provide the same
`HELP` for each, as otherwise the mismatched `HELP` will cause an error.
:::

Usually you will create and update the *.prom* files with a cronjob. As
a scrape can happen at any time, it is important that the Node Exporter
does not see partially written files.[]{#ch07.xhtml#idm45207100550864
primary="rename system call" data-type="indexterm"} To this end you
should write first to a temporary file in the same directory and then
move the complete file to the final
filename.^[14](#ch07.xhtml#idm45207100549888){#ch07.xhtml#idm45207100549888-marker
data-type="noteref"}^

[Example 7-3](#ch07.xhtml#textfile_crontab){data-type="xref"} shows a
cronjob that outputs to the textfile
collector.[]{#ch07.xhtml#idm45207100547744 primary="cronjobs"
secondary="outputting to textfile collector" data-type="indexterm"} It
creates the metrics in a temporary
file,^[15](#ch07.xhtml#idm45207100546464){#ch07.xhtml#idm45207100546464-marker
data-type="noteref"}^ and renames them to the final filename. This is a
trivial example that uses short commands, but in most real-world use
cases you will want to create a script to keep things readable.

::: {#ch07.xhtml#textfile_crontab data-type="example"}
##### [Example 7-3. ]{.label}/etc/crontab that exposes the number of lines in /etc/shadow as the `shadow_entries` metric using the textfile collector

``` {data-type="programlisting"}
TEXTFILE=/path/to/textfile/directory

# This must all be on one line
*/5 * * * * root (echo -n 'shadow_entries '; grep -c . /etc/shadow)
    > $TEXTFILE/shadow.prom.$$
    && mv $TEXTFILE/shadow.prom.$$ $TEXTFILE/shadow.prom
```
:::

A number of example scripts for use with the textfile collector are
available in the [textfile collector example scripts GitHub
repository](https://oreil.ly/HMkDo).
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="Timestamps" data-type="sect2"}
::: {#ch07.xhtml#idm45207100572096 .sect2}
## Timestamps

While the exposition format supports timestamps, you cannot use them
with the textfile collector. []{#ch07.xhtml#idm45207100539552
primary="textfile collector" secondary="timestamps"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100538576
primary="timestamps" secondary="textfile collector and"
data-type="indexterm"}This is because it doesn't make sense
semantically, as your metrics would not appear with the same timestamp
as other metrics from the scrape.[]{#ch07.xhtml#idm45207100537344
primary="mtime" data-type="indexterm"}

Instead, the
mtime^[16](#ch07.xhtml#idm45207100536288){#ch07.xhtml#idm45207100536288-marker
data-type="noteref"}^ of the file is available to you in the
`node_textfile_mtime_​`[`seconds`]{.keep-together} metric. You can use
this to alert on your cronjobs not working, because if this value is
from too long ago it can indicate a problem:

``` {data-type="programlisting"}
# HELP node_textfile_mtime_seconds Unixtime mtime of textfiles successfully read.
# TYPE node_textfile_mtime_seconds gauge
node_textfile_mtime_seconds{file="example.prom"} 1.516205651e+09
```

Now that you have the Node Exporter running, let's look at how you can
tell Prometheus about all the machines you have it running
on.[]{#ch07.xhtml#idm45207100532864 primary="Node Exporter"
secondary="textfile collector" startref="ix_NdEtxtfl"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100531616
primary="textfile collector" startref="ix_txtfl"
data-type="indexterm"}[]{#ch07.xhtml#idm45207100530672
primary="Node Exporter" startref="ix_NdE" data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch07.xhtml#idm45207100799344-marker)^ The Node Exporter has
nothing to do with Node.js; it's node in the sense of compute node.

^[2](#ch07.xhtml#idm45207100794144-marker)^ The Windows Exporter was
previously known as the WMI Exporter.

^[3](#ch07.xhtml#idm45207100784032-marker)^ Docker is a platform for
developers and system administrators to build, package, and deploy
applications in containers. A container is a lightweight, portable, and
self-sufficient packaging technology that allows developers to deploy an
application and its dependencies as a single unit.

^[4](#ch07.xhtml#idm45207100705824-marker)^ A sector is always 512 bytes
in */proc/diskstats*; you do not need to worry if your disks are using
larger sector sizes.[]{#ch07.xhtml#idm45207100704752 primary="sectors"
data-type="indexterm"} This is an example of something that is only
apparent from reading the Linux source code.

^[5](#ch07.xhtml#idm45207100679888-marker)^ Almost.

^[6](#ch07.xhtml#idm45207100674288-marker)^ Prometheus 2.0, for example,
relies on page cache.

^[7](#ch07.xhtml#idm45207100664320-marker)^ *Labels* here does not mean
Prometheus labels; they are sensor labels and come from files such as
*/sys/devices/platform/coretemp.0/hwmon/hwmon1/temp3_label*.

^[8](#ch07.xhtml#idm45207100659600-marker)^ Such as our laptop, which
the preceding metric output is from.

^[9](#ch07.xhtml#idm45207100649920-marker)^ It used to also provide CPU
metrics, which have now been refactored into the CPU collector.

^[10](#ch07.xhtml#idm45207100589024-marker)^ Self-Monitoring, Analysis,
and Reporting Technology, metrics from hard drives that can be useful to
predict and detect failure.

^[11](#ch07.xhtml#idm45207100580032-marker)^ Chef is a configuration
management tool that allows for automated infrastructure provisioning
and management through the use of reusable scripts called "cookbooks."

^[12](#ch07.xhtml#idm45207100576688-marker)^ A distributed database.

^[13](#ch07.xhtml#idm45207100576032-marker)^ If a metric about a batch
job has a different lifecycle than the machine, it is likely a
service-level batch job and you may wish to use the Pushgateway, as
discussed in ["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}.

^[14](#ch07.xhtml#idm45207100549888-marker)^ The `rename` system call is
atomic, but can only be used on the same filesystem.

^[15](#ch07.xhtml#idm45207100546464-marker)^ `$$` in shell expands to
the current process ID (pid).

^[16](#ch07.xhtml#idm45207100536288-marker)^ The mtime is the last time
the file was modified.
:::
:::
:::

[]{#ch08.xhtml}

::: {#ch08.xhtml#sbo-rt-content}
::: {#ch08.xhtml#service_discovery_chapter .chapter}
# [Chapter 8. ]{.label}Service Discovery

Thus far you've had Prometheus find what to scrape using static
configuration via `static_configs`. []{#ch08.xhtml#ix_serdi
primary="service discovery" data-type="indexterm"}This is fine for
simple use
cases,^[1](#ch08.xhtml#idm45207100526768){#ch08.xhtml#idm45207100526768-marker
data-type="noteref"}^ but having to manually keep your *prometheus.yml*
up to date as machines are added and removed would get annoying,
particularly if you were in a dynamic environment where new instances
might be brought up every minute. This chapter will show you how you can
let Prometheus know what to scrape.

You already know where all of your machines and services are, and how
they are laid out. Service discovery (SD) enables you to provide that
information to Prometheus from whichever database you store it in.
[]{#ch08.xhtml#idm45207100524688 primary="SD" see="service discovery"
data-type="indexterm"}Prometheus supports many common sources of service
information, such as Consul, Amazon EC2, and Kubernetes out of the box.
[]{#ch08.xhtml#idm45207100523440 primary="Consul"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100522768
primary="Kubernetes"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100522096
primary="EC2 (Elastic Compute Cloud)" data-type="indexterm"}If your
particular source isn't already supported, you can use the file-based
and HTTP-based service discovery mechanisms to hook it
in.[]{#ch08.xhtml#idm45207100521120
primary="file service discovery (file SD)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100520432
primary="configuration management systems"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100519744 primary="Ansible"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100519072 primary="Chef"
data-type="indexterm"} For file-based service discovery, this could be
by having your configuration management system, such as Ansible or Chef,
write the list of machines and services they know about in the right
format, or a script running regularly to pull it from whatever data
source you use.[]{#ch08.xhtml#idm45207100518272
primary="HTTP service discovery (HTTP SD)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100517584 primary="NetBox"
data-type="indexterm"} For HTTP-based service discovery, third-party
tools such as [NetBox](https://oreil.ly/-cbF3) offer plug-ins that can
be installed to offer Prometheus-compatible HTTP service discovery
endpoints. Note that some SD
projects^[2](#ch08.xhtml#idm45207100515904){#ch08.xhtml#idm45207100515904-marker
data-type="noteref"}^ support both HTTP-based service discovery and
file-based service discovery.[]{#ch08.xhtml#idm45207100514624
primary="Prometheus vCloud Director SD" data-type="indexterm"}

Knowing what your monitoring targets are, and thus what should be
scraped, is only the first step. Labels are a key part of Prometheus
(see [Chapter 5](#ch05.xhtml#labels_chapter){data-type="xref"}), and
assigning *target labels* to targets allows them to be grouped and
organized in ways that make sense to
you.[]{#ch08.xhtml#idm45207100512160 primary="labels"
secondary="target labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100511184
primary="target labels" data-type="indexterm"} Target labels allow you
to aggregate targets performing the same role, that are in the same
environment, or are run by the same team.

As target labels are configured in Prometheus rather than in the
applications and exporters themselves, this allows your different teams
to have label hierarchies that make sense to them. Your infrastructure
team might care only about which rack and
PDU^[3](#ch08.xhtml#idm45207100509584){#ch08.xhtml#idm45207100509584-marker
data-type="noteref"}^ a machine is on, while your database team would
care that it is the PostgreSQL master for their production environment.
If you had a kernel developer who was investigating a rarely occurring
problem, they might just care which kernel version was in use.

Service discovery and the pull model allow all these views of the world
to coexist, as each of your teams can run their own Prometheus with the
target labels that make sense to them.

::: {.section pdf-bookmark="Service Discovery Mechanisms" data-type="sect1"}
::: {#ch08.xhtml#idm45207100505296 .sect1}
# Service Discovery Mechanisms

Service discovery is designed to integrate with the machine and service
databases that you already have.[]{#ch08.xhtml#ix_serdimch
primary="service discovery" secondary="mechanisms"
data-type="indexterm"} Out of the box, Prometheus 2.37.0 has support for
Azure, Consul, DigitalOcean, Docker, Docker Swarm, DNS, Eureka,
EC2,^[4](#ch08.xhtml#idm45207100502064){#ch08.xhtml#idm45207100502064-marker
data-type="noteref"}^ file-based service discovery,
GCE,^[5](#ch08.xhtml#idm45207100501312){#ch08.xhtml#idm45207100501312-marker
data-type="noteref"}^ Hetzner, HTTP-based service discovery, IONOS
Cloud, Kubernetes, Kuma, LightSail, Linode (Akamai), Marathon, Nerve,
Nomad, OpenStack, PuppetDB, Scaleway, Serverset, Uyuni, Triton, and
Vultr service discovery in addition to the static discovery you have
already seen.

Service discovery isn't just about you providing a list of machines to
Prometheus, or monitoring. It is a more general concern that you will
see across your systems; applications need to find their dependencies to
talk to, and hardware technicians need to know which machines are safe
to turn off and repair. Accordingly, you should not only have a raw list
of machines and services, but also conventions around how they are
organized and their lifecycles.

A good service discovery mechanism will provide you with *metadata*.
[]{#ch08.xhtml#idm45207100499328 primary="metadata"
secondary="provided by service discovery" data-type="indexterm"}This may
be the name of a service, its description, which team owns it,
structured tags about it, or anything else that you may find useful.
Metadata is what you will convert into target labels, and generally the
more metadata you have, the better.

A full discussion of service discovery is beyond the scope of this book.
If you haven't gotten around to formalizing your configuration
management and service databases yet, Consul tends to be a good place to
start.[]{#ch08.xhtml#idm45207100497232 primary="Consul"
data-type="indexterm"}

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch08.xhtml#idm45207100496400 .sidebar}
# Top-Down Versus Bottom-Up

There are two broad categories of service discovery mechanisms you will
come across. []{#ch08.xhtml#idm45207100495136
primary="service discovery" secondary="mechanisms"
tertiary="top-down versus bottom-up" data-type="indexterm"}Those where
the service instances register with service discovery, such as Consul,
are bottom-up. Those where instead the service discovery knows what
should be there, such as EC2, are top-down.

Both approaches are common. Top-down makes it easy for you to detect if
something is meant to be running but isn't. However, for bottom-up you
would need a separate reconciliation process to ensure things are in
sync, so that cases such as an application instance that stalls before
it can register are caught.
:::

```{=html}
</aside>
```
::: {.section pdf-bookmark="Static" data-type="sect2"}
::: {#ch08.xhtml#idm45207100492752 .sect2}
## Static

You have already seen static configuration in
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"},
where targets are provided directly in *prometheus.yml*.
[]{#ch08.xhtml#idm45207100489456
primary="static config  service discovery"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100488704
primary="service discovery" secondary="mechanisms" tertiary="static"
data-type="indexterm"}It is useful if you have a small and simple setup
that rarely changes. This might be your home network, a scrape config
that is only for a local Pushgateway, or even Prometheus scraping
itself, as in
[Example 8-1](#ch08.xhtml#sd_prometheus_self_scrape){data-type="xref"}.

::: {#ch08.xhtml#sd_prometheus_self_scrape data-type="example"}
##### [Example 8-1. ]{.label}Using static service discovery to have Prometheus scrape itself

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
      - localhost:9090
```
:::

If you are using a configuration management tool such as Ansible, you
could have its[]{#ch08.xhtml#idm45207100479840 primary="Ansible"
secondary="using its templating to create targets for Node Exporter"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100478992
primary="templating"
secondary="using Ansible jinja2 to create targets for Node Exporter"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100478144
primary="jinja2 templating (Ansible)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100477536 primary="targets"
secondary="creating in static config service discovery"
data-type="indexterm"} Jinja2 templating system write out a list of all
the machines it knows about to have their Node Exporters scraped, such
as in [Example 8-2](#ch08.xhtml#ansible_template){data-type="xref"}.

::: {#ch08.xhtml#ansible_template data-type="example"}
##### [Example 8-2. ]{.label}Using Ansible's templating to create targets for the Node Exporter on all machines

``` {data-type="programlisting"}
scrape_configs:
 - job_name: node
   static_configs:
    - targets:
{% for host in groups["all"] %}
      - {{ host }}:9100
{% endfor %}
```
:::

In addition to providing a list of targets, a static config can also
provide labels for those targets in the `labels` field.
[]{#ch08.xhtml#idm45207100447360 primary="labels"
secondary="provided for targets by static config"
data-type="indexterm"}If you find yourself needing this, then file SD,
covered in ["File"](#ch08.xhtml#file_sd){data-type="xref"}, tends to be
a better approach.

The plural in `static_configs` indicates that it is a list, and you can
specify multiple static configs in one scrape config, as shown in
[Example 8-3](#ch08.xhtml#two_static_configs){data-type="xref"}. While
[]{#ch08.xhtml#idm45207100443664 primary="static_configs"
data-type="indexterm"}there is not much point to doing this for static
configs, it can be useful with other service discovery mechanisms if you
want to talk to multiple data sources. You can even mix and match
service discovery mechanisms within a scrape config, though that is
unlikely to result in a particularly understandable configuration.

::: {#ch08.xhtml#two_static_configs data-type="example"}
##### [Example 8-3. ]{.label}Two monitoring targets are provided, each in its own static config

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: node
   static_configs:
    - targets: 
       - host1:9100
      labels:
        datacenter: paris
    - targets: 
       - host2:9100
       - host3:9100
      labels:
        datacenter: montreal
```
:::

[![1](assets/1.png){height="12" width="12"}](#ch08.xhtml#co_service_discovery_CO1-1){#ch08.xhtml#callout_service_discovery_CO1-1 .co}

:   The first static config, containing a single target, with a label
    `datacenter` set to `paris`.

[![2](assets/2.png){height="12" width="12"}](#ch08.xhtml#co_service_discovery_CO1-2){#ch08.xhtml#callout_service_discovery_CO1-2 .co}

:   The second static config, containing two targets, with a label
    `datacenter` set to `montreal`.

The same applies to `scrape_configs`, a list of scrape configs in which
you can specify as many as you like. The only restriction is that the
`job_name` must be unique.[]{#ch08.xhtml#idm45207100345280
primary="job_name"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100344544
primary="scrape_configs" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="File" data-type="sect2"}
::: {#ch08.xhtml#file_sd .sect2}
## File

File service discovery, usually referred to as *file SD*, does not use
the network. Instead, it reads monitoring targets from files you provide
on the local filesystem. []{#ch08.xhtml#ix_fileSD
primary="file service discovery (file SD)"
data-type="indexterm"}[]{#ch08.xhtml#ix_serdimchfile
primary="service discovery" secondary="mechanisms" tertiary="file SD"
data-type="indexterm"}This allows you to integrate with service
discovery systems Prometheus doesn't support out of the box, or when
Prometheus can't quite do the things you need with the metadata
available.

You can provide files in either JSON or YAML formats. The file extension
must be *.json* for JSON, and either *.yml* or *.yaml* for
YAML.[]{#ch08.xhtml#idm45207100297104 primary="JSON"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100296368 primary="YAML"
data-type="indexterm"} You can see a JSON example in
[Example 8-4](#ch08.xhtml#filesd_json){data-type="xref"}, which you
would put in a file called *filesd.json*. You can have as many or as few
targets as you like in a single file.

::: {#ch08.xhtml#filesd_json data-type="example"}
##### [Example 8-4. ]{.label}*filesd.json* with three targets

``` {code-language="json" data-type="programlisting"}
[
  {
    "targets": [ "host1:9100", "host2:9100" ],
    "labels": {
      "team": "infra",
      "job": "node"
    }
  },
  {
    "targets": [ "host1:9090" ],
    "labels": {
      "team": "monitoring",
      "job": "prometheus"
    }
  }
]
```
:::

::: {data-type="caution"}
###### Caution

The JSON format is not perfect. One issue you will likely encounter here
is that the last item in a list or hash cannot have a trailing comma. We
would recommend using a JSON library to generate JSON files rather than
trying to do it by hand.
:::

Configuration in Prometheus uses `file_sd_configs` in your scrape
config, as shown in
[Example 8-5](#ch08.xhtml#filesd_prometheus_yml){data-type="xref"}.
[]{#ch08.xhtml#idm45207100193312 primary="file_sd_configs"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100192608
primary="globs, use in filenames" data-type="indexterm"}Each file SD
config takes a list of filepaths, and you can use globs in the
filename.^[6](#ch08.xhtml#idm45207100191808){#ch08.xhtml#idm45207100191808-marker
data-type="noteref"}^ Paths are relative to Prometheus's working
directory, which is to say the directory you start Prometheus in.

::: {#ch08.xhtml#filesd_prometheus_yml data-type="example"}
##### [Example 8-5. ]{.label}*prometheus.yml* using file service discovery

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
```
:::

Usually you would not provide metadata for use with relabeling when
using file SD, but rather the ultimate target labels you would like to
have.[]{#ch08.xhtml#idm45207100158416 primary="target labels"
secondary="metadata for, in file SD"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100157568
primary="metadata" secondary="viewing for target labels in file SD"
data-type="indexterm"}

If you visit *http://localhost:9090/service-discovery* in your
browser^[7](#ch08.xhtml#idm45207100138752){#ch08.xhtml#idm45207100138752-marker
data-type="noteref"}^ and click "show more," you will see
[Figure 8-1](#ch08.xhtml#file_service_discovery){data-type="xref"}, with
both `job` and `team` labels []{#ch08.xhtml#idm45207100136400
primary="job labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100135664
primary="team labels" data-type="indexterm"}from
*filesd.json*.^[8](#ch08.xhtml#idm45207100134448){#ch08.xhtml#idm45207100134448-marker
data-type="noteref"}^ As these are made-up targets, the scrapes will
fail, unless you actually happen to have a `host1` and `host2` on your
network.

<figure class="smallereighty">
<div id="ch08.xhtml#file_service_discovery" class="figure">
<img src="assets/pur2_0801.png" width="600" height="693"
alt="Service Discovery status page showing three targets" />
<h6><span class="label">Figure 8-1. </span>Service discovery status page
showing three discovered targets from file SD</h6>
</div>
</figure>

Providing the targets with a file means it could come from templating in
a configuration management system, a daemon that writes it out
regularly, or even from a web service via a cronjob using
`wget`.[]{#ch08.xhtml#idm45207100127664 primary="rename system call"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100126928
primary="cronjobs" data-type="indexterm"} Changes are picked up
automatically using inotify, so it would be wise to ensure file changes
are made atomically using `rename`, similarly to how you did in
["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="HTTP" data-type="sect2"}
::: {#ch08.xhtml#http_sd .sect2}
## HTTP

HTTP service discovery, usually referred to as *HTTP SD*, fetches a
target list using HTTP.[]{#ch08.xhtml#idm45207100097488
primary="service discovery" secondary="mechanisms"
startref="ix_serdimchfile" tertiary="file SD"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100096160
primary="file service discovery (file SD)" startref="ix_fileSD"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100095312
primary="HTTP service discovery (HTTP SD)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100094704
primary="service discovery" secondary="mechanisms" tertiary="HTTP SD"
data-type="indexterm"} This mechanism makes it possible to integrate any
application directly with Prometheus, without the need to produce files
locally on the Prometheus server.[]{#ch08.xhtml#idm45207100093488
primary="targets" secondary="in HTTP service discovery"
secondary-sortas="HTTP" data-type="indexterm"}

With HTTP service discovery, Prometheus will refresh the target list
every
minute.^[9](#ch08.xhtml#idm45207100091904){#ch08.xhtml#idm45207100091904-marker
data-type="noteref"}^ As shown in
[Example 8-6](#ch08.xhtml#simple_http_example){data-type="xref"}, the
minimum configuration to provide to Prometheus is the URL of the service
discovery endpoint.

::: {#ch08.xhtml#simple_http_example data-type="example"}
##### [Example 8-6. ]{.label}*prometheus.yml* with `http_sd_configs`

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: cmdb
   http_sd_configs:
    - url: http://cmdb.local/prometheus-service-discovery
```
:::

Service discovery endpoints must use the same JSON format as in
[Example 8-4](#ch08.xhtml#filesd_json){data-type="xref"}. The HTTP
Header `Content-Type` must be `application/json` and the HTTP response
code must be 200. Unlike file SD, YAML is not supported by HTTP
SD.[]{#ch08.xhtml#idm45207100082288 primary="JSON"
secondary="use by HTTP SD endpoints" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

In case of a failure, the last discovered targets are kept by
Prometheus. As this targets list is not persisted on disk, failures
happening directly after a Prometheus restart will produce an empty
target list.

You can monitor the health of the HTTP service discovery with the
counter `prometheus_sd_http_failures_total`. If it is continuously
increasing, Prometheus can't refresh its
targets.[]{#ch08.xhtml#idm45207100061024
primary="health monitoring, HTTP SD"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100060384
primary="prometheus_sd_http_failures_total" data-type="indexterm"}
:::

The HTTP SD provides all the necessary HTTP options to
authenticate.[]{#ch08.xhtml#idm45207100059232 primary="http_sd_configs"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100058528
primary="authentication" secondary="in HTTP SD" secondary-sortas="HTTP"
data-type="indexterm"} An example with TLS certificates and credentials
is shown in [Example 8-7](#ch08.xhtml#secure_http_sd){data-type="xref"}.

::: {#ch08.xhtml#secure_http_sd data-type="example"}
##### [Example 8-7. ]{.label}*prometheus.yml* with `http_sd_configs` and security options

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: cmdb
   http_sd_configs:
    - url: http://cmdb.local/prometheus-service-discovery
      authorization:
        credentials_file: token
      tls_config:
        ca_file: ca.crt
```
:::
:::
:::

::: {.section pdf-bookmark="Consul" data-type="sect2"}
::: {#ch08.xhtml#consul_sd .sect2}
## Consul

Consul service discovery is a service discovery mechanism from
HashiCorp.[]{#ch08.xhtml#idm45207100003696 primary="Consul"
secondary="service discovery"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100002720
primary="service discovery" secondary="mechanisms" tertiary="Consul"
data-type="indexterm"} If you do not already have a service discovery
system within your organization, Consul is one of the easier ones to get
up and running. Consul has an agent that runs on each of your machines,
and these gossip among themselves.[]{#ch08.xhtml#idm45207100001376
primary="agents (Consul)" data-type="indexterm"} Applications talk only
to the local agent on a machine. Some number of agents are also servers,
providing persistence and consistency.

To try it out, you can set up a development Consul agent by following
[Example 8-8](#ch08.xhtml#consul_dev_setup){data-type="xref"}. If you
wish to use Consul in production, you should follow the official
[Getting Started](https://oreil.ly/SmhRW) guide.

::: {#ch08.xhtml#consul_dev_setup data-type="example"}
##### [Example 8-8. ]{.label}Setting up a Consul agent in development mode

``` {data-type="programlisting"}
hostname $ wget https://releases.hashicorp.com/consul/1.0.2/
    consul_1.0.2_linux_amd64.zip
hostname $ unzip consul_1.0.2_linux_amd64.zip
hostname $ ./consul agent -dev
```
:::

The Consul UI should now be available in your browser on
*http://localhost:8500/*. Consul has a notion of services, and in the
development setup has a single service, which is Consul itself. Next,
run a Prometheus with the configuration in
[Example 8-9](#ch08.xhtml#consul_prometheus_yml){data-type="xref"}.

::: {#ch08.xhtml#consul_prometheus_yml data-type="example"}
##### [Example 8-9. ]{.label}*prometheus.yml* using Consul service discovery

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: consul
   consul_sd_configs:
    - server: 'localhost:8500'
```
:::

Go to *http://localhost:9090/service-discovery* in your browser and you
will see the screen in
[Figure 8-2](#ch08.xhtml#consul_service_discovery){data-type="xref"},
showing that the Consul service discovery has discovered a single target
with some metadata, which became a target with `instance` and `job`
labels. []{#ch08.xhtml#idm45207099892576 primary="targets"
secondary="provided by Consul service discovery"
data-type="indexterm"}If you had more agents and services, they would
also show up here.

<figure>
<div id="ch08.xhtml#consul_service_discovery" class="figure">
<img src="assets/pur2_0802.png" width="600" height="614"
alt="Service Discovery status page showing one target" />
<h6><span class="label">Figure 8-2. </span>Service discovery status page
showing one discovered target, its metadata, and target labels from
Consul</h6>
</div>
</figure>

Consul does not expose metrics behind a */metrics* path, so the scrapes
from your Prometheus will fail. []{#ch08.xhtml#idm45207099916176
primary="Node Exporter" secondary="running with Consul"
data-type="indexterm"}But it does still provide enough to find all your
machines running a Consul agent, and thus should be running a Node
Exporter that you can scrape. We will look at how in
["Relabeling"](#ch08.xhtml#relabeling){data-type="xref"}.

::: {data-type="tip"}
###### Tip

If you want to monitor Consul itself, you will need to configure both
Prometheus and Consul accordingly.[]{#ch08.xhtml#idm45207099913136
primary="Consul" secondary="monitoring Consul" data-type="indexterm"}
See [the Consul documentation](https://oreil.ly/Sz3bP) for more
details.[]{#ch08.xhtml#idm45207099911344 primary="exporters"
secondary="Consul Exporter" data-type="indexterm"} A [Consul
Exporter](https://oreil.ly/4ZDhM) provides cluster-level and kv-based
metrics.
:::
:::
:::

::: {.section pdf-bookmark="EC2" data-type="sect2"}
::: {#ch08.xhtml#idm45207099956256 .sect2}
## EC2

Amazon Elastic Compute Cloud, more commonly known as EC2, is a popular
provider of virtual machines.[]{#ch08.xhtml#ix_serdimchEC2
primary="service discovery" secondary="mechanisms" tertiary="EC2"
data-type="indexterm"}[]{#ch08.xhtml#ix_EC2
primary="EC2 (Elastic Compute Cloud)" data-type="indexterm"} It is one
of several cloud providers that Prometheus allows you to use out of the
box for service discovery.[]{#ch08.xhtml#idm45207099905088
primary="Amazon EC2" see="EC2" data-type="indexterm"}

To use it you must provide Prometheus with credentials to use the EC2
API.[]{#ch08.xhtml#idm45207099903760 primary="IAM user (Amazon)"
data-type="indexterm"} One way you can do this is by setting up an IAM
user with the `AmazonEC2ReadOnlyAccess`
policy^[10](#ch08.xhtml#idm45207099902544){#ch08.xhtml#idm45207099902544-marker
data-type="noteref"}^ and providing the access key and secret key in the
configuration file, as shown in
[Example 8-10](#ch08.xhtml#ec2_prometheus_yml){data-type="xref"}.

::: {#ch08.xhtml#ec2_prometheus_yml data-type="example"}
##### [Example 8-10. ]{.label}*prometheus.yml* using EC2 service discovery

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: ec2
   ec2_sd_configs:
    - region: <region>
      access_key: <access key>
      secret_key: <secret key>
```
:::

If you aren't already running some, start at least one EC2 instance in
the EC2 region you have configured Prometheus to look at. If you go to
*http://localhost:9090/service-discovery* in your browser, you can see
the discovered targets and the metadata extracted from
EC2.[]{#ch08.xhtml#idm45207099814160 primary="targets"
secondary="discovered by EC2"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099813312
primary="metadata" secondary="target discovered by EC2 SD"
data-type="indexterm"} `__meta_ec2_tag_Name="My Display Name"`, for
example, is the `Name` tag on this instance, which is the name you will
see in the EC2 Console
([Figure 8-3](#ch08.xhtml#ec2_service_discovery){data-type="xref"}).

You may notice that the `instance` label is using the private IP. This
is a sensible default as it is presumed that Prometheus will be running
beside what it is monitoring. Not all EC2 instances have public IPs, and
there are network charges for talking to an EC2 instance's public IP.

You will find that service discovery for other cloud providers is
broadly similar, but the configuration required and metadata returned
vary.[]{#ch08.xhtml#idm45207099847424 primary="service discovery"
secondary="mechanisms" startref="ix_serdimchEC2" tertiary="EC2"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099845904
primary="EC2 (Elastic Compute Cloud)" startref="ix_EC2"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099844992
primary="service discovery" secondary="mechanisms"
startref="ix_serdimch" data-type="indexterm"}

<figure>
<div id="ch08.xhtml#ec2_service_discovery" class="figure">
<img src="assets/pur2_0803.png" width="600" height="535"
alt="Service Discovery status page showing one target" />
<h6><span class="label">Figure 8-3. </span>Service discovery status page
showing one discovered target, its metadata, and target labels from
EC2</h6>
</div>
</figure>
:::
:::
:::
:::

::: {.section pdf-bookmark="Relabeling" data-type="sect1"}
::: {#ch08.xhtml#relabeling .sect1}
# Relabeling

As seen in the preceding examples of service discovery mechanisms, the
targets and their metadata can be a little
raw.[]{#ch08.xhtml#ix_serdirelbl primary="service discovery"
secondary="relabeling" data-type="indexterm"}[]{#ch08.xhtml#ix_relbl
primary="relabeling" data-type="indexterm"} You could integrate with
file SD and provide Prometheus with exactly the targets and labels you
want, but in most cases you won't need to.
[]{#ch08.xhtml#idm45207099798656 primary="targets"
secondary="mapping from metadata using relabeling"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099797808
primary="metadata" secondary="mapping to targets using relabeling"
data-type="indexterm"}Instead, you can tell Prometheus how to map from
metadata to targets using *relabeling*.

::: {.less_space .pagebreak-before data-type="tip"}
###### Tip

Many characters, such as periods and asterisks, are not valid in
Prometheus label names, so will be sanitized to underscores in service
discovery metadata.
:::

In an ideal world you will have service discovery and relabeling
configured so that new machines and applications are picked up and
monitored automatically. In the real world it is not unlikely that as
your setup matures it will get sufficiently intricate that you have to
regularly update the Prometheus configuration file, but by then you will
likely also have an infrastructure so complex that it is only a minor
hurdle.

::: {.section pdf-bookmark="Choosing What to Scrape" data-type="sect2"}
::: {#ch08.xhtml#idm45207099794528 .sect2}
## Choosing What to Scrape

The first thing you will want to configure is which targets you actually
want to scrape.[]{#ch08.xhtml#ix_relblchscrp primary="relabeling"
secondary="choosing what to scrape"
data-type="indexterm"}[]{#ch08.xhtml#ix_scrptrgt primary="scraping"
secondary="choosing which targets to scrape in relabeling"
data-type="indexterm"}[]{#ch08.xhtml#ix_serdirelblch
primary="service discovery" secondary="relabeling"
tertiary="choosing what to scrape" data-type="indexterm"} If you are
part of one team running one service, you don't want your Prometheus to
be scraping every target in the same EC2 region.

Continuing on from
[Example 8-5](#ch08.xhtml#filesd_prometheus_yml){data-type="xref"}, what
if you just wanted to monitor the infrastructure team's machines?
[]{#ch08.xhtml#idm45207099787104 primary="keep (relabel action)"
data-type="indexterm"}You can do this with the `keep` *relabel action*,
as shown in
[Example 8-11](#ch08.xhtml#file_prometheus_yml_keep){data-type="xref"}.
The `regex` is applied to the values of the labels listed in
`source_labels` (joined by a semicolon), and if the regex matches, the
target is kept. []{#ch08.xhtml#idm45207099783712 primary="source_labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099783008
primary="regular expressions"
secondary="using to match targets in relabeling"
data-type="indexterm"}As there is only one action here, this results in
all targets with `team="infra"` being kept.

But for a target with a `team="monitoring"` label, the regex will not
match, and the target will be dropped.

::: {.note data-type="note"}
###### Note

Regular expressions in relabeling are *fully anchored*, meaning that the
pattern `infra` will not match `fooinfra` or
`infrabar`.[]{#ch08.xhtml#idm45207099777920
primary="fully anchored regular expressions" data-type="indexterm"}
:::

::: {#ch08.xhtml#file_prometheus_yml_keep data-type="example"}
##### [Example 8-11. ]{.label}Using a `keep` relabel action to only monitor targets with a `team="infra"` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [team]
      regex: infra
      action: keep
```
:::

You can have multiple relabel actions in a `relabel_configs`; all of
them will be processed in order unless either a `keep` or `drop` action
drops the target.[]{#ch08.xhtml#idm45207099748480
primary="relabel_configs" data-type="indexterm"} For example,
[Example 8-12](#ch08.xhtml#file_prometheus_yml_double_keep){data-type="xref"}
will drop all targets, as a label cannot have both `infra` and
`monitoring` as a value.

::: {#ch08.xhtml#file_prometheus_yml_double_keep data-type="example"}
##### [Example 8-12. ]{.label}Two relabel actions requiring contradictory values for the `team` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [team]
      regex: infra
      action: keep
    - source_labels: [team]
      regex: monitoring
      action: keep
```
:::

To allow multiple values []{#ch08.xhtml#idm45207099657184
primary="| (pipe symbol), alternation operator"
data-type="indexterm"}for a label you would use `|` (the pipe symbol)
for the alternation operator, which is a fancy way of saying one or the
other.
[Example 8-13](#ch08.xhtml#file_prometheus_yml_keep_alternation){data-type="xref"}
shows the right way to keep only targets for either the infrastructure
or monitoring teams.

::: {#ch08.xhtml#file_prometheus_yml_keep_alternation data-type="example"}
##### [Example 8-13. ]{.label}Using `|` to allow one label value or another

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [team]
      regex: infra|monitoring
      action: keep
```
:::

In addition to the `keep` action that drops targets that do not match,
you can also use the `drop` action to drop targets that do
match.[]{#ch08.xhtml#idm45207099537104 primary="drop (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099536496
primary="; (semicolon) separator in source_labels"
data-type="indexterm"} You can also provide multiple labels in
`source_labels`; their values will be joined with a
semicolon.^[11](#ch08.xhtml#idm45207099535344){#ch08.xhtml#idm45207099535344-marker
data-type="noteref"}^ If you don't want to scrape the Prometheus jobs of
the monitoring team, you can combine these, as in
[Example 8-14](#ch08.xhtml#file_prometheus_yml_multi_drop){data-type="xref"}.

::: {#ch08.xhtml#file_prometheus_yml_multi_drop data-type="example"}
##### [Example 8-14. ]{.label}Using multiple source labels

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [job, team]
      regex: prometheus;monitoring
      action: drop
```
:::

How you use relabeling is up to you. You should define some conventions.
For example, EC2 instances should have a `team` tag with the name of the
team that owns it, or all production services should have a `production`
tag in Consul. []{#ch08.xhtml#idm45207099450464
primary="production tags (Consul)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099449856
primary="EC2 (Elastic Compute Cloud)" secondary="team tags"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099448976 primary="Consul"
secondary="production tags for production services"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099448064
primary="team labels" secondary="EC2 instances"
data-type="indexterm"}Without conventions every new service will require
special handling for monitoring, which is probably not the best use of
your time.

If your service discovery mechanism includes health checking of some
form, do not use this to drop unhealthy instances. Even when an instance
is reporting as unhealthy, it could be producing useful metrics,
particularly around startup and shutdown.

::: {.note data-type="note"}
###### Note

Prometheus needs to have a target for each of your individual
application instances. []{#ch08.xhtml#idm45207099445600
primary="targets" secondary="for each individual application instance"
secondary-sortas="each" data-type="indexterm"}Scraping through load
balancers will not work, as you can hit a different instance on each
scrape, which could, for example, make counters appear to go backward.
:::

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch08.xhtml#regex .sidebar}
# Regular Expressions

Prometheus uses the *RE2* engine for regular expressions that comes with
Go.[]{#ch08.xhtml#idm45207099441632
primary="RE2 engine for regular expressions"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099440960 primary="Go"
secondary="RE2 regular expression engine"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099440048
primary="regular expressions" secondary="RE2 engine for"
data-type="indexterm"} RE2 is designed to be linear-time but does not
support back references, lookahead assertions, and some other advanced
features.

If you are not familiar with regular expressions, they let you provide a
rule (called a *pattern*) that is then tested against
text.[]{#ch08.xhtml#idm45207099395488
primary="patterns (regular expressions)" data-type="indexterm"} The
following table is a quick primer on regular
expressions.[]{#ch08.xhtml#idm45207099394752
primary="regular expressions" secondary="quick primer on"
data-type="indexterm"}

              Matches
  ----------- -------------------------------------------------------------------
  a           The character `a`
  .           Any single character
  \\.         A single period
  .\*         Any number of characters
  .+          At least one character
  a+          One or more `a` characters
  \[0-9\]     Any single digit, 0--9
  \\d         Any single digit, 0--9
  \\d\*       Any number of digits
  \[\^0-9\]   A single character that is not a digit
  ab          The character `a` followed by the character `b`
  a(b\|c\*)   An `a`, followed by a single `b`, or any number of `c` characters

In addition, parentheses create a capture group. So if you had the
pattern `(.)(\d+)` and the text `a123`, then the first capture group
would contain `a` and the second `123`. Capture groups are useful to
extract parts of a string for later use.[]{#ch08.xhtml#idm45207099367136
primary="relabeling" secondary="choosing what to scrape"
startref="ix_relblchscrp"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099365888
primary="scraping"
secondary="choosing which targets to scrape in relabeling"
startref="ix_scrptrgt"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099364576
primary="service discovery" secondary="relabeling"
startref="ix_serdirelblch" tertiary="choosing what to scrape"
data-type="indexterm"}
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Target Labels" data-type="sect2"}
::: {#ch08.xhtml#target_labels .sect2}
## Target Labels

Target labels are labels that are added to the labels of every time
series returned from a scrape.[]{#ch08.xhtml#ix_serdirelbltrgt
primary="service discovery" secondary="relabeling"
tertiary="using for target labels"
data-type="indexterm"}[]{#ch08.xhtml#ix_relbltrgt primary="relabeling"
secondary="using to specify target labels"
data-type="indexterm"}[]{#ch08.xhtml#ix_trgtlbl primary="target labels"
data-type="indexterm"} They are the identity of your
targets,^[12](#ch08.xhtml#idm45207099357488){#ch08.xhtml#idm45207099357488-marker
data-type="noteref"}^ and accordingly they should not generally vary
over time as might be the case with version numbers or machine owners.

Every time your target labels change the labels of the scraped time
series, their identities also change. This will cause discontinuities in
your graphs, and can cause issues with rules and alerts.

So what does make a good target label? You have already seen `job` and
`instance`, target labels all targets have. It is also common to add
target labels for the broader scope of the application, such as whether
it is in development or production, their region, datacenter, and which
team manages them. Labels for structure within your application can also
make sense, for example, if there is sharding.

Target labels ultimately allow you to select, group, and aggregate
targets in PromQL. For example, you might want alerts for development to
be handled differently than production, to know which shard of your
application is the most loaded, or which team is using the most CPU
time.

But target labels come with a cost. While it is quite cheap to add one
more label in terms of resources, the real cost comes when you are
writing PromQL. Every additional label is one more you need to keep in
mind for every single PromQL expression you
write.[]{#ch08.xhtml#idm45207099353104 primary="host labels"
data-type="indexterm"} For example, if you were to add a `host` label
that was unique per target, that would violate the expectation that only
`instance` is unique per target, which could break all of your
aggregation that used `without(instance)`. This is discussed further in
[Chapter 14](#ch14.xhtml#promql_aggregation_chapter){data-type="xref"}.

As a rule of thumb your target labels should be a hierarchy, with each
one adding additional distinctiveness. []{#ch08.xhtml#idm45207099349728
primary="target labels" secondary="hierarchy of"
data-type="indexterm"}For example, you might have a hierarchy where
regions contain datacenters that contain environments that contain
services that contain jobs that contain instances. This isn't a
hard-and-fast rule; you might plan ahead a little and have a datacenter
label even if you only have one datacenter
today.^[13](#ch08.xhtml#idm45207099348592){#ch08.xhtml#idm45207099348592-marker
data-type="noteref"}^

For labels the application knows about but don't make sense to have as
target labels, such as version numbers, you can expose them using info
metrics, as discussed in
["Info"](#ch05.xhtml#info_metrics){data-type="xref"}.

If you find that you want every target[]{#ch08.xhtml#idm45207099346432
primary="external_labels" data-type="indexterm"} in a Prometheus to
share some labels such as `region`, you should instead use
`external_labels` for them, as discussed in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}.

::: {.section pdf-bookmark="replace" data-type="sect3"}
::: {#ch08.xhtml#replace_action .sect3}
### replace

So how do you use relabeling to specify your target labels? The answer
is the `replace` action. []{#ch08.xhtml#idm45207099341104
primary="target labels" secondary="using relabeling to specify"
tertiary="replace action"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099339760
primary="replace (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207099339072
primary="relabeling" secondary="using to specify target labels"
tertiary="replace action" data-type="indexterm"}The `replace` action
allows you to copy labels around, while also applying regular
expressions.[]{#ch08.xhtml#idm45207099337296
primary="regular expressions" secondary="use in replace relabel action"
data-type="indexterm"}

Continuing on from
[Example 8-5](#ch08.xhtml#filesd_prometheus_yml){data-type="xref"},
let's say that the monitoring team was renamed to the monitor team and
you can't change the file SD input yet so you want to use relabeling
instead.
[Example 8-15](#ch08.xhtml#file_prometheus_yml_monitoring_monitor){data-type="xref"}
looks for a `team` label that matches the regular expression
`monitoring` (which is to say, the exact string `monitoring`), and if it
finds it, puts the replacement value `monitor` in the `team`
label.[]{#ch08.xhtml#idm45207099331824 primary="team labels"
secondary="using replace relabel action with" data-type="indexterm"}

::: {#ch08.xhtml#file_prometheus_yml_monitoring_monitor data-type="example"}
##### [Example 8-15. ]{.label}Using a `replace` relabel action to replace `team="monitoring"` with `team="monitor"`

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [team]
      regex: monitoring
      replacement: monitor
      target_label: team
      action: replace
```
:::

That's fairly simple, but in practice having to specify replacement
label values one by one would be a lot of work for you. Let's say it
turns out that the problem was the `ing` in `monitoring`, and you wanted
relabeling to strip any trailing "ings" in team names.
[Example 8-16](#ch08.xhtml#file_prometheus_yml_no_ing){data-type="xref"}
does this by applying the regular expression `(.*)ing`, which matches
all strings that end with `ing` and puts the start of the label value in
the first capture group. The replacement value consists of that first
capture group, which will be placed in the `team` label.

::: {#ch08.xhtml#file_prometheus_yml_no_ing data-type="example"}
##### [Example 8-16. ]{.label}Using a `replace` relabel action to remove a trailing "ing" from the `team` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: [team]
      regex: '(.*)ing'
      replacement: '${1}'
      target_label: team
      action: replace
```
:::

If one of your targets does not have a label value that matches, such as
`team="infra"`, then the `replace` action has no effect on that target,
as you can see in
[Figure 8-4](#ch08.xhtml#file_service_discovery_no_ing){data-type="xref"}.

<figure>
<div id="ch08.xhtml#file_service_discovery_no_ing" class="figure">
<img src="assets/pur2_0804.png" width="600" height="690"
alt="Service Discovery status page showing three targets" />
<h6><span class="label">Figure 8-4. </span>The “ing” is removed from
monitoring, while the “infra” targets are <span
class="keep-together">unaffected</span></h6>
</div>
</figure>

A label []{#ch08.xhtml#idm45207099181824 primary="team labels"
secondary="removing using replace relabel action"
data-type="indexterm"}with an empty value is the same as not having that
label, so if you wanted to you could remove the `team` label using
[Example 8-17](#ch08.xhtml#file_prometheus_yml_no_team){data-type="xref"}.

::: {#ch08.xhtml#file_prometheus_yml_no_team .pagebreak-before data-type="example"}
##### [Example 8-17. ]{.label}Using a `replace` relabel action to remove the `team` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
      - '*.json'
   relabel_configs:
    - source_labels: []
      regex: '(.*)'
      replacement: '${1}'
      target_label: team
      action: replace
```
:::

::: {.note data-type="note"}
###### Note

All labels beginning with `__` are discarded at the end of relabeling
for target labels, so you don't need to do this yourself.
:::

Since performing a regular expression against the whole string,
capturing it, and using it as the replacement is common, these are all
defaults.[]{#ch08.xhtml#idm45207099052832 primary="team labels"
secondary="using defaults to remove succinctly" data-type="indexterm"}
Thus you can omit
them,^[14](#ch08.xhtml#idm45207099051824){#ch08.xhtml#idm45207099051824-marker
data-type="noteref"}^ and
[Example 8-18](#ch08.xhtml#file_prometheus_yml_no_team_defaults){data-type="xref"}
will have the same effect as
[Example 8-17](#ch08.xhtml#file_prometheus_yml_no_team){data-type="xref"}.

::: {#ch08.xhtml#file_prometheus_yml_no_team_defaults data-type="example"}
##### [Example 8-18. ]{.label}Using the defaults to remove the `team` label succinctly

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: file
   file_sd_configs:
    - files:
       - '*.json'
   relabel_configs:
    - source_labels: []
      target_label: team
```
:::

Now that you have more of a sense of how the `replace` action works,
let's look at a more realistic example.
[Example 8-9](#ch08.xhtml#consul_prometheus_yml){data-type="xref"}
produced a target with port 80, but it'd be useful if you could change
that to port 9100 where the Node Exporter is
running.[]{#ch08.xhtml#idm45207098983632 primary="__address__ labels"
primary-sortas="address"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098982752 primary="Consul"
secondary="service discovery"
tertiary="using replace to relabel team label" data-type="indexterm"} In
[Example 8-19](#ch08.xhtml#consul_prometheus_yml_9100){data-type="xref"}
we take the address from Consul and append `:9100` to it, placing it in
the `__address__` label.

::: {#ch08.xhtml#consul_prometheus_yml_9100 data-type="example"}
##### [Example 8-19. ]{.label}Using the IP from Consul with port 9100 for the Node Exporter

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: node
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel_configs:
    - source_labels: [__meta_consul_address]
      regex: '(.*)'
      replacement: '${1}:9100'
      target_label: __address__
```
:::

::: {data-type="tip"}
###### Tip

If relabeling produces two identical targets from one of your scrape
configs, they will be deduplicated
automatically.[]{#ch08.xhtml#idm45207098939344 primary="relabeling"
secondary="automatic deduplication with" data-type="indexterm"} So if
you have many Consul services running on each machine, only one target
per machine would result from
[Example 8-19](#ch08.xhtml#consul_prometheus_yml_9100){data-type="xref"}.
:::
:::
:::

::: {.section pdf-bookmark="job, instance, and __address__" data-type="sect3"}
::: {#ch08.xhtml#job_instance_address .sect3}
### job, instance, and \_\_address\_\_

In the preceding examples you may have noticed that there was an
`instance` target label, but no matching `instance` label in the
metadata.[]{#ch08.xhtml#idm45207098898016 primary="instance labels"
secondary="relabeling"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098897008
primary="job labels" secondary="relabeling"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098896064
primary="__address__ labels" primary-sortas="address"
secondary="relabeling" data-type="indexterm"} So where did it come from?
The answer is that if your target has no `instance` label, it is
defaulted to the value of the `__address__`
label.[]{#ch08.xhtml#idm45207098893920 primary="relabeling"
secondary="using to specify target labels"
tertiary="job, instance, and __address__"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098892704
primary="target labels" secondary="using relabeling to specify"
tertiary="job, instance, and __address__" data-type="indexterm"}

`instance` along with `job` are two labels your targets will always
have, `job` being defaulted from the `job_name` configuration option.
The `job` label indicates a set of instances that serve the same
purpose, and will generally all be running with the same binary and
configuration.^[15](#ch08.xhtml#idm45207098847552){#ch08.xhtml#idm45207098847552-marker
data-type="noteref"}^ The `instance` label identifies one instance
within a job.

The `__address__` is the host and port your Prometheus will connect to
when scraping. While it provides a default for the `instance` label, it
is separate so you can have a different value for it. For example, you
may wish to use the Consul node name in the `instance` label, while
leaving the address pointing to the IP address, as in
[Example 8-20](#ch08.xhtml#consul_prometheus_yml_9100_instance){data-type="xref"}.
This is a better approach than adding an additional `host`, `node`, or
`alias` label with a nicer name, as it avoids adding a second label
unique to each target, which would cause complications in your PromQL.

::: {#ch08.xhtml#consul_prometheus_yml_9100_instance .pagebreak-before .less_space_example data-type="example"}
##### [Example 8-20. ]{.label}Using the IP from Consul with port 9100 as the address, with the node name in the `instance` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: consul
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel_configs:
    - source_labels: [__meta_consul_address]
      regex: '(.*)'
      replacement: '${1}:9100'
      target_label: __address__
    - source_labels: [__meta_consul_node]
      regex: '(.*)'
      replacement: '${1}:9100'
      target_label: instance
```
:::

::: {data-type="tip"}
###### Tip

Prometheus will perform DNS resolution on the `__address__`, so one way
you can have more readable `instance` labels is by providing `host:port`
rather than `ip:port`.
:::
:::
:::

::: {.section pdf-bookmark="labelmap" data-type="sect3"}
::: {#ch08.xhtml#labelmap_action .sect3}
### labelmap

The `labelmap` action is different from the `drop`, `keep`, and
`replace` actions you have already seen in that it applies to label
names rather than label values.[]{#ch08.xhtml#idm45207098788416
primary="labelmap (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098787744
primary="relabeling" secondary="using to specify target labels"
tertiary="labelmap action"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098786560
primary="target labels" secondary="using relabeling to specify"
tertiary="labelmap action" data-type="indexterm"}

Where you might find this useful is if the service discovery you are
using already has a form of key-value labels, and you would like to use
some of those as target labels. This might be to allow configuration of
arbitrary target labels, without having to change your Prometheus
configuration every time there is a new label.

EC2's tags, for example, are key-value pairs.
[]{#ch08.xhtml#idm45207098784032 primary="EC2 (Elastic Compute Cloud)"
secondary="relabeling tags using labelmap action"
data-type="indexterm"}You might have an existing convention to have the
name of the service go in the `service` tag, and its semantics align
with what the `job` label means in Prometheus. You might also declare a
convention that any tags prefixed with `monitor_` will become target
labels. For example, an EC2 tag of `monitor_foo=bar` would become a
Prometheus target label of `foo="bar"`.
[Example 8-21](#ch08.xhtml#ec2_prometheus_yml_labelmap){data-type="xref"}
shows this setup, using a `replace` action for the `job` label and a
`labelmap` action for the `monitor_` prefix.

::: {#ch08.xhtml#ec2_prometheus_yml_labelmap .pagebreak-before .less_space_example data-type="example"}
##### [Example 8-21. ]{.label}Use the EC2 service tag as the `job` label, with all tags prefixed with `monitor_` as additional target labels

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: ec2
   ec2_sd_configs:
    - region: <region>
      access_key: <access key>
      secret_key: <secret key>
   relabel_configs:
    - source_labels: [__meta_ec2_tag_service]
      target_label: job
    - regex: __meta_ec2_public_tag_monitor_(.*)
      replacement: '${1}'
      action: labelmap
```
:::

But you should be wary of blindly copying all labels in a scenario like
this, as it is unlikely that Prometheus is the only consumer of metadata
such as this within your overall architecture. For example, a new cost
center tag might be added to all of your EC2 instances for internal
billing reasons. []{#ch08.xhtml#idm45207098700928 primary="billing"
data-type="indexterm"}If that tag automatically became a target label
due to a `labelmap` action, that would change all of your target labels
and likely break graphing and alerting. Thus, using either well-known
names (such as the [`service`]{.keep-together} tag here) or clearly
namespaced names (such as `monitor_`) is wise.
:::
:::

::: {.section pdf-bookmark="Case" data-type="sect3"}
::: {#ch08.xhtml#idm45207098656528 .sect3}
### Case

Sometimes, it is useful to change the case of label
values.[]{#ch08.xhtml#idm45207098654944
primary="case, changing for label values"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098654272
primary="relabeling" secondary="using to specify target labels"
tertiary="changing case"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098653088
primary="target labels" secondary="using relabeling to specify"
tertiary="case" data-type="indexterm"} This enables you to make labels
consistent across multiple service discoveries.
[]{#ch08.xhtml#idm45207098651776 primary="lowercase (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098651136
primary="uppercase (relabel action)" data-type="indexterm"}You can
change the case with the `lowercase` and `uppercase` relabel actions, as
shown in [Example 8-22](#ch08.xhtml#ionos_lowercase){data-type="xref"}.

::: {#ch08.xhtml#ionos_lowercase data-type="example"}
##### [Example 8-22. ]{.label}*prometheus.yml* with `lowercase` relabel config

``` {code-language="yaml" data-type="programlisting"}
- job_name: ionos
  ionos_sd_configs:
    - basic_auth:
        username: john.doe@example.com
        password: <secret>
      datacenter_id: 57375146-e890-4b84-8d59-c045d3eb6f4c
  relabel_configs:
    - source_labels: [__meta_ionos_server_type]
      target_label: server_type
      action: lowercase
```
:::
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="Lists" data-type="sect3"}
::: {#ch08.xhtml#idm45207098513856 .sect3}
### Lists

Not all service discovery mechanisms have key-value labels or tags; some
just have a list of tags, with the canonical example being Consul's
tags.[]{#ch08.xhtml#idm45207098512368 primary="target labels"
secondary="using relabeling to specify" tertiary="lists"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098511184
primary="relabeling" secondary="using to specify target labels"
tertiary="lists" data-type="indexterm"}[]{#ch08.xhtml#idm45207098510000
primary="lists" secondary="produced by service discovery, relabeling"
data-type="indexterm"} While Consul is the most likely place that you
will run into this, there are various other places where a service
discovery mechanism must somehow convert a list into key-value metadata
such []{#ch08.xhtml#idm45207098508960
primary="EC2 (Elastic Compute Cloud)" data-type="indexterm"}as the EC2
subnet
ID.^[16](#ch08.xhtml#idm45207098508192){#ch08.xhtml#idm45207098508192-marker
data-type="noteref"}^

This is done by joining the items in the list with a comma and using the
now-joined items as a label value.[]{#ch08.xhtml#idm45207098507008
primary="Consul" secondary="keeping only Consul services with prod tag"
data-type="indexterm"} A comma is also put at the start and the end of
the value to make writing correct regular expressions easier.

As an example, say a Consul service had `dublin` and `prod` tags. The
`__meta_​`[`consul_tags`]{.keep-together} label could have the value
`,dublin,prod,` or `,prod,dublin,` as tags are unordered. If you wanted
to only scrape production targets, you would use a `keep` action, as
shown in
[Example 8-23](#ch08.xhtml#consul_prometheus_yml_9100_tags_keep){data-type="xref"}.

::: {#ch08.xhtml#consul_prometheus_yml_9100_tags_keep data-type="example"}
##### [Example 8-23. ]{.label}Keeping only Consul services with the `prod` tag

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: node
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel_configs:
    - source_labels: [__meta_consul_tags]
      regex:  '.*,prod,.*'
      action: keep
```
:::

Sometimes you will have tags that are only the value of a key-value
pair. You can convert such values to labels, but you need to know the
potential values.
[Example 8-24](#ch08.xhtml#consul_prometheus_yml_9100_tags_replace){data-type="xref"}
shows how a tag indicating the environment of a target can be converted
into an `env` label.

::: {#ch08.xhtml#consul_prometheus_yml_9100_tags_replace data-type="example"}
##### [Example 8-24. ]{.label}Using `prod`, `staging`, and `dev` tags to fill an `env` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: node
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel_configs:
    - source_labels: [__meta_consul_tags]
      regex:  '.*,(prod|staging|dev),.*'
      target_label: env
```
:::

::: {data-type="tip"}
###### Tip

With sophisticated relabeling rules you may find yourself needing a
temporary label to put a value in. The `__tmp` prefix is reserved for
this purpose.[]{#ch08.xhtml#idm45207098354864
primary="__tmp prefix for labels" primary-sortas="tmp"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098353952
primary="service discovery" secondary="relabeling"
startref="ix_serdirelbltrgt" tertiary="using for target labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098352464
primary="relabeling" secondary="using to specify target labels"
startref="ix_relbltrgt"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098351280
primary="target labels" startref="ix_trgtlbl"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098350336
primary="service discovery" secondary="relabeling"
startref="ix_serdirelbl"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098349120
primary="relabeling" startref="ix_relbl" data-type="indexterm"}
:::
:::
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="How to Scrape" data-type="sect1"}
::: {#ch08.xhtml#how_to_scrape .sect1}
# How to Scrape

You now have targets with their target labels and the `__address__` to
connect to. []{#ch08.xhtml#ix_serdiscrp primary="service discovery"
secondary="how to scrape"
data-type="indexterm"}[]{#ch08.xhtml#ix_scrphow primary="scraping"
secondary="how to scrape" data-type="indexterm"}There are some
additional things you may wish to configure, such as a path other than
*/metrics* or client authentication.[]{#ch08.xhtml#idm45207098306368
primary="scrape_configs" secondary="showing several available options"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098305520 primary="paths"
secondary="options in scrape config" data-type="indexterm"}

[Example 8-25](#ch08.xhtml#prometheus_yml_scrape_config_options){data-type="xref"}
shows some of the more common options you can use. As these change over
time, check [the documentation](https://oreil.ly/xHd-o) for the most
up-to-date settings.

::: {#ch08.xhtml#prometheus_yml_scrape_config_options .less_space_example data-type="example"}
##### [Example 8-25. ]{.label}A scrape config showing several of the available options

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: example
   consul_sd_configs:
    - server: 'localhost:8500'
   scrape_timeout: 5s
   metrics_path: /admin/metrics
   params:
     foo: [bar]
   scheme: https
   tls_config:
     insecure_skip_verify: true
   basic_auth:
     username: brian
     password: hunter2
```
:::

`metrics_path` is only the path of the URL, and if you tried to put
*/metrics?foo=bar*, for example, it would get escaped to
*/metrics%3Ffoo=bar*. Instead, any URL parameters should be placed in
`params`, though you usually only need this for federation and the
classes of exporters that include the SNMP and Blackbox
Exporters.[]{#ch08.xhtml#idm45207098255072
primary="URL parameters (scrape config)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098254432
primary="metrics_path" data-type="indexterm"} It is not possible to add
arbitrary headers, as that would make debugging more difficult. If you
need flexibility beyond what is offered, you can always use a proxy
server with `proxy_url` to tweak your scrape
requests.[]{#ch08.xhtml#idm45207098253248 primary="proxy_url"
data-type="indexterm"}

`scheme` can be `http` or `https`; and with `https` you can provide
additional options, including the `key_file` and `cert_file` if you wish
to use TLS client authentication. `insecure_skip_verify` allows you to
disable validation of a scrape target's TLS cert, which is not advisable
security-wise.[]{#ch08.xhtml#idm45207098249024
primary="TLS client authentication"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098248352
primary="scheme (scrape config)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098247680
primary="authentication" secondary="options in scrape config"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098246768
primary="HTTP Basic Authentication"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098246128
primary="HTTP Bearer Token Authentication" data-type="indexterm"}

Aside from TLS client authentication, HTTP Basic Authentication and HTTP
Bearer Token Authentication are offered via `basic_auth`, `OAuth2`, and
`authorization`.[]{#ch08.xhtml#idm45207098243568 primary="basic_auth"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098176000 primary="OAuth2"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098175392
primary="authorization" data-type="indexterm"} The token can also be
read from a file, rather than from the configuration, using
`authorization`'s `credentials_file`. []{#ch08.xhtml#idm45207098173888
primary="credentials_file" data-type="indexterm"}As the tokens and basic
authentication passwords are expected to contain secrets, they will be
masked on the status pages of Prometheus so that you don't accidentally
leak them.

In addition to overriding the `scrape_timeout` in a scrape config, you
can also override the `scrape_interval`, but in general you should aim
for a single scrape interval in a Prometheus for
sanity.[]{#ch08.xhtml#idm45207098172016 primary="scrape_interval"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098171408
primary="scrape_timeout" data-type="indexterm"}

Of these scrape []{#ch08.xhtml#idm45207098170352 primary="relabeling"
secondary="using to override scrape config settings"
data-type="indexterm"}config settings, the scheme, path, and URL
parameters are available to you and can be overridden by you via
relabeling, with the label names `__scheme__`, `__metrics_path__`, and
`__param_<name>`. If there are multiple URL parameters with the same
name, only the first is available. It is not possible to relabel other
settings for reasons varying from sanity to security.

Service discovery metadata is not considered security
sensitive^[17](#ch08.xhtml#idm45207098167328){#ch08.xhtml#idm45207098167328-marker
data-type="noteref"}^ and will be accessible to anyone with access to
the Prometheus UI. As secrets can only be specified per scrape config,
it is recommended that any credentials you use are made standard across
your services.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch08.xhtml#job_name_default .sidebar}
# Duplicate Jobs

While `job_name` must be unique, as it is only a default, you are not
prevented from having different scrape configs producing targets with
the same `job` label.[]{#ch08.xhtml#idm45207098163904 primary="jobs"
secondary="duplicate"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098162896
primary="job labels" secondary="relabeling" data-type="indexterm"}

For example, if you had some jobs that required a different secret which
were indicated by a Consul tag, you could segregate them using `keep`
and `drop` actions, and then use a `replace` to set the `job` label:

``` {code-language="yaml" data-type="programlisting"}
 - job_name: my_job
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel
    - source_labels: [__meta_consul_tag]
      regex:  '.*,specialsecret,.*'
      action: drop
   basic_auth:
     username: brian
     password: normalSecret

 - job_name: my_job_special_secret
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel
    - source_labels: [__meta_consul_tag]
      regex:  '.*,specialsecret,.*'
      action: keep
    - replacement: my_job
      target_label: job
   basic_auth:
     username: brian
     password: specialSecret
```
:::

```{=html}
</aside>
```
::: {.section pdf-bookmark="metric_relabel_configs" data-type="sect2"}
::: {#ch08.xhtml#metric_relabel_configs .sect2}
## metric_relabel_configs

In addition to relabeling being used for its original purpose of mapping
service discovery metadata to target labels, relabeling has also been
applied to other areas of Prometheus.[]{#ch08.xhtml#idm45207098024704
primary="service discovery" secondary="how to scrape"
tertiary="metric_relabel_configs"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098023456
primary="scraping" secondary="how to scrape"
tertiary="metric_relabel_configs"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098022240
primary="relabeling" secondary="metrics, using metric_relabel_configs"
data-type="indexterm"} One of those is *metric relabeling*: relabeling
applied to the time series scraped from a
target.[]{#ch08.xhtml#idm45207098020784 primary="metric_relabel_configs"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098020080 primary="metrics"
secondary="relabeling" data-type="indexterm"}

The `keep`, `drop`, `replace`, `lowercase`, `uppercase`, and `labelmap`
actions you have already seen can all be used in
`metric_relabel_configs` as there are no restrictions on which relabel
actions can be used
where.^[18](#ch08.xhtml#idm45207098015424){#ch08.xhtml#idm45207098015424-marker
data-type="noteref"}^

::: {data-type="tip"}
###### Tip

To help you remember which is which, `relabel_configs` occurs when
figuring out what to scrape, and `metrics_relabel_configs` happens after
the scrape has occurred.[]{#ch08.xhtml#idm45207098013184
primary="relabel_configs" secondary="versus metric_relabel_configs"
secondary-sortas="metric"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098011968
primary="metric_relabel_configs" secondary="versus relabel_configs"
secondary-sortas="relabel" data-type="indexterm"}
:::

There are two cases where you might use metric relabeling: when dropping
expensive metrics and when fixing bad metrics. While it is better to fix
such problems at the source, it is always good to know that you have
tactical options while the fix is in progress.

Metric relabeling gives you access to the time series after it is
scraped but before it is written to
storage.[]{#ch08.xhtml#idm45207098009472 primary="keep (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207098008768
primary="drop (relabel action)" data-type="indexterm"} The `keep` and
`drop` actions can be applied to the `__name__` label (discussed in
["Reserved Labels and
\_\_name\_\_"](#ch05.xhtml#name_label){data-type="xref"}) to select
which time series you actually want to ingest. If, for example, you
discovered that the
`http_request_size_bytes`^[19](#ch08.xhtml#idm45207098005456){#ch08.xhtml#idm45207098005456-marker
data-type="noteref"}^ metric of Prometheus had excessive cardinality and
was causing performance issues, you could drop it, as shown in
[Example 8-26](#ch08.xhtml#prometheus_drop_expensive_metric){data-type="xref"}.
It is still being transferred over the network and parsed, but this
approach can still offer you some breathing room.

::: {#ch08.xhtml#prometheus_drop_expensive_metric data-type="example"}
##### [Example 8-26. ]{.label}Dropping an expensive metric using `metric_relabel_configs`

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
       - localhost:9090
   metric_relabel_configs:
    - source_labels: [__name__]
      regex: http_request_size_bytes
      action: drop
```
:::

The `le` labels are also available.[]{#ch08.xhtml#idm45207097873440
primary="le labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097916720
primary="histograms" secondary="dropping buckets to reduce cardinality"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097915872
primary="buckets (in histograms)"
secondary="dropping to reduce cardinality" data-type="indexterm"} As
mentioned in ["Cumulative
Histograms"](#ch03.xhtml#cumulative_histograms){data-type="xref"}, you
can also drop certain buckets (but not `+Inf`) of histograms and you
will still be able to calculate quantiles.
[Example 8-27](#ch08.xhtml#prometheus_drop_buckets){data-type="xref"}
shows this with the
`prometheus_tsdb_​`[`compaction_duration_seconds`]{.keep-together}
histogram in Prometheus.

::: {#ch08.xhtml#prometheus_drop_buckets data-type="example"}
##### [Example 8-27. ]{.label}Dropping histogram buckets to reduce cardinality

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
       - localhost:9090
   metric_relabel_configs:
    - source_labels: [__name__, le]
      regex: 'prometheus_tsdb_compaction_duration_seconds_bucket;(4|32|256)'
      action: drop
```
:::

::: {.note data-type="note"}
###### Note

`metric_relabel_configs` only applies to metrics that you scrape from
the target. It does not apply to metrics like `up`, which are about the
scrape itself, and which will have only the target labels.
:::

You could also use `metric_relabel_configs` to rename metrics, rename
labels, or even extract labels from metric names.

::: {.section pdf-bookmark="labeldrop and labelkeep" data-type="sect3"}
::: {#ch08.xhtml#labeldrop .sect3}
### labeldrop and labelkeep

There are two further relabel actions that are unlikely to be ever
required for target relabeling, but that can come up in metric
relabeling.[]{#ch08.xhtml#idm45207097823024 primary="relabeling"
secondary="labeldrop and labelkeep actions"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097822080
primary="labeldrop (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097821440
primary="labelkeep (relabel action)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097820800
primary="scraping" secondary="how to scrape"
tertiary="labeldrop and labelkeep" data-type="indexterm"} Sometimes
exporters can be overly enthusiastic in the labels they apply, or
confuse instrumentation labels with target labels and return what they
think should be the target labels in a scrape. The `replace` action can
only deal with label names you know the name of in advance, which
sometimes isn't the case.

This is where `labeldrop` and `labelkeep` come in. Similar to
`labelmap`, they apply to label names rather than to label values.
Instead of copying labels, `labeldrop` and `labelkeep` remove labels.
[Example 8-28](#ch08.xhtml#prometheus_drop_node_labels){data-type="xref"}
uses `labeldrop` to drop all labels with a given prefix.

::: {#ch08.xhtml#prometheus_drop_node_labels data-type="example"}
##### [Example 8-28. ]{.label}Dropping all scraped labels that begin with `node_`

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: misbehaving
   static_configs:
    - targets:
       - localhost:1234
   metric_relabel_configs:
    - regex: 'node_.*'
      action: labeldrop
```
:::

When you have to use these actions, prefer using `labeldrop` where
practical. With `labelkeep` you need to list every single label you want
to keep, including `__name__`, `le`, and `quantile`.
:::
:::
:::
:::

::: {.section pdf-bookmark="Label Clashes and honor_labels" data-type="sect2"}
::: {#ch08.xhtml#honor_labels .sect2}
## Label Clashes and honor_labels

While `labeldrop` can be used when an exporter incorrectly presumes it
knows what labels you want, there is a small set of exporters where the
exporter does know the labels you want.[]{#ch08.xhtml#idm45207097744272
primary="labels" secondary="clashes in and honor_labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097743328
primary="service discovery" secondary="how to scrape"
tertiary="label clashes and honor_labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097742144
primary="scraping" secondary="how to scrape"
tertiary="label clashes and honor_labels" data-type="indexterm"} For
example, metrics in the Pushgateway should not have an `instance` label,
as was mentioned in
["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}, so you need
some way of not having the Pushgateway's instance target label apply.

But first let's look at what happens when there is a target label with
the same name as an instrumentation label from a
scrape.[]{#ch08.xhtml#idm45207097739232 primary="instrumentation labels"
secondary="clashes with target labels, honor_labels and"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097738288
primary="target labels" secondary="clashes in and honor_labels"
data-type="indexterm"} To avoid misbehaving applications interfering
with your target label setup, it is the target label that wins. If you
had a clash on the `job` label, for example, the instrumentation label
would be renamed to `exported_job`.

If instead you want the instrumentation label to win and override the
target label, you can set `honor_labels: true` in your scrape config.
[]{#ch08.xhtml#idm45207097686768 primary="honor_labels (scrape config)"
data-type="indexterm"}This is the one place in Prometheus where an empty
label is not the same thing as a missing label. If a scraped metric
explicitly has an `instance=""` label, and `honor_labels: true` is
configured, the resultant time series will have no instance label. This
technique is used by the Pushgateway.

Aside from the Pushgateway, `honor_labels` can also come up when
ingesting metrics from other monitoring systems if you do not follow the
recommendation in
[Chapter 11](#ch11.xhtml#other_monitoring_systems_chapter){data-type="xref"}
to run one exporter per application instance.

::: {data-type="tip"}
###### Tip

If you want more fine-grained control for handling clashing target and
instrumentation labels, you can use `metric_relabel_configs` to adjust
the labels before the metrics are added to the storage. Handling of
label clashes and `honor_labels` is performed before
`metric_relabel_configs`.
:::

Now that you understand service discovery, you're ready to look at
monitoring containers and how service discovery can be used with
Kubernetes.[]{#ch08.xhtml#idm45207097680096 primary="scraping"
secondary="how to scrape" startref="ix_scrphowclsh"
tertiary="label clashed and honor_labels"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097678608
primary="service discovery" secondary="how to scrape"
startref="ix_serdiscrp"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097677392
primary="scraping" secondary="how to scrape" startref="ix_scrphow"
data-type="indexterm"}[]{#ch08.xhtml#idm45207097676176
primary="service discovery" startref="ix_serdi" data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch08.xhtml#idm45207100526768-marker)^ Brian's home Prometheus uses
a hardcoded static configuration, for example, as I only have a handful
of machines.

^[2](#ch08.xhtml#idm45207100515904-marker)^ Like [Prometheus vCloud
Director SD](https://oreil.ly/sxi0j)

^[3](#ch08.xhtml#idm45207100509584-marker)^ The Power Distribution Unit
(PDU), part of the electrical system in a
datacenter.[]{#ch08.xhtml#idm45207100508928 primary="CPUs"
secondary="PDUs and"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100507856
primary="PDUs (Power Distribution Units)"
data-type="indexterm"}[]{#ch08.xhtml#idm45207100507216
primary="Power Distribution Units (PDUs)" data-type="indexterm"} PDUs
usually feed a group of racks with electricity, and knowing the CPU load
on each machine could be useful to ensure each PDU can provide the power
required.

^[4](#ch08.xhtml#idm45207100502064-marker)^ Amazon Elastic Compute Cloud

^[5](#ch08.xhtml#idm45207100501312-marker)^ Google Compute Engine

^[6](#ch08.xhtml#idm45207100191808-marker)^ You cannot, however, put
globs in the directory, so `a/b/*.json` is fine, `a/*/file.json` is not.

^[7](#ch08.xhtml#idm45207100138752-marker)^ This endpoint was added in
Prometheus 2.1.0. On older versions you can hover over the Labels on the
Targets page to see the metadata.

^[8](#ch08.xhtml#idm45207100134448-marker)^ `job_name` is only a
default, which we will look at further in ["Duplicate
Jobs"](#ch08.xhtml#job_name_default){data-type="xref"}. The other `__`
labels are special and will be covered in ["How to
Scrape"](#ch08.xhtml#how_to_scrape){data-type="xref"}.

^[9](#ch08.xhtml#idm45207100091904-marker)^ By default, one request is
made every 60 seconds. This can be changed with the `refresh_interval`
parameter.

^[10](#ch08.xhtml#idm45207099902544-marker)^ Only the
`EC2:DescribeInstances` permission is needed, but policies are generally
easier for you to set up initially.

^[11](#ch08.xhtml#idm45207099535344-marker)^ You can override the
character used to join with the `separator` field.

^[12](#ch08.xhtml#idm45207099357488-marker)^ It is possible for two of
your targets to have the same target labels, with other settings
different, but this should be avoided because metrics such as `up` will
clash.

^[13](#ch08.xhtml#idm45207099348592-marker)^ On the other hand, don't
try to plan too far in advance. It's not unusual that, as your
architecture changes over the years, your target label hierarchy will
need to change with it. Predicting exactly how it will change is usually
impossible. Consider, for example, if you were moving from a traditional
datacenter setup to a provider like EC2, which has availability zones.

^[14](#ch08.xhtml#idm45207099051824-marker)^ You could also omit
`source_labels: []`. We left it in here to make it clearer that the
label was being removed.

^[15](#ch08.xhtml#idm45207098847552-marker)^ A job could potentially be
further divided into shards with another label.

^[16](#ch08.xhtml#idm45207098508192-marker)^ An EC2 instance can have
multiple network interfaces, each of which could be in different
subnets.

^[17](#ch08.xhtml#idm45207098167328-marker)^ Nor are the service
discovery systems typically designed to hold secrets.

^[18](#ch08.xhtml#idm45207098015424-marker)^ Which is not to say that
all relabel actions make sense in all relabel contexts.

^[19](#ch08.xhtml#idm45207098005456-marker)^ In Prometheus 2.3.0 this
metric was changed to a histogram and renamed to
[`prometheus_http_response_size_bytes`]{.keep-together}.
:::
:::
:::

[]{#ch09.xhtml}

::: {#ch09.xhtml#sbo-rt-content}
::: {#ch09.xhtml#containers_k8_chapter .chapter}
# [Chapter 9. ]{.label}Containers and Kubernetes

Container deployments are becoming more common with technologies such as
Docker and Kubernetes---you may even already be using
them.[]{#ch09.xhtml#ix_cntnr primary="containers" data-type="indexterm"}
In this chapter we will cover exporters that you can use with
containers, and explain how to use Prometheus with Kubernetes.

All Prometheus components run happily in containers, with the sole
exception of the Node Exporter, as noted in
[Chapter 7](#ch07.xhtml#node_exporter_chapter){data-type="xref"}.

::: {.section pdf-bookmark="cAdvisor" data-type="sect1"}
::: {#ch09.xhtml#idm45207097671232 .sect1}
# cAdvisor

In the same way the Node Exporter provides metrics about the machine,
[cAdvisor](https://oreil.ly/tvOmH) is an exporter that provides metrics
about *cgroups*. []{#ch09.xhtml#ix_cAd primary="cAdvisor"
data-type="indexterm"}[]{#ch09.xhtml#ix_cntnrcAdv primary="containers"
secondary="cAdvisor" data-type="indexterm"}[]{#ch09.xhtml#ix_expcAd
primary="exporters" secondary="cAdvisor" data-type="indexterm"}Cgroups
are a Linux kernel isolation feature that are usually used to implement
containers on Linux, and are also used by runtime environments such as
systemd.[]{#ch09.xhtml#idm45207097664608 primary="cgroups"
secondary="metrics about" see="cAdvisor"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097663392 primary="Docker"
secondary="running cAdvisor with" data-type="indexterm"}

You can run cAdvisor with Docker:

``` {data-type="programlisting"}
docker run \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:rw \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --volume=/dev/disk/:/dev/disk:ro \
  --publish=8080:8080 \
  --detach=true \
  --name=cadvisor \
  gcr.io/cadvisor/cadvisor:v0.45.0
```

If you visit *http://localhost:8080/metrics*, you will see a long list
of metrics, as
[Figure 9-1](#ch09.xhtml#cadvisor_metrics){data-type="xref"} shows.

<figure>
<div id="ch09.xhtml#cadvisor_metrics" class="figure">
<img src="assets/pur2_0901.png" width="600" height="233"
alt="cAdvisor /metrics" />
<h6><span class="label">Figure 9-1. </span>The start of a
<em>/metrics</em> page from cAdvisor</h6>
</div>
</figure>

The container metrics are prefixed with `container_`, and you will
notice that they all have an `id` label.
[]{#ch09.xhtml#idm45207097655472 primary="metrics" secondary="container"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097654464
primary="id labels" data-type="indexterm"}The `id` labels starting with
`/docker/` or `/system.slice/docker-` are from Docker and its
containers, and if you have `/user.slice/` and `/system.slice/`, they
come from systemd running on your
machine.[]{#ch09.xhtml#idm45207097651360 primary="systemd"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097650656 primary="Docker"
secondary="id labels from" data-type="indexterm"} If you have other
software using cgroups, its cgroups will also be listed.

These metrics can be []{#ch09.xhtml#idm45207097649456
primary="scrape_configs" secondary="for cgroup scraping with cAdvisor"
secondary-sortas="cgroup" data-type="indexterm"}scraped with a
*prometheus.yml* such as:

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: cadvisor
   static_configs:
    - targets:
      - localhost:8080
```

::: {.section pdf-bookmark="CPU" data-type="sect2"}
::: {#ch09.xhtml#idm45207097624848 .sect2}
## CPU

You will find []{#ch09.xhtml#idm45207097619856 primary="cAdvisor"
secondary="container CPU metrics"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097618848 primary="CPUs"
secondary="metric for container CPUs"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097617936 primary="metrics"
secondary="container" tertiary="CPU"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097616720
primary="containers" secondary="CPU metrics" data-type="indexterm"}three
metrics for container CPU:
`container_cpu_usage_seconds_​`[`total`]{.keep-together},
`container_cpu_system_seconds_total`, and
`container_cpu_user_seconds_​`[`total`]{.keep-together}.

`container_cpu_usage_seconds_total` is split out by CPU, but not by
mode. [`container_cpu_system_seconds_total`]{.keep-together} and
`container_cpu_user_seconds_total` are the user and system modes,
respectively, similar to the Node Exporter's CPU collector, as described
in ["CPU Collector"](#ch07.xhtml#node_cpu){data-type="xref"}. These are
all counters with which you can use the `rate`
function.[]{#ch09.xhtml#idm45207097610000 primary="counters"
secondary="container CPU metrics" data-type="indexterm"}

::: {data-type="caution"}
###### Caution

With many containers and CPUs in one machine, you may find that the
aggregate cardinality of metrics from cAdvisor becomes a performance
issue.[]{#ch09.xhtml#idm45207097607984 primary="performance"
secondary="aggregate cAdvisor metrics, issue with"
data-type="indexterm"} You can use a `drop` relabel action, as discussed
in
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"},
to drop less-valuable metrics at scrape time.
:::
:::
:::

::: {.section pdf-bookmark="Memory" data-type="sect2"}
::: {#ch09.xhtml#idm45207097605232 .sect2}
## Memory

Similar to the Node Exporter, the []{#ch09.xhtml#idm45207097603648
primary="containers" secondary="memory usage metrics"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097579024 primary="memory"
secondary="container metrics for"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097578176
primary="cAdvisor" secondary="container memory metrics"
data-type="indexterm"}memory usage metrics are less than perfectly clear
and require digging through code and
[documentation](https://oreil.ly/VzlVe) to understand them.

`container_memory_cache` is the page cache used by the container, in
bytes. `container_memory_rss` is the resident set size (RSS), in
bytes.[]{#ch09.xhtml#idm45207097575024 primary="resident set size (RSS)"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097574320
primary="RSS (resident set size)" data-type="indexterm"} This is not the
same RSS or physical memory used as a process would have, as it excludes
the sizes of mapped
files.^[1](#ch09.xhtml#idm45207097573392){#ch09.xhtml#idm45207097573392-marker
data-type="noteref"}^ [`container`]{.keep-together}`_memory_usage_bytes`
is the RSS and the page cache, and is limited by
`container_spec_memory_limit_bytes` if the limit is nonzero.
`container_memory_working_set_bytes` is calculated by subtracting the
inactive [file-backed]{.keep-together} memory (`total_inactive_file`
from the kernel) from
`container_​`[`memory_usage_bytes`]{.keep-together}.

In practice, `container_memory_working_set_bytes` is the closest to RSS
that is exposed, and you may also wish to keep an eye on
`container_memory_usage_bytes` as it includes page cache.

In general, we would recommend relying on metrics such as
`process_resident_​`[`memory_bytes`]{.keep-together} from the process
itself rather than metrics from the cgroups. If your applications do not
expose Prometheus metrics, then cAdvisor is a good stopgap, and cAdvisor
metrics are still important for debugging and profiling.
:::
:::

::: {.section pdf-bookmark="Labels" data-type="sect2"}
::: {#ch09.xhtml#idm45207097564704 .sect2}
## Labels

Cgroups are organized in a hierarchy, with the */* cgroup at the root of
the hierarchy.[]{#ch09.xhtml#idm45207097562560 primary="containers"
secondary="labels"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097561584 primary="cgroups"
secondary="hierarchy of" data-type="indexterm"} The metrics for each of
your cgroups include the usage of the cgroups below
it.[]{#ch09.xhtml#idm45207097560512 primary="cAdvisor"
secondary="container labels"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097559568 primary="labels"
secondary="container" data-type="indexterm"} This goes against the usual
rule that within a metric the sum or average is meaningful, and is thus
an example of the table exception, as discussed in ["Table
Exception"](#ch05.xhtml#table_exception){data-type="xref"}.

In addition to the `id` label, cAdvisor adds in more labels about
containers if it has them. For Docker containers there will always be
the `image` and `name` labels, for the specific Docker image being run
and Docker's name for the container.[]{#ch09.xhtml#idm45207097555840
primary="Docker" secondary="container labels"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097554864
primary="image labels"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097554192
primary="name labels" data-type="indexterm"}

Any metadata labels Docker has for a container will also be included
with a `container_label_` prefix. []{#ch09.xhtml#idm45207097552496
primary="metadata" secondary="labels for Docker containers"
data-type="indexterm"}Arbitrary labels like these from a scrape can
break your monitoring, so you may wish to remove them with a
`labeldrop`, as shown in
[Example 9-1](#ch09.xhtml#prometheus_cadvisor_labeldrop){data-type="xref"},
and as we talked about in ["labeldrop and
labelkeep"](#ch08.xhtml#labeldrop){data-type="xref"}.^[2](#ch09.xhtml#idm45207097549152){#ch09.xhtml#idm45207097549152-marker
data-type="noteref"}^

::: {#ch09.xhtml#prometheus_cadvisor_labeldrop data-type="example"}
##### [Example 9-1. ]{.label}Using `labeldrop` to drop `container_label_ labels` from cAdvisor

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: cadvisor
   static_configs:
    - targets:
       - localhost:9090
   metric_relabel_configs:
    - regex: 'container_label_.*'
      action: labeldrop
```
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Kubernetes" data-type="sect1"}
::: {#ch09.xhtml#idm45207097670608 .sect1}
# Kubernetes

Kubernetes is a popular platform for orchestrating
containers.[]{#ch09.xhtml#idm45207097471008 primary="exporters"
secondary="cAdvisor" startref="ix_expcAd"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097519120
primary="cAdvisor" startref="ix_cAd"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097518176
primary="containers" secondary="cAdvisor" startref="ix_cntnrcAdv"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097516960
primary="containers" startref="ix_cntnr"
data-type="indexterm"}[]{#ch09.xhtml#ix_Kube primary="Kubernetes"
data-type="indexterm"} Like Prometheus, the Kubernetes project is part
of the Cloud Native Computing Foundation (CNCF). Here we are going to
cover running Prometheus on Kubernetes and working with its service
discovery.[]{#ch09.xhtml#idm45207097514944
primary="Cloud Native Computing Foundation (CNCF)"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097514304
primary="container orchestrators" seealso="Kubernetes"
data-type="indexterm"}

As Kubernetes is a large and fast-moving target, we are not going to
cover it in exhaustive detail. If you would like more depth, we suggest
the book *Kubernetes: Up and Running, 3rd Edition* by Brendan Burns, Joe
Beda, Kelsey Hightower, and Lachlan Evenson (O'Reilly).

::: {.section pdf-bookmark="Running in Kubernetes" data-type="sect2"}
::: {#ch09.xhtml#idm45207097512272 .sect2}
## Running in Kubernetes

To demonstrate using Prometheus with Kubernetes, we will use
[Minikube](https://oreil.ly/Sl3or), a tool used to run a single-node
Kubernetes cluster inside a virtual machine.[]{#ch09.xhtml#ix_Kuberun
primary="Kubernetes" secondary="running Prometheus in"
data-type="indexterm"}

Follow the steps in
[Example 9-2](#ch09.xhtml#minikube_run){data-type="xref"}. We are using
a Linux amd64 machine with VirtualBox already installed. If you are
running in a different environment, follow the [Minikube installation
documentation](https://oreil.ly/9oMjE). Here we are using Minikube
1.27.0 and Kubernetes 1.25.0.

::: {#ch09.xhtml#minikube_run data-type="example"}
##### [Example 9-2. ]{.label}Downloading and running Minikube

``` {data-type="programlisting"}
hostname $ curl -LO \
    https://storage.googleapis.com/minikube/releases/latesn/minikube-linux-amd64
hostname $ mv minikube-linux-amd64 minikube
hostname $ chmod +x minikube
hostname $ ./minikube start --kubernetes-version=v1.25.0
minikube v1.27.0 on Nixos 22.05 (Quokka)
Starting control plane node minikube in cluster minikube
Pulling base image ...
Downloading Kubernetes v1.25.0 preload ...
Creating docker container (CPUs=2, Memory=7800MB) ...
Preparing Kubernetes v1.25.0 on Docker 20.10.17 ...
> Generating certificates and keys ...
> Booting up control plane ...
> Configuring RBAC rules ...
Verifying Kubernetes components...
> Using image gcr.io/k8s-minikube/storage-provisioner:v5
Enabled addons: storage-provisioner, default-storageclass
```
:::

::: {data-type="tip"}
###### Tip

**`minikube dashboard --url`** will provide you with a URL for the
Kubernetes Dashboard, from which you can inspect your Kubernetes
cluster.
:::

You will also need to install `kubectl`, which is a command-line tool
used to interact with Kubernetes clusters.
[Example 9-3](#ch09.xhtml#kubectl_install){data-type="xref"} shows how
to install it and confirm that it can talk to your Kubernetes cluster.

::: {#ch09.xhtml#kubectl_install data-type="example"}
##### [Example 9-3. ]{.label}Downloading and testing `kubectl`

``` {data-type="programlisting"}
hostname $ wget \
    https://storage.googleapis.com/kubernetes-release/release/v1.25.0/bin/linux
    /amd64/kubectl
hostname $ chmod +x kubectl
hostname $ ./kubectl get services
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   44s
```
:::

[Example 9-4](#ch09.xhtml#kubectl_apply){data-type="xref"} shows how to
get an example Prometheus running on Minikube.
*prometheus-deployment.yml* contains permissions so that your Prometheus
can access resources such as pods and nodes in the cluster, a
*configMap* is created to hold the Prometheus configuration file, a
*deployment* to run Prometheus, and a *service* to make it easier for
you to access the Prometheus UI. The final command, the
`./minikube service`, will provide you with a URL where you can access
the Prometheus UI.

::: {#ch09.xhtml#kubectl_apply data-type="example"}
##### [Example 9-4. ]{.label}Setting up permissions and running Prometheus on Kubernetes

``` {data-type="programlisting"}
hostname $./kubectl apply -f prometheus-deployment.yml
hostname $./minikube service prometheus --url
http://192.168.99.100:30114
```
:::

The target status page should look like
[Figure 9-2](#ch09.xhtml#prometheus_targets_k8){data-type="xref"}. You
can find *prometheus-deployment.yml* [on
GitHub](https://oreil.ly/Qdwvp).

<figure class="smallerninety">
<div id="ch09.xhtml#prometheus_targets_k8" class="figure">
<img src="assets/pur2_0902.png" width="600" height="411"
alt="Target status page of Prometheus" />
<h6><span class="label">Figure 9-2. </span>Targets of the example
Prometheus running on Kubernetes</h6>
</div>
</figure>

This is a basic Kubernetes setup to demonstrate the core ideas behind
monitoring on Kubernetes, and it is not intended for you to use directly
in production; for example, all data is lost every time Prometheus
restarts.[]{#ch09.xhtml#idm45207097434992 primary="Kubernetes"
secondary="running Prometheus in" startref="ix_Kuberun"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Service Discovery" data-type="sect2"}
::: {#ch09.xhtml#idm45207097511680 .sect2}
## Service Discovery

There are currently six different[]{#ch09.xhtml#ix_KubeSD
primary="Kubernetes" secondary="service discovery"
data-type="indexterm"}[]{#ch09.xhtml#ix_serdiKube
primary="service discovery" secondary="Kubernetes"
data-type="indexterm"} types of Kubernetes service discoveries
[]{#ch09.xhtml#idm45207097429456 primary="endpoints (Kubernetes)"
data-type="indexterm"}you can use with Prometheus, namely *node*,
*endpoints*, *endpointslice*, *service*, *pod*, and
*ingress*.^[3](#ch09.xhtml#idm45207097426032){#ch09.xhtml#idm45207097426032-marker
data-type="noteref"}^ Prometheus uses the Kubernetes API to discover
targets.

::: {.section pdf-bookmark="Node" data-type="sect3"}
::: {#ch09.xhtml#idm45207097425312 .sect3}
### Node

Node service discovery is used to discover the nodes comprising the
Kubernetes cluster, and you will use it to monitor the infrastructure
around Kubernetes.[]{#ch09.xhtml#idm45207097423648 primary="Kubelet"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097422944
primary="node service discovery"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097422272
primary="Kubernetes" secondary="service discovery" tertiary="node"
data-type="indexterm"} The *Kubelet* is the name of the agent that runs
on each node, and you should scrape it as part of monitoring the health
of the Kubernetes cluster
([Example 9-5](#ch09.xhtml#kubelet_prometheus_yml){data-type="xref"}).

::: {#ch09.xhtml#kubelet_prometheus_yml data-type="example"}
##### [Example 9-5. ]{.label}prometheus.yml fragment to scrape the Kubelet

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
- job_name: 'kubelet'
  kubernetes_sd_configs:
  - role: node
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  authorization:
    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
```
:::

[Example 9-5](#ch09.xhtml#kubelet_prometheus_yml){data-type="xref"}
shows the configuration being used by Prometheus to scrape the Kubelet.
We are going to break down the scrape config:

``` {data-type="programlisting"}
job_name: 'kubelet'
```

A default `job` label[]{#ch09.xhtml#idm45207097390512
primary="job labels" secondary="kubelet" data-type="indexterm"} is
provided, and as there are no `relabel_configs`, `kubelet` will be the
`job`
label:^[4](#ch09.xhtml#idm45207097388096){#ch09.xhtml#idm45207097388096-marker
data-type="noteref"}^

``` {data-type="programlisting"}
kubernetes_sd_configs:
- role: node
```

A single Kubernetes service discovery is provided with the `node` role.
The `node` role discovers one target for each of your Kubelets. As
Prometheus is running inside the cluster, the defaults for the
Kubernetes service discovery are already set up to authenticate with the
Kubernetes API:

``` {data-type="programlisting"}
scheme: https
tls_config:
  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  insecure_skip_verify: true
authorization:
  credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
```

The Kubelet serves its */metrics* over HTTPS, so we must specify the
`scheme`. Kubernetes clusters usually have their own *certificate
authority* that are used to sign their TLS certs, and the `ca_file`
provides that for the scrape.[]{#ch09.xhtml#idm45207097381776
primary="ca_file" data-type="indexterm"}[]{#ch09.xhtml#idm45207097381072
primary="certificate authority"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097380400
primary="TLS certificates"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097319792
primary="scheme (scrape config)" data-type="indexterm"} Unfortunately
Minikube doesn't get this quite right, so `insecure_skip_verify` is
required to bypass security checks.

The `authorization` block and `credentials_file` parameters make
Prometheus use the service account token as the Bearer token when
scraping targets.[]{#ch09.xhtml#idm45207097317536
primary="credentials_file"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097316928
primary="authorization" data-type="indexterm"}

The returned target points at the Kubelet, and
authentication/authorization is turned off in this Minikube setup, so no
further configuration is needed.

::: {data-type="tip"}
###### Tip

The `tls_config` in the scrape config contains TLS settings for the
scrape. `kubernetes_sd_configs` also has a `tls_config` that contains
TLS settings for when service discovery talks to the Kubernetes
API.[]{#ch09.xhtml#idm45207097313648 primary="kubernetes_sd_configs"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097312944
primary="tls_config" data-type="indexterm"}
:::

The metadata available includes node annotations and labels, as you can
see in
[Figure 9-3](#ch09.xhtml#prometheus_sd_k8_kubelet){data-type="xref"}.
You could use this metadata with `relabel_configs` to add labels to
distinguish interesting subsets of nodes, such as those with different
hardware.[]{#ch09.xhtml#idm45207097310464 primary="metadata"
secondary="node service discovery in Kubernetes" data-type="indexterm"}

<figure>
<div id="ch09.xhtml#prometheus_sd_k8_kubelet" class="figure">
<img src="assets/pur2_0903.png" width="600" height="363"
alt="One discovered target on the service discovery status page" />
<h6><span class="label">Figure 9-3. </span>The Kubelet on the service
discovery status page of Prometheus</h6>
</div>
</figure>

The Kubelet's own */metrics* only contains metrics about the Kubelet
itself, not container-level information.
[]{#ch09.xhtml#idm45207097306336 primary="scraping"
secondary="Kubelet embedded cAdvisor"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097305344
primary="cAdvisor" secondary="embedded in Kubelet"
data-type="indexterm"}The Kubelet has an embedded cAdvisor on its
[*/metrics/cadvisor*]{.keep-together} endpoint. Scraping the embedded
cAdvisor only requires adding a [`metrics_path`]{.keep-together} to the
scrape config used with the Kubelet, as shown in
[Example 9-6](#ch09.xhtml#kubelet_prometheus_yml_cadvisor){data-type="xref"}.
The embedded cAdvisor includes labels for the Kubernetes `namespace` and
`pod_name`.[]{#ch09.xhtml#idm45207097300992 primary="metrics_path"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097300256
primary="namespace labels"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097299584
primary="pod_name labels" data-type="indexterm"}

::: {#ch09.xhtml#kubelet_prometheus_yml_cadvisor data-type="example"}
##### [Example 9-6. ]{.label}*prometheus.yml* fragment to scrape the Kubelet's embedded cAdvisor

``` {code-language="yaml" data-type="programlisting"}
- job_name: 'cadvisor'
  kubernetes_sd_configs:
  - role: node
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  authorization:
    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  metrics_path: /metrics/cadvisor
```
:::

Node service discovery can be used for anything you want to monitor that
runs on each machine in a Kubernetes cluster. If the Node Exporter was
running on your Minikube node, you could scrape it by relabeling the
port, for example.
:::
:::

::: {.section pdf-bookmark="Service" data-type="sect3"}
::: {#ch09.xhtml#idm45207097227712 .sect3}
### Service

Node service discovery is useful for monitoring the infrastructure of
and under Kubernetes, but not much use for monitoring your applications
running on [Kubernetes]{.keep-together}.[]{#ch09.xhtml#idm45207097270096
primary="services" secondary="service role in Kubernetes"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097269120
primary="Kubernetes" secondary="service discovery"
tertiary="service role" data-type="indexterm"}

There are several ways that you can organize your applications on
Kubernetes, and no single clear standard has emerged yet. But you are
likely using *services*, which is how applications on Kubernetes find
each other.

While there is a `service` role, it is not what you usually want. The
`service` role returns a single target for each
port^[5](#ch09.xhtml#idm45207097265696){#ch09.xhtml#idm45207097265696-marker
data-type="noteref"}^ of your services. Services are basically load
balancers, and scraping targets through load balancers is not wise, as
Prometheus can scrape a different application instance each time.
However, the `service` role can be useful for blackbox monitoring to
check if the service is responding at all.
:::
:::

::: {.section pdf-bookmark="Endpointslice" data-type="sect3"}
::: {#ch09.xhtml#idm45207097264256 .sect3}
### Endpointslice

Prometheus should be configured to have a target for each application
instance, and the `endpointslice` role provides just
that.[]{#ch09.xhtml#idm45207097261328
primary="endpointslice service discovery"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097260656
primary="Kubernetes" secondary="service discovery"
tertiary="endpointslice"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097259440
primary="pods (Kubernetes)" data-type="indexterm"} Services are backed
by *pods*. Pods are a group of tightly coupled containers that share
network and storage. For each Kubernetes service port, the endpoints
service discovery role returns a target for each pod backing that
service.[]{#ch09.xhtml#idm45207097258256
primary="endpoints (Kubernetes)" data-type="indexterm"} Additionally,
any other ports of the pods will be returned as targets.

That's a bit of a mouthful, so let's look at an
example.[]{#ch09.xhtml#idm45207097195152 primary="Minikube"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097194544
primary="kubernetes service"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097193936
primary="scrape_configs" secondary="for Kubernetes API servers"
secondary-sortas="Kubernetes" data-type="indexterm"} One of the services
that is running in your Minikube is the `kubernetes` service, which is
the Kubernetes API servers.
[Example 9-7](#ch09.xhtml#kubelet_prometheus_yml_apiserver){data-type="xref"}
is a scrape config that will discover and scrape the API servers.

::: {#ch09.xhtml#kubelet_prometheus_yml_apiserver data-type="example"}
##### [Example 9-7. ]{.label}*prometheus.yml* fragment used to scrape the Kubernetes API servers

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
- job_name: 'k8apiserver'
  kubernetes_sd_configs:
   - role: endpointslice
  scheme: https
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  authorization:
    credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
   - source_labels:
      - __meta_kubernetes_namespace
      - __meta_kubernetes_service_name
      - __meta_kubernetes_endpoint_port_name
     action: keep
     regex: default;kubernetes;https
```
:::

Breaking down this scrape config:

``` {data-type="programlisting"}
job_name: 'k8apiserver'
```

The `job` label is going to be `k8apiserver`, as there's no
target[]{#ch09.xhtml#idm45207097152784 primary="job labels"
secondary="k8apiserver" data-type="indexterm"} relabeling to change it:

``` {data-type="programlisting"}
kubernetes_sd_configs:
- role: endpointslice
```

::: {.note data-type="note"}
###### Note

Note that we are using the role `endpointslice` here, which is a
replacement for endpoints. In recent Kubernetes versions, the endpoint
role can only list up to 1,000 entries. In order to be able to list all
the targets, we recommend you use `endpointslices`, which do not have
that limitation, to address future growth of your infrastructure.
:::

There is a single Kubernetes service discovery using the `endpointslice`
role, which will return a target for every port of every pod backing
each of your services:

``` {data-type="programlisting"}
scheme: https
tls_config:
  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  insecure_skip_verify: true
authorization:
  credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
```

As with the Kubelet, the API servers are served over HTTPS. In addition,
authentication []{#ch09.xhtml#idm45207097146848 primary="authentication"
secondary="for Kubernetes API servers scrapes"
secondary-sortas="Kubernetes"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097145536
primary="credentials_file"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097144864
primary="relabel_configs" secondary="for Kubernetes API server scrapes"
secondary-sortas="Kubernetes" data-type="indexterm"}is required, which
is provided by the `credentials_file`:

``` {data-type="programlisting"}
relabel_configs:
- source_labels:
    - __meta_kubernetes_namespace
    - __meta_kubernetes_service_name
    - __meta_kubernetes_endpointslice_port_name
  action: keep
  regex: default;kubernetes;https
```

This relabel configuration will only return targets that are in the
`default` namespace, and are part of a service called `kubernetes` with
a port called `https`.

You can see the resulting target in
[Figure 9-4](#ch09.xhtml#prometheus_sd_k8_api){data-type="xref"}. The
API server is special, so there isn't much metadata. All the other
potential targets were dropped.

<figure>
<div id="ch09.xhtml#prometheus_sd_k8_api" class="figure">
<img src="assets/pur2_0904.png" width="588" height="510"
alt="One discovered target on the service discovery status page" />
<h6><span class="label">Figure 9-4. </span>The API server on the service
discovery status page of Prometheus</h6>
</div>
</figure>

While you will want to scrape the API servers, most of the time you'll
be focused on your applications.[]{#ch09.xhtml#idm45207097043888
primary="pods (Kubernetes)"
secondary="backing all Kubernetes services except API servers, scraping"
data-type="indexterm"}
[Example 9-8](#ch09.xhtml#k8_prometheus_yml_services){data-type="xref"}
shows how you can automatically scrape the pods for all of your
services.

::: {#ch09.xhtml#k8_prometheus_yml_services data-type="example"}
##### [Example 9-8. ]{.label}*prometheus.yml* fragment to scrape pods backing all Kubernetes services, except the API servers

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: 'k8services'
   kubernetes_sd_configs:
    - role: endpointslice
   relabel_configs:
    - source_labels:
       - __meta_kubernetes_namespace
       - __meta_kubernetes_service_name
      regex: default;kubernetes
      action: drop
    - source_labels:
       - __meta_kubernetes_namespace
      regex: default
      action: keep
    - source_labels: [__meta_kubernetes_service_name]
      target_label: job
```
:::

Once again we will break it down:

``` {data-type="programlisting"}
job_name: 'k8services'
kubernetes_sd_configs:
 - role: endpointslice
```

As with the previous example, this is providing a job name and the
Kubernetes `endpointslice` role, but this does not end up as the `job`
label due to later relabeling.

There are no HTTPS settings, as we know the targets are all plain HTTP.
There is no `credentials_file`, as no authentication is required, and
sending a bearer token to all of your services could allow them to
impersonate
you:^[6](#ch09.xhtml#idm45207096929856){#ch09.xhtml#idm45207096929856-marker
data-type="noteref"}^

``` {data-type="programlisting"}
relabel_configs:
- source_labels:
    - __meta_kubernetes_namespace
    - __meta_kubernetes_service_name
  regex: default;kubernetes
  action: drop
- source_labels:
    - __meta_kubernetes_namespace
  regex: default
  action: keep
```

We excluded the API server, as there is already another scrape config
handling it.[]{#ch09.xhtml#idm45207096928224 primary="default namespace"
data-type="indexterm"} We also only looked at the `default` namespace,
which is where we are launching
applications:^[7](#ch09.xhtml#idm45207096926960){#ch09.xhtml#idm45207096926960-marker
data-type="noteref"}^

``` {data-type="programlisting"}
- source_labels: [__meta_kubernetes_service_name]
  target_label: job
```

This relabel action takes the Kubernetes service name and uses it as the
`job` label.[]{#ch09.xhtml#idm45207096923136 primary="job labels"
secondary="Kubernetes service names" data-type="indexterm"} The
`job_name` we provided for the scrape config is only a default, and does
not apply.

In this way you can have your Prometheus automatically pick up new
services and start scraping them with a useful `job` label. In this case
that's just Prometheus itself, as shown in
[Figure 9-5](#ch09.xhtml#prometheus_sd_k8_svc_prom){data-type="xref"}.

<figure>
<div id="ch09.xhtml#prometheus_sd_k8_svc_prom" class="figure">
<img src="assets/pur2_0905.png" width="600" height="488"
alt="One discovered target on the service discovery status page" />
<h6><span class="label">Figure 9-5. </span>Prometheus has automatically
discovered itself using endpoint service <span
class="keep-together">discovery</span></h6>
</div>
</figure>

You could []{#ch09.xhtml#idm45207096916464 primary="relabeling"
secondary="using to add labels from Kubernetes service or pod metadata"
data-type="indexterm"}go a step further and use relabeling to add
additional labels from service or pod metadata, or even set `__scheme__`
or `__metrics_path__` based on a Kubernetes annotation, as shown in
[Example 9-9](#ch09.xhtml#prometheus_sd_k8_relabel_annotation){data-type="xref"}.
These would look for `prometheus.io/scheme`, `prometheus.io/path`, and
`prometheus.io/port` service
annotations,^[8](#ch09.xhtml#idm45207096890672){#ch09.xhtml#idm45207096890672-marker
data-type="noteref"}^ and use them if present.

::: {#ch09.xhtml#prometheus_sd_k8_relabel_annotation data-type="example"}
##### [Example 9-9. ]{.label}Relabeling using Kubernetes service annotations to optionally configure the scheme, path, and port of targets

``` {code-language="yaml" data-type="programlisting"}
relabel_configs:
 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
   regex: (.+)
   target_label: __scheme__
 - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
   regex: (.+)
   target_label: __metrics_path__
 - source_labels:
    - __address__
    - __meta_kubernetes_service_annotation_prometheus_io_port
   regex: ([^:]+)(:\d+)?;(\d+)
   replacement: ${1}:${3}
   target_label: __address__
```
:::

This is limited to monitoring only one port per service. You could have
another scrape config using the `prometheus.io/port2` annotation, and so
on for however many ports you need.
:::
:::

::: {.section pdf-bookmark="Pod" data-type="sect3"}
::: {#ch09.xhtml#pod_role .sect3}
### Pod

Discovering endpoints is great for monitoring the primary processes
backing your services, but it won't discover pods that are not part of
services.[]{#ch09.xhtml#idm45207096791520 primary="pods (Kubernetes)"
secondary="service discovery"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096790544
primary="Kubernetes" secondary="service discovery" tertiary="pod"
data-type="indexterm"}

The `pod` role discovers pods. It will return a target for each port of
every one of your pods. As it works off pods, service metadata such as
labels and annotations are not available, as pods do not know which
services they are members of. But you will have access to all pod
metadata. How you use this boils down to a question of what conventions
you want to use. The Kubernetes ecosystem is rapidly evolving, and there
is no one standard yet.

You could create a convention that all pods must be part of a service,
and then use the `endpointslice` role in service
discovery.[]{#ch09.xhtml#idm45207096787328
primary="endpointslice service discovery" data-type="indexterm"} You
could have a convention that all pods have a label indicating the
(single) Kubernetes service they are a part of, and use the `pod` role
for service discovery. As all ports have names, you could base a
convention off that and have ports named with a prefix of `prom-http` be
scraped with HTTP, and `prom-https` be scraped with HTTPS.

One of the components that comes with Minikube is *kube-dns*, which
provides DNS services. []{#ch09.xhtml#idm45207096784352
primary="kube-dns"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096783648 primary="DNS"
secondary="kube-dns"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096782704
primary="metrics (pod port)" data-type="indexterm"}Its pod has multiple
ports, including a port named `metrics` that serves Prometheus metrics.
[Example 9-10](#ch09.xhtml#k8_prometheus_yml_pod){data-type="xref"}
shows how you could discover this port and use the name of the container
as the `job` label, as
[Figure 9-6](#ch09.xhtml#prometheus_sd_k8_pods){data-type="xref"} shows.

::: {#ch09.xhtml#k8_prometheus_yml_pod data-type="example"}
##### [Example 9-10. ]{.label}*prometheus.yml* to discover all pod ports with the name `metrics` and to use the container name as the `job` label

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
- job_name: 'k8pods'
  kubernetes_sd_configs:
   - role: pod
  relabel_configs:
   - source_labels: [__meta_kubernetes_pod_container_port_name]
     regex: metrics
     action: keep
   - source_labels: [__meta_kubernetes_pod_container_name]
     target_label: job
```
:::

<figure>
<div id="ch09.xhtml#prometheus_sd_k8_pods" class="figure">
<img src="assets/pur2_0906.png" width="600" height="628"
alt="Discovered targets on the service discovery status page" />
<h6><span class="label">Figure 9-6. </span>Pod discovered using pod
service discovery</h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="Ingress" data-type="sect3"}
::: {#ch09.xhtml#idm45207096675328 .sect3}
### Ingress

An *ingress* is a way for a Kubernetes service to be exposed outside the
cluster.[]{#ch09.xhtml#idm45207096673392 primary="Kubernetes"
secondary="service discovery" tertiary="ingress"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096672144 primary="ingress"
data-type="indexterm"} As it is a layer on top of services, similar to
the `service` role, the `ingress` role is also basically a load
balancer. If multiple pods backed the service and thus ingress, this
would cause problems when scraping with Prometheus. Accordingly, you
should only use this role for blackbox
monitoring.[]{#ch09.xhtml#idm45207096710512 primary="service discovery"
secondary="Kubernetes" startref="ix_serdiKube"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096709264
primary="Kubernetes" secondary="service discovery" startref="ix_KubeSD"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="kube-state-metrics" data-type="sect2"}
::: {#ch09.xhtml#kube_state_metrics .sect2}
## kube-state-metrics

Using Kubernetes service discovery you can have Prometheus scrape your
applications and Kubernetes infrastructure, but this will not include
metrics about what Kubernetes knows about your services, pods,
deployments, and other resources.[]{#ch09.xhtml#idm45207096706112
primary="metrics" secondary="from kube-state-metrics"
secondary-sortas="kube"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096704864
primary="exporters" secondary="kube-state-metrics"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096703920
primary="Kubernetes" secondary="kube-state-metrics"
data-type="indexterm"} This is because applications such as the Kubelet
and Kubernetes API servers should expose information about their own
performance, not dump their internal data
structures.^[9](#ch09.xhtml#idm45207096702848){#ch09.xhtml#idm45207096702848-marker
data-type="noteref"}^

Instead, you would obtain such metrics from another
endpoint,^[10](#ch09.xhtml#idm45207096646160){#ch09.xhtml#idm45207096646160-marker
data-type="noteref"}^ or if that doesn't exist, have an exporter that
extracts the relevant information. For Kubernetes, `kube-state-metrics`
is that exporter.

To run `kube-state-metrics` you should follow the steps in
[Example 9-11](#ch09.xhtml#kubectl_apply_ksm){data-type="xref"} and then
visit the */metrics* on the returned URL in your browser. You can find
*kube-state-metrics.yml* [on GitHub](https://oreil.ly/xY3SK).

::: {#ch09.xhtml#kubectl_apply_ksm data-type="example"}
##### [Example 9-11. ]{.label}Running `kube-state-metrics`

``` {data-type="programlisting"}
hostname $./kubectl apply -f kube-state-metrics.yml
hostname $./minikube service kube-state-metrics --url
http://192.168.99.100:31774
```
:::

Some useful metrics include `kube_deployment_spec_replicas` for the
intended number of metrics in a deployment, `kube_node_status_condition`
for node problems, and `kube_pod_container_status_restarts_total` for
pod restarts.

::: {data-type="tip"}
###### Tip

This `kube-state-metrics` will be automatically scraped by Prometheus
due to the scrape config in
[Example 9-8](#ch09.xhtml#k8_prometheus_yml_services){data-type="xref"}.
:::

`kube-state-metrics` features several examples of enum and info metrics,
as discussed in ["Enum"](#ch05.xhtml#enum_metrics){data-type="xref"} and
["Info"](#ch05.xhtml#info_metrics){data-type="xref"} metrics, such as
`kube_node_status_condition` and `kube_pod_info`,
[respectively]{.keep-together}.
:::
:::
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="Alternative Deployments" data-type="sect1"}
::: {#ch09.xhtml#idm45207096630672 .sect1}
# Alternative Deployments

So far, you've []{#ch09.xhtml#idm45207096628928 primary="Kubernetes"
secondary="Prometheus deployment in, alternatives"
data-type="indexterm"}learned how to deploy Prometheus in Kubernetes
from scratch. While the task is not complex, there are a few projects
that provide additional resources,
CRD,^[11](#ch09.xhtml#idm45207096627824){#ch09.xhtml#idm45207096627824-marker
data-type="noteref"}^ and helpers, to make your life
easier.[]{#ch09.xhtml#idm45207096627168
primary="Helm, use to create components from Prometheus ecosystem"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096626528
primary="Prometheus Community Kubernetes Helm Charts"
data-type="indexterm"}[]{#ch09.xhtml#idm45207096625760
primary="Prometheus Operator project" data-type="indexterm"} While they
are out of scope for this book, we encourage you to look at two of them
in particular:

-   The [Prometheus Operator](https://oreil.ly/8S74Q) is a project
    supported by a large community, which includes CRD, configuration of
    Prometheus, and its targets.

-   The [Prometheus Community Kubernetes Helm
    Charts](https://oreil.ly/Mg824) provide more than 30 charts to spin
    up components from the Prometheus ecosystem using Helm.

Now that you have an idea about how to use Prometheus in container
environments, let's look at some of the common exporters you will run
into.[]{#ch09.xhtml#idm45207096620512 primary="Kubernetes"
startref="ix_Kube" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch09.xhtml#idm45207097573392-marker)^ Mapped files include both
`mmap` and any libraries used by a process. This is exposed by the
kernel as `file_​`[`mapped`]{.keep-together} but is not used by cAdvisor,
thus the standard RSS is not available from cAdvisor.

^[2](#ch09.xhtml#idm45207097549152-marker)^ The behavior of cAdvisor is
the main reason the `labeldrop` and `labelkeep` relabel actions
[]{#ch09.xhtml#idm45207097547664 primary="labeldrop (relabel action)"
data-type="indexterm"}[]{#ch09.xhtml#idm45207097546864
primary="labelkeep (relabel action)" data-type="indexterm"}were
originally added.

^[3](#ch09.xhtml#idm45207097426032-marker)^ Endpoints are deprecated in
Kubernetes, and we recommend using endpointslice.

^[4](#ch09.xhtml#idm45207097388096-marker)^ We don't use `node` as the
`job` label, as that's typically used for the Node Exporter.

^[5](#ch09.xhtml#idm45207097265696-marker)^ A service can have multiple
ports.

^[6](#ch09.xhtml#idm45207096929856-marker)^ This is also the case with
basic auth, but not for a challenge-response mechanism like TLS client
certificate authentication.

^[7](#ch09.xhtml#idm45207096926960-marker)^ And to not cause confusion
with
[Example 9-10](#ch09.xhtml#k8_prometheus_yml_pod){data-type="xref"}, as
`kube-dns` is in the `kube-system` namespace.

^[8](#ch09.xhtml#idm45207096890672-marker)^ Forward slashes are not
valid in label names, so they are sanitized to underscores.

^[9](#ch09.xhtml#idm45207096702848-marker)^ Put another way, a database
exporter does not dump the contents of the database as metrics.

^[10](#ch09.xhtml#idm45207096646160-marker)^ Such as the Kubelet
exposing cAdvisor's metrics on another endpoint.

^[11](#ch09.xhtml#idm45207096627824-marker)^ Kubernetes Custom Resources
Definition
:::
:::
:::

[]{#ch10.xhtml}

::: {#ch10.xhtml#sbo-rt-content}
::: {#ch10.xhtml#common_exporters_chapter .chapter}
# [Chapter 10. ]{.label}Common Exporters

You already saw the Node Exporter in
[Chapter 7](#ch07.xhtml#node_exporter_chapter){data-type="xref"}, but
there are literally hundreds of other exporters you can
use.[]{#ch10.xhtml#ix_exptr primary="exporters" data-type="indexterm"}

We are not going to go through all of the ever-growing number of
exporters out there; instead, we will show you some examples of the
types of things you will come across when using exporters. This will
prepare you to use exporters in your own environment.

At the simplest, exporters work out of the box, with no configuration
required on your part, as you already saw for the Node Exporter. Usually
you will need to do minimal configuration to tell the exporter which
application instance to scrape. At the far end, some exporters require
extensive configuration as the data they are working with is very
general.

You will generally have one exporter for every application instance that
needs one. []{#ch10.xhtml#idm45207096614384 primary="exporters"
secondary="guideline for" data-type="indexterm"}This is because the
intended way to use Prometheus is for every application to have direct
instrumentation and have Prometheus discover it and scrape it directly.
When that isn't possible, exporters are used, and you want to keep to
that architecture as much as possible. Having the exporter live right
beside the application instance it is exporting from is easier to manage
as you grow, and keeps your failure domains aligned. You will find that
some exporters violate this guideline and offer the ability to scrape
multiple instances, but you can still deploy them in the intended
fashion and use the techniques shown in
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"}
to remove any extraneous labels.

::: {.section pdf-bookmark="Consul" data-type="sect1"}
::: {#ch10.xhtml#consul_exporter .sect1}
# Consul

You already installed and ran Consul in
["Consul"](#ch08.xhtml#consul_sd){data-type="xref"}.
[]{#ch10.xhtml#idm45207096609552 primary="Consul" secondary="exporter"
startref="ix_Cnslexp" data-type="indexterm"}[]{#ch10.xhtml#ix_expCon
primary="exporters" secondary="Consul" data-type="indexterm"}Assuming it
is still running, you can download and run the Consul Exporter with the
commands in
[Example 10-1](#ch10.xhtml#consul_exporter_setup){data-type="xref"}.
Because Consul usually runs on port 8500, you don't need to do any extra
configuration as the Consul Exporter uses that port by
default.[]{#ch10.xhtml#idm45207096605888 primary="Consul"
secondary="exporter" tertiary="downloading and running"
data-type="indexterm"}

::: {#ch10.xhtml#consul_exporter_setup data-type="example"}
##### [Example 10-1. ]{.label}Downloading and running the Consul Exporter

``` {data-type="programlisting"}
hostname $ wget https://github.com/prometheus/consul_exporter/releases/
    download/v0.3.0/consul_exporter-0.8.0.linux-amd64.tar.gz
hostname $ tar -xzf consul_exporter-0.8.0.linux-amd64.tar.gz
hostname $ cd consul_exporter-0.8.0.linux-amd64/
hostname $ ./consul_exporter
msg="Starting consul_exporter" version="(version=0.8.0, branch=HEAD,
    revision=176aef0f2d437e9fd1cb3a9e29dc4730de717e05)"
build_context="(go=go1.17.6, user=root@566e953b1722, date=20220210-16:54:21)"
msg="Listening on address" address=:9107
```
:::

If you open *http://localhost:9107/metrics* in your browser, you will
see the metrics available.

The first metric you should make note of is `consul_up`.
[]{#ch10.xhtml#idm45207096598752 primary="Consul" secondary="exporter"
tertiary="metrics from"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096597472 primary="metrics"
secondary="from Consul Exporter" secondary-sortas="Consul"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096596256 primary="up"
secondary="consul_up" data-type="indexterm"}Some exporters will return
an HTTP error to Prometheus when fetching data fails, which results in
`up` being set to `0` in Prometheus. But many exporters will still be
successfully scraped in this scenario and use a metric such as
`consul_up` to indicate if there was a problem. Accordingly, when
alerting on Consul being down, you should check both `up` and
`consul_up`. If you stop Consul and then check the */metrics*, you will
see the value changes to `0`, and back to `1` again when Consul is
started again.

`consul_catalog_service_node_healthy` tells you about the health of the
various services in the Consul node, similar to how `kube-state-metrics`
(discussed in
["kube-state-metrics"](#ch09.xhtml#kube_state_metrics){data-type="xref"})
tells you about the health of nodes and containers but across an entire
Kubernetes cluster.

`consul_serf_lan_members` is the number of Consul agents in the
cluster.[]{#ch10.xhtml#idm45207096588752 primary="agents (Consul)"
data-type="indexterm"} You may wonder if this could come just from the
*leader* of the Consul cluster, but remember that each agent might have
a different view of how many members the cluster has if there is an
issue such as a network partition. In general, you should expose metrics
like this from every member of a cluster, and synthesize the value you
want using aggregation in PromQL.

There are also metrics about your Consul Exporter.
`consul_exporter_build_info` is its build information, and there are a
variety of `process_` and `go_` metrics about the process and the Go
runtime. These are useful for debugging issues with the Consul Exporter
itself.

You can configure Prometheus to scrape the Consul Exporter, as shown in
[Example 10-2](#ch10.xhtml#prometheus_yml_consul_exporter){data-type="xref"}.
Even though the scrape is going via an exporter, we used the `job` label
of `consul`, as it is really Consul we are
scraping.[]{#ch10.xhtml#idm45207096583296 primary="scrape_configs"
secondary="Prometheus scraping Consul Exporter" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

Exporters can be considered as a form of proxy. They take in a scrape
request from Prometheus, fetch metrics from a process, munge them into a
format that Prometheus can understand, and respond with them to
Prometheus.
:::

::: {#ch10.xhtml#prometheus_yml_consul_exporter data-type="example"}
##### [Example 10-2. ]{.label}*prometheus.yml* to scrape a local Consul Exporter

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: consul
   static_configs:
    - targets:
      - localhost:9107
```
:::
:::
:::

::: {.section pdf-bookmark="MySQLd" data-type="sect1"}
::: {#ch10.xhtml#mysqld_exporter .sect1}
# MySQLd

The MySQLd Exporter is a typical
exporter.[]{#ch10.xhtml#idm45207096572352 primary="exporters"
secondary="Consul" startref="ix_expCon"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096519744 primary="Consul"
secondary="exporter" startref="ix_Cnslexp"
data-type="indexterm"}[]{#ch10.xhtml#ix_MySQL primary="MySQLd Exporter"
data-type="indexterm"}[]{#ch10.xhtml#ix_expMyS primary="exporters"
secondary="MySQLd" data-type="indexterm"} To demonstrate it you will
need to launch an instance of MySQL and set up a Prometheus user.

You can then run MySQL and create the user, as shown in
[Example 10-3](#ch10.xhtml#downloading_running_mysql_exporter){data-type="xref"}.

::: {#ch10.xhtml#downloading_running_mysql_exporter data-type="example"}
##### [Example 10-3. ]{.label}Downloading and running the MySQL Exporter

``` {data-type="programlisting"}
hostname $ docker run -it --net=host --rm mysql mysql -h 127.0.0.1 -P 3306
    -uroot -pmy-secret-pw
mysql: [Warning] Using a password on the command line interface can be insecure.
mysql> CREATE USER 'prometheus'@'127.0.0.1' IDENTIFIED BY 'my-secret-prom-pw'
    WITH MAX_USER_CONNECTIONS 3;
Query OK, 0 rows affected (0.03 sec)

mysql> GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'prometheus'@'127.0.0.1';
Query OK, 0 rows affected (0.01 sec)
```
:::

Create a *my.cnf* file with the credentials shown in
[Example 10-4](#ch10.xhtml#with_prometheus_credentials){data-type="xref"}.

::: {#ch10.xhtml#with_prometheus_credentials data-type="example"}
##### [Example 10-4. ]{.label}*\~/.my.cnf* with Prometheus credentials

``` {data-type="programlisting"}
[client]
user = prometheus
password = my-secret-prom-pw
host = 127.0.0.1
```
:::

Next, you []{#ch10.xhtml#idm45207096496064 primary="MySQLd Exporter"
secondary="downloading and running" data-type="indexterm"}should
download and run the [MySQLd]{.keep-together} exporter, as shown in
[[Example 10-5](#ch10.xhtml#mysqld_exporter_setup){data-type="xref"}]{.keep-together}.

::: {#ch10.xhtml#mysqld_exporter_setup data-type="example"}
##### [Example 10-5. ]{.label}Downloading and running the MySQLd Exporter

``` {data-type="programlisting"}
hostname $ wget https://github.com/prometheus/mysqld_exporter/releases/download/
    v0.9.0/mysqld_exporter-0.9.0.linux-amd64.tar.gz
hostname $ tar -xzf mysqld_exporter-0.9.0.linux-amd64.tar.gz
hostname $ cd mysqld_exporter-0.9.0.linux-amd64/
hostname $ ./mysqld_exporter
```
:::

If you go to *http://localhost:9104/metrics*, you will see the metrics
being produced. []{#ch10.xhtml#idm45207096488384
primary="MySQLd Exporter" secondary="metrics from"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096487440 primary="metrics"
secondary="from MySQLd Exporter" secondary-sortas="MySQLd"
data-type="indexterm"}Similar to the Consul Exporter's `consul_up`,
there is a `mysql_up` metric, indicating if talking to MySQLd succeeded.

::: {.note data-type="note"}
###### Note

The name of the exporter is MySQLd Exporter. However, it also works with
forks of MySQL, such as MariaDB.
:::

You will notice that many MySQL-related metrics are presents:
`mysql_global_variables_` shows the value of global configuration
variables, and `mysql_global_status_` metrics show the current values
returned by `SHOW STATUS`.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch10.xhtml#idm45207096482080 .sidebar}
# Exporter Default Ports

You may have noticed that Prometheus, the Node Exporter, Alertmanager,
and other exporters in this chapter have similar port
numbers.[]{#ch10.xhtml#idm45207096480720
primary="ports, exporter default"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096480016
primary="exporters" secondary="default ports" data-type="indexterm"}

Back when there were only a handful of exporters, many had the same
default port number. Both the Node and HAProxy exporters used port 8080
by default, for example. This was annoying when trying out or deploying
Prometheus, so a [wiki page](https://oreil.ly/Cx_7b) was started to keep
the official exporters on different ports.

This organically grew to being a comprehensive list of exporters, and
aside from some users skipping over numbers, it now serves a purpose
beyond its initial one.
:::

```{=html}
</aside>
```
You can configure []{#ch10.xhtml#idm45207096477040
primary="MySQLd Exporter"
secondary="configuring for scraping by Prometheus"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096475968
primary="scraping" secondary="Prometheus scraping MySQLd Exporter"
data-type="indexterm"}the MySQLd Exporter to be scraped by Prometheus in
the same way as any other exporter, as you can see in
[Example 10-6](#ch10.xhtml#prometheus_yml_mysqld_exporter){data-type="xref"}.

::: {#ch10.xhtml#prometheus_yml_mysqld_exporter data-type="example"}
##### [Example 10-6. ]{.label}*prometheus.yml* to scrape a local MySQLd Exporter

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: mysqld
   static_configs:
    - targets:
      - localhost:9104
```
:::
:::
:::

::: {.section pdf-bookmark="Grok Exporter" data-type="sect1"}
::: {#ch10.xhtml#grok_exporter .sect1}
# Grok Exporter

Not all applications produce metrics in a form that can be converted
into something that Prometheus understands using an
exporter.[]{#ch10.xhtml#idm45207096405168 primary="exporters"
secondary="MySQLd" startref="ix_expMyS"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096449408
primary="MySQLd Exporter" startref="ix_MySQL"
data-type="indexterm"}[]{#ch10.xhtml#ix_Grok primary="Grok Exporter"
data-type="indexterm"}[]{#ch10.xhtml#ix_expGrok primary="exporters"
secondary="Grok" data-type="indexterm"}[]{#ch10.xhtml#idm45207096446304
primary="metrics" secondary="converting logs to with Grok Exporter"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096445392 primary="logging"
secondary="converting logs to metrics using Grok Exporter"
data-type="indexterm"} But such applications may produce logs, and the
[Grok Exporter](https://oreil.ly/6NaQL) can be used to convert those
into
metrics.^[1](#ch10.xhtml#idm45207096443696){#ch10.xhtml#idm45207096443696-marker
data-type="noteref"}^ Grok is a way to parse unstructured logs that is
commonly used with
Logstash.^[2](#ch10.xhtml#idm45207096441584){#ch10.xhtml#idm45207096441584-marker
data-type="noteref"}^ The Grok Exporter reuses the same pattern
language, allowing you to reuse patterns that you already
have.[]{#ch10.xhtml#idm45207096440800
primary="patterns (regular expressions)"
secondary="use by Grok Exporter" data-type="indexterm"}

Say that you had a simple log that looks like:

``` {data-type="programlisting"}
GET /foo 1.23
GET /bar 3.2
POST /foo 4.6
```

which was in a file called *example.log*. You could convert these logs
into metrics by using the Grok Exporter. First, download the [0.2.8 Grok
Exporter Linux amd64 release](https://oreil.ly/StPAZ) and unzip it.
[]{#ch10.xhtml#idm45207096394816 primary="Grok Exporter"
secondary="grok.yml to parse logfile and produce metrics"
data-type="indexterm"}Next, create a file called *grok.yml* with the
content in [Example 10-7](#ch10.xhtml#grok_yml){data-type="xref"}.

::: {#ch10.xhtml#grok_yml data-type="example"}
##### [Example 10-7. ]{.label}*grok.yml* to parse a simple logfile and produce metrics

``` {code-language="yaml" data-type="programlisting"}
global:
  config_version: 2
input:
  type: file
  path: example.log
  readall: true  # Use false in production
grok:
  additional_patterns:
   - 'METHOD [A-Z]+'
   - 'PATH [^ ]+'
   - 'NUMBER [0-9.]+'
metrics:
 - type: counter
   name: log_http_requests_total
   help: HTTP requests
   match: '%{METHOD} %{PATH:path} %{NUMBER:latency}'
   labels:
     path: '{{.path}}'
 - type: histogram
   name: log_http_request_latency_seconds_total
   help: HTTP request latency
   match: '%{METHOD} %{PATH:path} %{NUMBER:latency}'
   value: '{{.latency}}'
server:
   port: 9144
```
:::

Finally, run the Grok Exporter:

``` {data-type="programlisting"}
./grok_exporter -config grok.yml
```

We'll break this down. First, there is some boilerplate:

``` {data-type="programlisting"}
global:
  config_version: 2
```

Next, you need to define the file to be read. Here we are using
`readall: true`, so you will see the same results as in this example. In
production you would leave it to the default of `false` so that the file
is tailed:

``` {data-type="programlisting"}
input:
  type: file
  path: example.log
  readall: true  # Use false in production
```

Grok works with patterns based on regular expressions.
[]{#ch10.xhtml#idm45207096221488 primary="regular expressions"
secondary="patterns based on, use by Grok Exporter"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096220496
primary="Grok Exporter"
secondary="use of patterns based on regular expressions"
data-type="indexterm"}We have defined all of our patterns here manually
so you can better understand what's going on, but you can also reuse
ones you already have:

``` {data-type="programlisting"}
grok:
  additional_patterns:
   - 'METHOD [A-Z]+'
   - 'PATH [^ ]+'
   - 'NUMBER [0-9.]+'
```

We have two metrics.[]{#ch10.xhtml#idm45207096218272
primary="path labels"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096217536 primary="metrics"
secondary="from Grok Exporter" secondary-sortas="Grok"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096216320
primary="Grok Exporter" secondary="metrics produced by"
data-type="indexterm"} The first is a counter called
`log_http_requests_total`, which has a label `path`:

``` {data-type="programlisting"}
metrics:
 - type: counter
   name: log_http_requests_total
   help: HTTP requests
   match: '%{METHOD} %{PATH:path} %{NUMBER:latency}'
   labels:
     path: '{{.path}}'
```

Our second is a histogram called
`log_http_request_latency_seconds_total`,
which[]{#ch10.xhtml#idm45207096212768 primary="HTTP requests"
secondary="logging latency for, using Grok Exporter"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096211824 primary="latency"
secondary="logging for HTTP requests using Grok Exporter"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096210912
primary="histograms" secondary="HTTP request latency from Grok Exporter"
data-type="indexterm"} is observing the latency value, and has no
labels:

``` {data-type="programlisting"}
 - type: histogram
   name: log_http_request_latency_seconds_total
   help: HTTP request latency
   match: '%{METHOD} %{PATH:path} %{NUMBER:latency}'
   value: '{{.latency}}'
```

Finally, we define where we want the []{#ch10.xhtml#idm45207096208432
primary="Grok Exporter" secondary="defining where to expose its metrics"
data-type="indexterm"}exporter to expose its metrics:

``` {data-type="programlisting"}
server:
    port: 9144
```

When you visit *http://localhost:9144*, among its output you will find
the[]{#ch10.xhtml#idm45207096205664 primary="metrics"
secondary="from Grok Exporter" secondary-sortas="Grok"
data-type="indexterm"} following metrics:

``` {data-type="programlisting"}
# HELP log_http_request_latency_seconds_total HTTP request latency
# TYPE log_http_request_latency_seconds_total histogram
log_http_request_latency_seconds_total_bucket{le="0.005"} 0
log_http_request_latency_seconds_total_bucket{le="0.01"} 0
log_http_request_latency_seconds_total_bucket{le="0.025"} 0
log_http_request_latency_seconds_total_bucket{le="0.05"} 0
log_http_request_latency_seconds_total_bucket{le="0.1"} 1
log_http_request_latency_seconds_total_bucket{le="0.25"} 2
log_http_request_latency_seconds_total_bucket{le="0.5"} 3
log_http_request_latency_seconds_total_bucket{le="1"} 3
log_http_request_latency_seconds_total_bucket{le="2.5"} 3
log_http_request_latency_seconds_total_bucket{le="5"} 3
log_http_request_latency_seconds_total_bucket{le="10"} 3
log_http_request_latency_seconds_total_bucket{le="+Inf"} 3
log_http_request_latency_seconds_total_sum 0.57
log_http_request_latency_seconds_total_count 3
# HELP log_http_requests_total HTTP requests
# TYPE log_http_requests_total counter
log_http_requests_total{path="/bar"} 1
log_http_requests_total{path="/foo"} 2
```

As you can see, the Grok Exporter is more involved to configure than
your typical exporter; it's closer to direct instrumentation in terms of
effort, as you must individually define each metric you want to
expose.[]{#ch10.xhtml#idm45207096202096 primary="Grok Exporter"
secondary="configuring for scraping by Prometheus"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096201056
primary="scraping" secondary="prometheus.yml to scrape Grok Exporter"
data-type="indexterm"} You would generally run one per application
instance that needs to be monitored, and scrape it with Prometheus in
the usual way, as shown in
[Example 10-8](#ch10.xhtml#prometheus_yml_grok_exporter){data-type="xref"}.

::: {#ch10.xhtml#prometheus_yml_grok_exporter data-type="example"}
##### [Example 10-8. ]{.label}*prometheus.yml* to scrape a local Grok Exporter

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: grok
   static_configs:
    - targets:
       - localhost:9144
```
:::
:::
:::

::: {.section pdf-bookmark="Blackbox" data-type="sect1"}
::: {#ch10.xhtml#blackbox .sect1}
# Blackbox

While the recommended way to deploy exporters
[]{#ch10.xhtml#idm45207096161104 primary="Grok Exporter"
startref="ix_Grok"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096160128
primary="exporters" secondary="Grok" startref="ix_expGrok"
data-type="indexterm"}[]{#ch10.xhtml#ix_blckbx
primary="Blackbox exporters"
data-type="indexterm"}[]{#ch10.xhtml#ix_expBlck primary="exporters"
secondary="Blackbox" data-type="indexterm"}is to run one right beside
each application instance, there are cases where this is not possible
for technical
reasons.^[3](#ch10.xhtml#idm45207096141264){#ch10.xhtml#idm45207096141264-marker
data-type="noteref"}^ This is usually the case with blackbox
monitoring---monitoring the system from the outside with no special
knowledge of the internals. We like to think of blackbox monitoring as
similar to smoke tests when unit testing; their purpose is primarily to
quickly tell you when things have gone hilariously wrong.

If you are monitoring whether a web service works from the standpoint of
a user, you usually want to monitor that through the same load balancers
and virtual IP (VIP) addresses the user is hitting. You can't exactly
run an exporter on a VIP as it is, well, virtual. A different
architecture is needed.

In Prometheus there is a class of exporters usually referred to as
Blackbox-style or SNMP-style, after the two primary examples of
exporters that cannot run beside an application
instance.[]{#ch10.xhtml#idm45207096139808 primary="SNMP-style exporters"
seealso="Blackbox exporters" data-type="indexterm"} The Blackbox
Exporter by necessity usually needs to run somewhere else on the
network, and there is no application instance to run on. For the
SNMP^[4](#ch10.xhtml#idm45207096138544){#ch10.xhtml#idm45207096138544-marker
data-type="noteref"}^ Exporter, it's rare for you to be able to run your
own code on a network device---and if you could, you would use the Node
Exporter instead.

So how are Blackbox-style or SNMP-style exporters different? Instead of
you configuring them to talk to only one target, they take in the target
as a URL parameter.[]{#ch10.xhtml#idm45207096137344
primary="Blackbox exporters" secondary="downloading and running"
data-type="indexterm"} Any other configuration is provided by you on the
exporter side as usual. This keeps the responsibilities of service
discovery and scrape scheduling with Prometheus, and the responsibility
of translating metrics into a form understandable by Prometheus with
your exporter.

The Blackbox Exporter allows you to perform ICMP, TCP, HTTP, and DNS
probing. We will show you each in turn, but first you should get the
Blackbox Exporter running, as shown in
[Example 10-9](#ch10.xhtml#blackbox_exporter_setup){data-type="xref"}.

::: {#ch10.xhtml#blackbox_exporter_setup data-type="example"}
##### [Example 10-9. ]{.label}Downloading and running the Blackbox Exporter

``` {data-type="programlisting"}
hostname $ wget https://github.com/prometheus/blackbox_exporter/releases/download/
    v0.22.0/blackbox_exporter-0.22.0.linux-amd64.tar.gz
hostname $ tar -xzf blackbox_exporter-0.22.0.linux-amd64.tar.gz
hostname $ cd blackbox_exporter-0.22.0.linux-amd64/
hostname $ sudo ./blackbox_exporter
msg="Starting blackbox_exporter" version="(version=0.22.0,
  branch=HEAD, revision=0bbd65d1264722f7afb87a72ec4128b9214e5840)"
msg="Loaded config file"
msg="Listening on address" address=:9115
```
:::

If you visit *http://localhost:9115/* in your browser, you should see a
status page like the one in
[Figure 10-1](#ch10.xhtml#blackbox_status_empty){data-type="xref"}.

<figure>
<div id="ch10.xhtml#blackbox_status_empty" class="figure">
<img src="assets/pur2_1001.png" width="600" height="606"
alt="A Blackbox Exporter status page with no recent probes." />
<h6><span class="label">Figure 10-1. </span>The Blackbox Exporter’s
status page</h6>
</div>
</figure>

::: {.section pdf-bookmark="ICMP" data-type="sect2"}
::: {#ch10.xhtml#idm45207096086560 .sect2}
## ICMP

The Internet Control Message Protocol (ICMP) is a part of the Internet
Protocol (IP).[]{#ch10.xhtml#ix_ICMP primary="ICMP probing"
data-type="indexterm"}[]{#ch10.xhtml#ix_blckbxICMP
primary="Blackbox exporters" secondary="ICMP probes"
data-type="indexterm"} In the context of the Blackbox exporter, it is
the *echo reply* and *echo request* messages that are of interest to
you, more commonly known as
*ping*.^[5](#ch10.xhtml#idm45207096080656){#ch10.xhtml#idm45207096080656-marker
data-type="noteref"}^

::: {.note data-type="note"}
###### Note

ICMP uses raw sockets so it requires more privileges than a typical
exporter, which is why
[Example 10-9](#ch10.xhtml#blackbox_exporter_setup){data-type="xref"}
uses `sudo`. []{#ch10.xhtml#idm45207096077536 primary="ICMP probing"
secondary="extra privileges required for" data-type="indexterm"}On Linux
you could instead give the Blackbox Exporter the `CAP_NET_RAW`
[capability]{.keep-together}.
:::

To start, you should ask the Blackbox Exporter to ping localhost by
visiting *http://localhost:9115/probe?module=icmp&target=localhost* in
your browser, which should produce something
[]{#ch10.xhtml#idm45207096074432 primary="ICMP probing"
secondary="pinging localhost" data-type="indexterm"}like:

``` {data-type="programlisting"}
# HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns
    lookup in seconds
# TYPE probe_dns_lookup_time_seconds gauge
probe_dns_lookup_time_seconds 0.000580415
# HELP probe_duration_seconds Returns how long the probe took to complete
    in seconds
# TYPE probe_duration_seconds gauge
probe_duration_seconds 0.001044791
# HELP probe_icmp_duration_seconds Duration of icmp request by phase
# TYPE probe_icmp_duration_seconds gauge
probe_icmp_duration_seconds{phase="resolve"} 0.000580415
probe_icmp_duration_seconds{phase="rtt"} 0.000123794
probe_icmp_duration_seconds{phase="setup"} 0.000130416
# HELP probe_icmp_reply_hop_limit Replied packet hop limit (TTL for ipv4)
# TYPE probe_icmp_reply_hop_limit gauge
probe_icmp_reply_hop_limit 64
# HELP probe_ip_addr_hash Specifies the hash of IP address. It's useful
    to detect if the IP address changes.
# TYPE probe_ip_addr_hash gauge
probe_ip_addr_hash 1.751717746e+09
# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6
# TYPE probe_ip_protocol gauge
probe_ip_protocol 6
# HELP probe_success Displays whether or not the probe was a success
# TYPE probe_success gauge
probe_success 1
```

The key metric here is `probe_success`, which is `1` if your probe
succeeded and `0` otherwise.[]{#ch10.xhtml#idm45207096069616
primary="probe_success" data-type="indexterm"} This is similar to
`consul_up`, and you should check that neither `up` nor `probe_success`
are `0` when alerting. There is an example of this in
["for"](#ch18.xhtml#for){data-type="xref"}.

::: {data-type="tip"}
###### Tip

The */metrics* of the Blackbox Exporter provides metrics about the
Blackbox Exporter itself, such as how much CPU it has used. To perform
blackbox probes, you use */probe*.
:::

There are also other useful []{#ch10.xhtml#idm45207096063648
primary="metrics" secondary="produced by Blackbox probes"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096062704
primary="durations" secondary="metrics on"
tertiary="probe_duration_seconds" data-type="indexterm"}metrics that all
types of probes produce. `probe_ip_protocol` indicates the IP protocol
used, IPv4 in this case; `probe_ip_addr_hash` is a hash of the IP
address, useful to detect when it changes; and `probe_duration_seconds`
is how long the entire probe took, including DNS resolution.

::: {.warning data-type="warning"}
###### Warning

The name resolution used by Prometheus and the Blackbox Exporter is DNS
resolution, not the `gethostbyname` syscall.
[]{#ch10.xhtml#idm45207096058176 primary="DNS"
secondary="name resolution used by Prometheus and Blackbox exporters"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096057120
primary="Blackbox exporters" secondary="DNS name resolution"
data-type="indexterm"}Other potential sources of name resolution, such
as */etc/hosts* and *nsswitch.conf*, are not considered by the Blackbox
Exporter. This can lead to the `ping` command working, but the Blackbox
Exporter failing due to not being able to resolve its target via DNS.
:::

If you look inside *blackbox.yml*, you will find the `icmp` module:

``` {data-type="programlisting"}
  icmp:
    prober: icmp
```

This says that there is a module called `icmp`, which you had requested
with the [`?module=icmp`]{.keep-together} in the URL.
[]{#ch10.xhtml#idm45207096050528 primary="ICMP probing"
secondary="icmp module" data-type="indexterm"}This module uses the
`icmp` prober, with no additional options specified. ICMP is quite
simple, so only in niche use cases might you need to specify
`dont_fragment` or `payload_size`.

You can also try other targets. For example, to probe *google.com* you
can visit *http://localhost:9115/probe?module=icmp&
target=www.google.com* in your browser. []{#ch10.xhtml#idm45207096046704
primary="ICMP probing"
secondary="failed probe from IPv6 target URL parameter"
data-type="indexterm"}For the `icmp` probe, the `target` URL parameter
is an IP address or hostname.

You may find that this probe fails, with output like:

``` {data-type="programlisting"}
# HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns
    lookup in seconds
# TYPE probe_dns_lookup_time_seconds gauge
probe_dns_lookup_time_seconds 0.018805905
# HELP probe_duration_seconds Returns how long the probe took to complete
    in seconds
# TYPE probe_duration_seconds gauge
probe_duration_seconds 0.019061888
# HELP probe_icmp_duration_seconds Duration of icmp request by phase
# TYPE probe_icmp_duration_seconds gauge
probe_icmp_duration_seconds{phase="resolve"} 0.018805905
probe_icmp_duration_seconds{phase="rtt"} 0
probe_icmp_duration_seconds{phase="setup"} 9.8677e-05
# HELP probe_ip_addr_hash Specifies the hash of IP address. It's useful to
detect if the IP address changes.
# TYPE probe_ip_addr_hash gauge
probe_ip_addr_hash 4.125764906e+09
# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6
# TYPE probe_ip_protocol gauge
probe_ip_protocol 6
# HELP probe_success Displays whether or not the probe was a success
# TYPE probe_success gauge
probe_success 0
```

`probe_success` is `0` here, indicating the failure. Notice that
`probe_ip_protocol` is `6`, indicating IPv6.
[]{#ch10.xhtml#idm45207096041184 primary="probe_ip_protocol"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096040448 primary="IPv6"
data-type="indexterm"}In this case the machine we are using doesn't have
a working IPv6 setup. Why is the Blackbox Exporter using IPv6?

When resolving the Blackbox Exporter, targets will prefer a returned
IPv6 address if there is one; otherwise, it will use an IPv4 address.
*google.com* has both, so IPv6 is chosen and fails on our
machine.[]{#ch10.xhtml#idm45207096038624 primary="ICMP probing"
secondary="probing google.com with debug=true ending URL"
data-type="indexterm"}

You can see this in more detail if you add `&debug=true` on to the end
of the URL, giving
*http://localhost:9115/probe?module=icmp&target=www.google.com&debug=true*,
which will produce output like:

``` {data-type="programlisting"}
Logs for the probe:
... module=icmp target=www.google.com level=info
        msg="Beginning probe" probe=icmp timeout_seconds=119.5
... module=icmp target=www.google.com level=info
        msg="Resolving target address" preferred_ip_protocol=ip6
... module=icmp target=www.google.com level=info
        msg="Resolved target address" ip=2a00:1450:400c:c07::69
... module=icmp target=www.google.com level=info
        msg="Creating socket"
... module=icmp target=www.google.com level=info
        msg="Creating ICMP packet" seq=10 id=3483
... module=icmp target=www.google.com level=info
        msg="Writing out packet"
... module=icmp target=www.google.com level=warn
        msg="Error writing to socket" err="write udp
        [::]:3->[2a00:1450:400c:c07::69]:0: sendto: network is unreachable"
... module=icmp target=www.google.com level=error
        msg="Probe failed" duration_seconds=0.001902969

Metrics that would have been returned:
# HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns
    lookup in seconds
# TYPE probe_dns_lookup_time_seconds gauge
probe_dns_lookup_time_seconds 0.001635165
# HELP probe_duration_seconds Returns how long the probe took to complete
    in seconds
# TYPE probe_duration_seconds gauge
probe_duration_seconds 0.001902969
# HELP probe_icmp_duration_seconds Duration of icmp request by phase
# TYPE probe_icmp_duration_seconds gauge
probe_icmp_duration_seconds{phase="resolve"} 0.001635165
probe_icmp_duration_seconds{phase="rtt"} 0
probe_icmp_duration_seconds{phase="setup"} 9.6612e-05
# HELP probe_ip_addr_hash Specifies the hash of IP address. It's useful to
    detect if the IP address changes.
# TYPE probe_ip_addr_hash gauge
probe_ip_addr_hash 4.142542525e+09
# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6
# TYPE probe_ip_protocol gauge
probe_ip_protocol 6
# HELP probe_success Displays whether or not the probe was a success
# TYPE probe_success gauge
probe_success 0

Module configuration:
prober: icmp
http:
    ip_protocol_fallback: true
    follow_redirects: true
    enable_http2: true
tcp:
    ip_protocol_fallback: true
icmp:
    ip_protocol_fallback: true
    ttl: 64
dns:
    ip_protocol_fallback: true
    recursion_desired: true
```

The debug output is extensive, and by carefully reading through it you
can understand exactly what the probe is doing. The error you see here
is from the `sendto` syscall, which cannot assign an IPv6
address.[]{#ch10.xhtml#idm45207096032784 primary="IPv4"
data-type="indexterm"} To prefer IPv4 instead, you can add a new module
with the `preferred_ip_protocol: ipv4` option to *blackbox.yml*:

``` {data-type="programlisting"}
  icmp_ipv4:
    prober: icmp
    icmp:
      preferred_ip_protocol: ip4
```

After restarting the Blackbox
Exporter,^[6](#ch10.xhtml#idm45207096029936){#ch10.xhtml#idm45207096029936-marker
data-type="noteref"}^ if you use this module via
*http://localhost:9115/probe?module=icmp_ipv4&target=www.google.com*, it
will now work via IPv4.[]{#ch10.xhtml#idm45207096027504
primary="ICMP probing" startref="ix_ICMP"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096026480
primary="Blackbox exporters" secondary="ICMP probes"
startref="ix_blckbxICMP" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="TCP" data-type="sect2"}
::: {#ch10.xhtml#idm45207096085616 .sect2}
## TCP

The Transmission Control Protocol is the TCP in TCP/IP.
[]{#ch10.xhtml#ix_TCP primary="TCP probing"
data-type="indexterm"}[]{#ch10.xhtml#ix_blckbxTCP
primary="Blackbox exporters" secondary="TCP probes"
data-type="indexterm"}Many standard protocols use it, including websites
(HTTP), email (SMTP), remote login (Telnet and SSH), and chat (IRC). The
`tcp` probe of the Blackbox Exporter allows you to check TCP services,
and perform simple conversations for those that use line-based text
protocols.[]{#ch10.xhtml#idm45207096021024 primary="SSH"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096020320
primary="TCP probing"
secondary="checking if local SSH server is listening on port 22"
data-type="indexterm"}

To start, you can check if your local SSH server is listening on port 22
with
*http://localhost:9115/probe?module=tcp_connect&target=localhost:22*:

``` {data-type="programlisting"}
# HELP probe_dns_lookup_time_seconds Returns the time taken for probe dns lookup
    in seconds
# TYPE probe_dns_lookup_time_seconds gauge
probe_dns_lookup_time_seconds 0.000202381
# HELP probe_duration_seconds Returns how long the probe took to complete in
    seconds
# TYPE probe_duration_seconds gauge
probe_duration_seconds 0.000881654
# HELP probe_failed_due_to_regex Indicates if probe failed due to regex
# TYPE probe_failed_due_to_regex gauge
probe_failed_due_to_regex 0
# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6
# TYPE probe_ip_protocol gauge
probe_ip_protocol 4
# HELP probe_success Displays whether or not the probe was a success
# TYPE probe_success gauge
probe_success 1
```

This is quite similar to the metrics produced by the ICMP probe, and you
can see that this probe succeeded as `probe_success` is `1`.
[]{#ch10.xhtml#idm45207096016016 primary="TCP probing"
secondary="tcp_connect module" data-type="indexterm"}The definition of
the `tcp_connect` module in *blackbox.yml* is:

``` {data-type="programlisting"}
  tcp_connect:
    prober: tcp
```

This will try to connect to your target, and once it is connected
immediately, it will close the
connection.[]{#ch10.xhtml#idm45207096012880 primary="ssh_banner module"
data-type="indexterm"}[]{#ch10.xhtml#idm45207096012176
primary="TCP probing" secondary="ssh_banner module"
data-type="indexterm"} The `ssh_banner` module goes further, checking
for a particular response from the remote server:

``` {data-type="programlisting"}
  ssh_banner:
    prober: tcp
    tcp:
      query_response:
      - expect: "^SSH-2.0-"
```

As the very start of an SSH session is in plain text, you can check for
this part of the protocol with the `tcp` probe. This is better than
`tcp_connect`, as you are not only checking that the TCP port is open,
but that the server on the other end is responding with an SSH banner.

If your server returned something[]{#ch10.xhtml#idm45207096007776
primary="regular expressions"
secondary="expect regex failing in TCP probe" data-type="indexterm"}
different, the `expect` regex will not match, and `probe_success` will
be `0`. In addition, `probe_failed_due_to_regex` would be `1`. Since
Prometheus is a metrics-based system, the full debug output cannot be
saved, as that would be event
logging.^[7](#ch10.xhtml#idm45207096004432){#ch10.xhtml#idm45207096004432-marker
data-type="noteref"}^ However, the Blackbox Exporter can provide a small
number of metrics to help you to piece together what went wrong after
the fact.

::: {data-type="tip"}
###### Tip

If you find that every service needs a different module, consider
standardizing what your health checks look like across
services.[]{#ch10.xhtml#idm45207096002800
primary="health checks, standardizing across services"
data-type="indexterm"} If a service exposes a */metrics* page, then
there is not much need for basic connectivity checks with the Blackbox
Exporter, as Prometheus's scrapes will already provide that.
:::

The `tcp` probe can also connect via TLS.
[]{#ch10.xhtml#idm45207096000208 primary="TCP probing"
secondary="tcp_connect_tls"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095999232 primary="TLS"
secondary="tcp probe connecting via" data-type="indexterm"}Add a
`tcp_connect_tls` to your *blackbox.yml* file with the following
configuration:

``` {data-type="programlisting"}
  tcp_connect_tls:
    prober: tcp
    tcp:
      tls: true
```

After restarting the Blackbox Exporter, if you now visit
*http://localhost:9115/probe?module=tcp_connect_tls&target=www.oreilly.com:443*,
you can check if O'Reilly's website can be contacted with
HTTPS.^[8](#ch10.xhtml#idm45207095995360){#ch10.xhtml#idm45207095995360-marker
data-type="noteref"}^ For the `tcp` prober, the `target` URL parameter
is an IP address or hostname, followed by a colon, and then the port
[number]{.keep-together}.

You may notice among the metrics output:

``` {data-type="programlisting"}
# HELP probe_ssl_last_chain_expiry_timestamp_seconds Returns last SSL chain
  expiry in timestamp
# TYPE probe_ssl_last_chain_expiry_timestamp_seconds gauge
probe_ssl_last_chain_expiry_timestamp_seconds 1.686095999e+09
```

`probe_ssl_last_chain_expiry_timestamp_seconds` is produced as a side
effect of probing, indicating when your TLS/SSL
certificate^[9](#ch10.xhtml#idm45207095991168){#ch10.xhtml#idm45207095991168-marker
data-type="noteref"}^ will expire. []{#ch10.xhtml#idm45207095990416
primary="TLS/SSL certificate expiring" data-type="indexterm"}You can use
this to catch expiring certificates before they become outages.

While HTTP is a line-oriented text
protocol^[10](#ch10.xhtml#idm45207095989024){#ch10.xhtml#idm45207095989024-marker
data-type="noteref"}^ that you could use the `tcp` probe with, there is
an `http` probe that is more suitable for this
purpose.[]{#ch10.xhtml#idm45207095987264 primary="Blackbox exporters"
secondary="TCP probes" startref="ix_blckbxTCP"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095986016
primary="TCP probing" startref="ix_TCP" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="HTTP" data-type="sect2"}
::: {#ch10.xhtml#idm45207096025008 .sect2}
## HTTP

The HyperText Transfer Protocol (HTTP) is the basis for the modern web,
and likely what most of the services you provide
use.[]{#ch10.xhtml#ix_blckbxHTTP primary="Blackbox exporters"
secondary="HTTP probes" data-type="indexterm"}[]{#ch10.xhtml#ix_HTTPprb
primary="HTTP" secondary="probing with Blackbox exporter"
data-type="indexterm"} While most monitoring of web applications is best
done by Prometheus scraping metrics over HTTP, sometimes you will want
to perform blackbox monitoring of your HTTP services.

The `http` prober takes a
URL^[11](#ch10.xhtml#idm45207095979616){#ch10.xhtml#idm45207095979616-marker
data-type="noteref"}^ for the `target` URL parameter. If you visit
*http://localhost:9115/probe?module=http_2xx&target=https://www.oreilly.com/*,
you can check O'Reilly's website over HTTPS using the `http_2xx`
module,^[12](#ch10.xhtml#idm45207095977520){#ch10.xhtml#idm45207095977520-marker
data-type="noteref"}^ producing output [similar to]{.keep-together}:

``` {data-type="programlisting"}
# HELP probe_dns_lookup_time_seconds Returns the time taken for probe
    dns lookup in seconds
# TYPE probe_dns_lookup_time_seconds gauge
probe_dns_lookup_time_seconds 0.001481084
# HELP probe_duration_seconds Returns how long the probe took to complete
    in seconds
# TYPE probe_duration_seconds gauge
probe_duration_seconds 0.165316519
# HELP probe_failed_due_to_regex Indicates if probe failed due to regex
# TYPE probe_failed_due_to_regex gauge
probe_failed_due_to_regex 0
# HELP probe_http_content_length Length of http content response
# TYPE probe_http_content_length gauge
probe_http_content_length -1
# HELP probe_http_duration_seconds Duration of http request by phase, summed
    over all redirects
# TYPE probe_http_duration_seconds gauge
probe_http_duration_seconds{phase="connect"} 0.02226464
probe_http_duration_seconds{phase="processing"} 0.05238605
probe_http_duration_seconds{phase="resolve"} 0.001481084
probe_http_duration_seconds{phase="tls"} 0.043717698
probe_http_duration_seconds{phase="transfer"} 0.044905889
# HELP probe_http_last_modified_timestamp_seconds Returns the Last-Modified
    HTTP response header in unixtime
# TYPE probe_http_last_modified_timestamp_seconds gauge
probe_http_last_modified_timestamp_seconds 1.665390603e+09
# HELP probe_http_redirects The number of redirects
# TYPE probe_http_redirects gauge
probe_http_redirects 0
# HELP probe_http_ssl Indicates if SSL was used for the final redirect
# TYPE probe_http_ssl gauge
probe_http_ssl 1
# HELP probe_http_status_code Response HTTP status code
# TYPE probe_http_status_code gauge
probe_http_status_code 200
# HELP probe_http_uncompressed_body_length Length of uncompressed response body
# TYPE probe_http_uncompressed_body_length gauge
probe_http_uncompressed_body_length 75719
# HELP probe_http_version Returns the version of HTTP of the probe response
# TYPE probe_http_version gauge
probe_http_version 2
# HELP probe_ip_addr_hash Specifies the hash of IP address. It's useful to
    detect if the IP address changes.
# TYPE probe_ip_addr_hash gauge
probe_ip_addr_hash 1.793027101e+09
# HELP probe_ip_protocol Specifies whether probe ip protocol is IP4 or IP6
# TYPE probe_ip_protocol gauge
probe_ip_protocol 4
# HELP probe_ssl_earliest_cert_expiry Returns earliest SSL cert expiry
    in unixtime
# TYPE probe_ssl_earliest_cert_expiry gauge
probe_ssl_earliest_cert_expiry 1.697068799e+09
# HELP probe_ssl_last_chain_expiry_timestamp_seconds Returns last SSL chain
    expiry in timestamp seconds
# TYPE probe_ssl_last_chain_expiry_timestamp_seconds gauge
probe_ssl_last_chain_expiry_timestamp_seconds 1.697068799e+09
# HELP probe_ssl_last_chain_info Contains SSL leaf certificate information
# TYPE probe_ssl_last_chain_info gauge
probe_ssl_last_chain_info{fingerprint_sha256="849c8863b"} 1
# HELP probe_success Displays whether or not the probe was a success
# TYPE probe_success gauge
probe_success 1
# HELP probe_tls_version_info Contains the TLS version used
# TYPE probe_tls_version_info gauge
probe_tls_version_info{version="TLS 1.3"} 1
```

You can see `probe_success`, but also a number of other useful metrics
for debugging, such as the status code, HTTP version, and timings for
different phases of the request.

The `http` probe has many options to both affect how the request is
made, and whether the response is considered successful. You can specify
HTTP authentication, headers, POST body, and then in the response, check
that the status code, HTTP version, and body are acceptable.

For example, we may want to test that users of
[*http://www.oreilly.com*](http://www.oreilly.com){.bare} end up
redirected to an HTTPS website, with a 200 status code, and that the
word "Prometheus" is in the body. To do so you could create a module
like:

``` {data-type="programlisting"}
  http_200_ssl_prometheus:
    prober: http
    http:
      valid_status_codes: [200]
      fail_if_not_ssl: true
      fail_if_not_matches_regexp:
        - oreillymedia
```

Visiting
*[*http://localhost:9115/probe?module=http_200_ssl_prometheus&tar⁠get=https://oreilly.com*](http://localhost:9115/probe?module=http_200_ssl_prometheus&tar%E2%81%A0get=https://oreilly.com){.bare}*
in your browser, you should see that this works as `probe_success` is
`1`. You could also use the same request against
[*http://prometheus.io*](http://prometheus.io){.bare} if you visit
*http://localhost:9115/probe?module=http_200_ssl_prometheus&target=http://prometheus.io*
[in your]{.keep-together}
browser.^[13](#ch10.xhtml#idm45207095961168){#ch10.xhtml#idm45207095961168-marker
data-type="noteref"}^

::: {.warning data-type="warning"}
###### Warning

While the Blackbox Exporter will follow HTTP
redirects,^[14](#ch10.xhtml#idm45207095958992){#ch10.xhtml#idm45207095958992-marker
data-type="noteref"}^ not all features work perfectly across redirects.
:::

This example is a little contrived, but each module of the Blackbox
Exporter is a specific test that you can run against different targets
by providing different `target` URL parameters as you did here with
[*http://www.oreilly.com*](http://www.oreilly.com){.bare} and
[*http://prometheus.io*](http://prometheus.io){.bare}. For example, you
might check that each frontend application instance serving your website
is returning the right result. If different services need different
tests, then you can create modules for each of them. It is not possible
to override modules via URL parameters, as that would lead to the
Blackbox Exporter being an open
proxy^[15](#ch10.xhtml#idm45207095953472){#ch10.xhtml#idm45207095953472-marker
data-type="noteref"}^ and would confuse the division of responsibilities
between Prometheus and exporters.

The `http` probe is the most configurable of the Blackbox Exporter's
probes (the [documentation](https://oreil.ly/Jj0mf) lists all of the
options). While flexible, the Blackbox Exporter cannot handle all
possible use cases, as it is a relatively simple HTTP probe at the end
of the day. If you need something more sophisticated, you may need to
write your own exporter, or take advantage of existing exporters such as
the [WebDriver Exporter](https://oreil.ly/qTbHY), which simulates a
browser visiting a URL.[]{#ch10.xhtml#idm45207095950480
primary="Blackbox exporters" secondary="HTTP probes"
startref="ix_blckbxHTTP"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095949232 primary="HTTP"
secondary="probing with Blackbox exporter" startref="ix_HTTPprb"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="DNS" data-type="sect2"}
::: {#ch10.xhtml#idm45207095984480 .sect2}
## DNS

The `dns` probe is primarily for testing DNS servers;
[]{#ch10.xhtml#ix_blckbxDNS primary="Blackbox exporters"
secondary="DNS probes" data-type="indexterm"}[]{#ch10.xhtml#ix_DNSprb
primary="DNS" secondary="probing with Blackbox exporter"
data-type="indexterm"}for example, checking that all of your DNS
replicas are returning results.

If you wanted to test that your DNS servers were responding over
TCP,^[16](#ch10.xhtml#idm45207095942944){#ch10.xhtml#idm45207095942944-marker
data-type="noteref"}^ you could create a module in your *blackbox.yml*
like this:

``` {data-type="programlisting"}
  dns_tcp:
    prober: dns
    dns:
      transport_protocol: "tcp"
      query_name: "www.prometheus.io"
```

After restarting the Blackbox Exporter, you can visit
*http://localhost:9115/probe?module=dns_tcp&target=8.8.8.8* to check if
Google's Public DNS
service^[17](#ch10.xhtml#idm45207095939536){#ch10.xhtml#idm45207095939536-marker
data-type="noteref"}^ works via TCP. Note that the `target` URL
parameter is the DNS server that is talked to, and the `query_name` is
the DNS request sent to the DNS server. This is the same as if you ran
the command `dig -tcp @8.8.8.8 www.prometheus.io`.

For the `dns` prober, the `target` URL parameter is an IP address or
hostname, followed by a colon, and then the port number. You can also
provide just the IP address or hostname, in which case the standard DNS
port of 53 will be used.

Aside from testing DNS servers, you could also use a `dns` probe to
confirm that specific results are being returned by DNS resolution. But
usually you want to go further and communicate to the returned service
via HTTP, TCP, or ICMP, in which case one of those probes makes more
sense as you get the DNS check for free.

An example of using the `dns` probe to check for specific results would
be to check that your MX
records^[18](#ch10.xhtml#idm45207095934288){#ch10.xhtml#idm45207095934288-marker
data-type="noteref"}^ have not disappeared.

You could create a module in your *blackbox.yml* like this:

``` {data-type="programlisting"}
  dns_mx_present_rp_io:
    prober: dns
    dns:
      query_name: "prometheus.io"
      query_type: "MX"
      validate_answer_rrs:
        fail_if_not_matches_regexp:
          - ".+"
```

After restarting the Blackbox Exporter, you can visit
*http://localhost:9115/probe?module=dns_mx_present_rp_io&target=8.8.8.8*
to check that `prometheus.io` has MX records. Note that as the
`query_name` is specified per module, you will need a module for every
domain that you want to check. We are using 8.8.8.8 here, as Google's
Public DNS is a public DNS resolver, but you could also use a local
resolver.

The `dns` probe has more features intended to help check for aspects of
DNS responses, such as authority and additional records, which you can
find out more about in the [documentation](https://oreil.ly/3pY9E). For
a better understanding of DNS, we recommend RFCs
[1034](https://oreil.ly/y7Rra) and
[1035](https://oreil.ly/dtwcY),^[19](#ch10.xhtml#idm45207095926736){#ch10.xhtml#idm45207095926736-marker
data-type="noteref"}^ or a book such as *DNS and BIND* by Paul Albitz
and Cricket Liu (O'Reilly).[]{#ch10.xhtml#idm45207095925408
primary="Blackbox exporters" secondary="DNS probes"
startref="ix_blckbxDNS"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095924160 primary="DNS"
secondary="probing with Blackbox exporter" startref="ix_DNSprb"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Prometheus Configuration" data-type="sect2"}
::: {#ch10.xhtml#idm45207095947408 .sect2}
## Prometheus Configuration

As you have seen, the Blackbox Exporter takes a `module` and `target`
URL parameter on the */probe* endpoint. []{#ch10.xhtml#idm45207095919472
primary="exporters"
secondary="Prometheus configuration for Blackbox probe URL parameters"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095918416
primary="Blackbox exporters" secondary="Prometheus configuration"
data-type="indexterm"}Using the `params` and `metrics_path`, as
discussed in ["How to
Scrape"](#ch08.xhtml#how_to_scrape){data-type="xref"}, you can provide
these in a scrape config, but that would mean having a scrape config per
target, which would be unwieldy as you could not take advantage of
Prometheus's ability to do service discovery.

The good news is that you can take advantage of service discovery, as
the `__param_<name>` label can be used to provide URL parameters in
relabeling. []{#ch10.xhtml#idm45207095914736 primary="relabeling"
secondary="providing URL parameters for Blackbox exporters in"
data-type="indexterm"}In addition, the `instance` and `__address__`
labels are distinct, as discussed in ["job, instance, and
\_\_address\_\_"](#ch08.xhtml#job_instance_address){data-type="xref"},
so you can have Prometheus talk to the Blackbox Exporter while having an
`instance` label of your actual target.

[Example 10-10](#ch10.xhtml#prometheus_blackbox_static){data-type="xref"}
shows an example of this in practice.

::: {#ch10.xhtml#prometheus_blackbox_static data-type="example"}
##### [Example 10-10. ]{.label}*prometheus.yml* to check if several websites work

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: blackbox
   metrics_path: /probe
   params:
     module: [http_2xx]
   static_configs:
    - targets:
       - http://www.prometheus.io
       - http://www.robustperception.io
       - http://demo.robustperception.io
   relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: 127.0.0.1:9115
```
:::

To break it down:

``` {data-type="programlisting"}
 - job_name: 'blackbox'
   metrics_path: /probe
   params:
     module: [http_2xx]
```

A default `job` label, custom path, and one URL parameter are specified:

``` {data-type="programlisting"}
   static_configs:
    - targets:
      - https://www.prometheus.io
      - https://www.oreilly.com
      - https://demo.do.prometheus.io
```

There are three websites that you will be probing:

``` {data-type="programlisting"}
   relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: 127.0.0.1:9115
```

The `relabel_configs` is where the magic happens. First, the
`__address__` label becomes the `target` URL parameter and secondly also
the `instance` label. At this point, the `instance` label and `target`
URL parameter have the value you want, but the `__address__` is still a
URL rather than the Blackbox Exporter. The final relabeling action sets
the `__address__` to the host and port of the local Blackbox Exporter.

If you run Prometheus with this configuration and look at the Targets
status page, you will see something like
[Figure 10-2](#ch10.xhtml#blackbox_prometheus_status){data-type="xref"}.
The endpoint has the desired URL parameters, and the instance label is
the URL.

<figure>
<div id="ch10.xhtml#blackbox_prometheus_status" class="figure">
<img src="assets/pur2_1002.png" width="600" height="371"
alt="A blackbox Exporter status page with no recent probes." />
<h6><span class="label">Figure 10-2. </span>The Blackbox Exporter’s
status page</h6>
</div>
</figure>

::: {.warning data-type="warning"}
###### Warning

That the State is `UP` for the Blackbox Exporter does not mean that the
probe was successful, merely that the Blackbox Exporter was scraped
successfully.^[20](#ch10.xhtml#idm45207095843008){#ch10.xhtml#idm45207095843008-marker
data-type="noteref"}^ You need to check that `probe_success` is `1`.
:::

This approach is not limited to `static_configs`. You can use any other
service discovery mechanism (as discussed in
[Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}).
For example, building on
[Example 8-19](#ch08.xhtml#consul_prometheus_yml_9100){data-type="xref"},
which scraped the Node Exporter for all nodes registered in Consul,
[Example 10-11](#ch10.xhtml#blackbox_consul_prometheus_yml){data-type="xref"}
will check that SSH is responding for all nodes registered in Consul.

::: {#ch10.xhtml#blackbox_consul_prometheus_yml .pagebreak-before .less_space_example data-type="example"}
##### [Example 10-11. ]{.label}Checking SSH on all nodes registered in Consul

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: node
   metrics_path: /probe
   params:
     module: [ssh_banner]
   consul_sd_configs:
    - server: 'localhost:8500'
   relabel_configs:
    - source_labels: [__meta_consul_address]
      regex: '(.*)'
      replacement: '${1}:22'
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: 127.0.0.1:9115
```
:::

The power of this approach allows you to reuse service discovery for not
just scraping of */metrics*, but also to do blackbox monitoring of your
applications.

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch10.xhtml#idm45207095734496 .sidebar}
# Blackbox Timeouts

You may be wondering how to configure timeouts for your probes. The good
news is that the Blackbox prober determines the timeout automatically
based on the `scrape_timeout` in Prometheus.

Prometheus sends an HTTP header called
`X-Prometheus-Scrape-Timeout-Seconds` with every scrape. The Blackbox
Exporter uses this for its timeouts, less a
buffer.^[21](#ch10.xhtml#idm45207095683776){#ch10.xhtml#idm45207095683776-marker
data-type="noteref"}^ The end result is that the Blackbox Exporter will
usually return with some metrics that will be useful in debugging in the
event of the target being slow, rather than the scrape as a whole
failing.

You can reduce the timeout further using the `timeout` field in
*blackbox.yml*.
:::

```{=html}
</aside>
```
Now that you have an idea of the sorts of exporters you will run into,
you're ready to learn how to pull metrics from your existing monitoring
systems.[]{#ch10.xhtml#idm45207095680816 primary="exporters"
secondary="Blackbox" startref="ix_expBlck"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095679568
primary="Blackbox exporters" startref="ix_blckbx"
data-type="indexterm"}[]{#ch10.xhtml#idm45207095678624
primary="exporters" startref="ix_exptr" data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch10.xhtml#idm45207096443696-marker)^ There is also
[*https://oreil.ly/1phnz*](https://oreil.ly/1phnz){.bare} in this space.

^[2](#ch10.xhtml#idm45207096441584-marker)^ The L in the ELK stack.

^[3](#ch10.xhtml#idm45207096141264-marker)^ As distinct from cases where
it's not possible for political reasons.

^[4](#ch10.xhtml#idm45207096138544-marker)^ Simple Network Management
Protocol, a standard for (among other things) exposing metrics on
network devices. It can also sometimes be found on other hardware.

^[5](#ch10.xhtml#idm45207096080656-marker)^ Some pings can also work via
UDP or TCP instead, but those are relatively rare.

^[6](#ch10.xhtml#idm45207096029936-marker)^ Similar to Prometheus, you
can also send a `SIGHUP` to the Blackbox Exporter to have it reload its
[configuration]{.keep-together}.

^[7](#ch10.xhtml#idm45207096004432-marker)^ However, the debug
information for the most recent probes is available from the Blackbox
Exporter's status page.

^[8](#ch10.xhtml#idm45207095995360-marker)^ 443 is the standard port for
HTTPS.

^[9](#ch10.xhtml#idm45207095991168-marker)^ More exactly, the first
certificate that will expire in your certificate chain.

^[10](#ch10.xhtml#idm45207095989024-marker)^ At least for HTTP versions
prior to 2.0.

^[11](#ch10.xhtml#idm45207095979616-marker)^ You can even include URL
parameters, if they are appropriately encoded.

^[12](#ch10.xhtml#idm45207095977520-marker)^ The `http_2xx` module is
incidentally the default module name if you don't provide one as a URL
parameter.

^[13](#ch10.xhtml#idm45207095961168-marker)^ Presuming you have a
working IPv6 setup; if not, add **`preferred_ip_protocol: ip4`**.

^[14](#ch10.xhtml#idm45207095958992-marker)^ Unless `follow_redirects`
is set to `false`.

^[15](#ch10.xhtml#idm45207095953472-marker)^ Which would be unwise from
a security standpoint.

^[16](#ch10.xhtml#idm45207095942944-marker)^ While DNS usually uses UDP,
it can also use TCP in cases such as for large responses. Unfortunately,
many site operators are not aware of this and block TCP on port 53,
which is the DNS port.

^[17](#ch10.xhtml#idm45207095939536-marker)^ Which is offered on the IPs
8.8.8.8, 8.8.4.4, 2001:4860:4860::8888, and 2001:4860:4860::8844.

^[18](#ch10.xhtml#idm45207095934288-marker)^ Used for email, MX stands
for Mail eXchanger.

^[19](#ch10.xhtml#idm45207095926736-marker)^ We learned DNS from these
RFCs; they're a little outdated but still give a good sense of how DNS
operates.

^[20](#ch10.xhtml#idm45207095843008-marker)^ Indeed, in
[Figure 10-2](#ch10.xhtml#blackbox_prometheus_status){data-type="xref"}
the probe of
[*http://www.prometheus.io*](http://www.prometheus.io){.bare} is
failing, as our machine has a broken IPv6 setup.

^[21](#ch10.xhtml#idm45207095683776-marker)^ Specified by the
`--timeout-offset` command-line flag.
:::
:::
:::

[]{#ch11.xhtml}

::: {#ch11.xhtml#sbo-rt-content}
::: {#ch11.xhtml#other_monitoring_systems_chapter .chapter}
# [Chapter 11. ]{.label}Working with Other Monitoring Systems

In an ideal world all of your applications would be directly exposing
Prometheus metrics, but this is unlikely to be the world you
inhabit.[]{#ch11.xhtml#ix_monsys primary="monitoring systems  (other)"
data-type="indexterm"} You may have other monitoring systems already in
use, and doing a big switchover one day to Prometheus is not practical.

The good news is that among the hundreds of exporters for Prometheus
there are several that convert data from other monitoring systems into
the Prometheus format. While your ideal end goal would be to move
completely to Prometheus, exporters like the ones you'll learn about in
this chapter are very helpful when you are still
[transitioning]{.keep-together}.

::: {.section pdf-bookmark="Other Monitoring Systems" data-type="sect1"}
::: {#ch11.xhtml#idm45207095674128 .sect1}
# Other Monitoring Systems

Monitoring systems vary in how compatible they are with Prometheus; some
require notable effort, while others require close to
none.[]{#ch11.xhtml#ix_monsysabt primary="monitoring systems  (other)"
secondary="about" data-type="indexterm"}[]{#ch11.xhtml#idm45207095581104
primary="InfluxDB"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095580496
primary="exporters" secondary="other monitoring systems"
data-type="indexterm"} For example, InfluxDB has a data model fairly
similar to Prometheus, so you can have your application push the
InfluxDB line protocol to the [InfluxDB
Exporter](https://oreil.ly/NPmXE), which can then be scraped by
Prometheus.

Other systems like collectd do not[]{#ch11.xhtml#idm45207095578352
primary="collectd" data-type="indexterm"} have labels, but it is
possible to automatically convert the metrics it outputs into an OK
Prometheus metric with no additional configuration using the [Collectd
Exporter](https://oreil.ly/ErtjW). As of version 5.7, collectd even
includes this natively with the [Write Prometheus
plug-in](https://oreil.ly/mWhRg).

But not all monitoring systems have data models that can be
automatically converted into reasonable Prometheus metrics.
[]{#ch11.xhtml#idm45207095575760 primary="Graphite"
data-type="indexterm"}Historically, Graphite did not not support
key-value labels, and some configuration labels can be extracted from
the dotted strings it uses using the [Graphite
Exporter](https://oreil.ly/6ah0Q).^[1](#ch11.xhtml#idm45207095574112){#ch11.xhtml#idm45207095574112-marker
data-type="noteref"}^ StatsD has basically the same dotted-string model
as Graphite; StatsD uses events rather than metrics, so the [StatsD
Exporter](https://oreil.ly/ECuBA) aggregates the events into metrics,
and can also extract labels.[]{#ch11.xhtml#idm45207095572496
primary="StatsD" data-type="indexterm"}

In the Java/JVM space, JMX (Java Management eXtensions) is a standard
often used for exposing metrics, but how it is used varies quite a bit
from application to application.[]{#ch11.xhtml#idm45207095571408
primary="JMX (Java Management eXtensions)"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095570688 primary="Java"
secondary="JMX (Java Management eXtensions)" data-type="indexterm"} The
[JMX Exporter](https://oreil.ly/A549a) has OK defaults, but given the
lack of standardization of the mBean structure, the only sane way to
configure it is via regular expressions. The good news is that there are
a variety of example configurations provided, and that the JMX Exporter
is intended to run as a Java agent so you don't have to manage a
separate exporter process.

SNMP actually has a data model []{#ch11.xhtml#idm45207095568144
primary="SNMP" data-type="indexterm"}[]{#ch11.xhtml#idm45207095567440
primary="Management Information Base (MIBs)"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095566704
primary="MIBs (Management Information Base)" data-type="indexterm"}that
is quite close to that of Prometheus, and by using
MIBs,^[2](#ch11.xhtml#idm45207095565888){#ch11.xhtml#idm45207095565888-marker
data-type="noteref"}^ SNMP metrics can be automatically produced by the
[SNMP Exporter](https://oreil.ly/HW3wu). The bad news is twofold. First,
MIBs from vendors are often not freely available, so you need to acquire
the MIBs yourself and use the *generator* included with the SNMP
Exporter to convert the MIBs into a form the SNMP Exporter can
understand. Second, many vendors follow the letter of [the
SNMP]{.keep-together} specification but not the spirit, so additional
configuration and/or munging with PromQL is sometimes required.
[]{#ch11.xhtml#idm45207095562960 primary="SNMP-style exporters"
data-type="indexterm"}The SNMP Exporter is a Blackbox/SNMP-style
exporter, as was discussed in
["Blackbox"](#ch10.xhtml#blackbox){data-type="xref"}, so unlike almost
all other exporters, you typically run one per Prometheus rather than
one per application instance.

::: {.note data-type="note"}
###### Note

SNMP is a very chatty network protocol. It is advisable to have SNMP
Exporters as close as you can on the network to the network devices they
are monitoring to mitigate this. Furthermore, many SNMP devices can
speak the SNMP protocol but not return metrics in anything resembling a
reasonable time frame. You may need to be judicious in what metrics you
request and generous in your `scrape_interval`.
:::

There are also exporters you can use to extract metrics
[]{#ch11.xhtml#idm45207095559008 primary="exporters"
secondary="SaaS monitoring systems"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095558032
primary="software as a service (SaaS) monitoring systems"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095557264
primary="SaaS (software as a service ) monitoring systems"
data-type="indexterm"}from a variety of software as a service (SaaS)
monitoring systems, including the [CloudWatch
Exporter](https://oreil.ly/lQ9Fe), [New Relic
Exporter](https://oreil.ly/YWPcw), [Pingdom
Exporter](https://oreil.ly/UU4br), and [Stackdriver
Exporter](https://oreil.ly/JH2T9). One thing to watch with such
exporters is that there may be rate limits and financial costs for using
the APIs they access.

The [NRPE Exporter](https://oreil.ly/BBkqg) is an SNMP/Blackbox-style
exporter that allows you to run NRPE checks.
[]{#ch11.xhtml#idm45207095552464 primary="Blackbox exporters"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095551760
primary="SNMP-style exporters"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095551088
primary="Nagios Remote Program Execution (NRPE)"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095550448
primary="NRPE Exporter" data-type="indexterm"}NRPE stands for Nagios
Remote Program Execution, a way to run Nagios checks on remote machines.
While many existing checks in a Nagios-style monitoring setup can be
replaced by metrics from the Node and other exporters, you may have some
custom checks that are a bit harder to migrate. The NRPE Exporter gives
you a transition option here, allowing you to later convert these checks
to another solution such as the textfile collector, as discussed in
["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}.

Integration []{#ch11.xhtml#idm45207095548208
primary="monitoring systems  (other)"
secondary="Prometheus integration with"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095547248
primary="Dropwizard metrics"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095546544 primary="Java"
secondary="client library" tertiary="integration with Dropwizard"
data-type="indexterm"}with other monitoring systems isn't limited to
running separate exporters; there are also integrations with popular
instrumentation systems such as Dropwizard
metrics.^[3](#ch11.xhtml#idm45207095545040){#ch11.xhtml#idm45207095545040-marker
data-type="noteref"}^ The [Java client](https://oreil.ly/IdIHo) has an
integration that can pull metrics from Dropwizard metrics using its
reporting feature that will then appear alongside any direct
instrumentation you have on */metrics*.

::: {data-type="tip"}
###### Tip

Dropwizard can also expose its metrics via
JMX.[]{#ch11.xhtml#idm45207095542240
primary="JMX (Java Management eXtensions)"
secondary="Dropwizard exposing metrics via" data-type="indexterm"} If
possible (i.e., you control the codebase), you should prefer using the
Java client's Dropwizard integration over JMX, since going via JMX has
higher overhead and requires more
configuration.[]{#ch11.xhtml#idm45207095541056
primary="monitoring systems  (other)" secondary="about"
startref="ix_monsysabt" data-type="indexterm"}
:::
:::
:::

::: {.section pdf-bookmark="InfluxDB" data-type="sect1"}
::: {#ch11.xhtml#idm45207095673472 .sect1}
# InfluxDB

The InfluxDB Exporter accepts the InfluxDB line protocol that was added
in version 0.9.0 of InfluxDB.[]{#ch11.xhtml#idm45207095538016
primary="InfluxDB"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095537280
primary="monitoring systems  (other)" secondary="InfluxDB"
data-type="indexterm"} The protocol works over HTTP, so the same TCP
port can be used both to accept writes and serve */metrics*. To run the
InfluxDB Exporter, follow the steps in
[Example 11-1](#ch11.xhtml#influxdb_exporter_setup){data-type="xref"}.

::: {#ch11.xhtml#influxdb_exporter_setup data-type="example"}
##### [Example 11-1. ]{.label}Downloading and running the InfluxDB Exporter

``` {data-type="programlisting"}
hostname $ wget https://github.com/prometheus/influxdb_exporter/releases/download/
    v0.10.0/influxdb_exporter-0.10.0.linux-amd64.tar.gz
hostname $ tar -xzf influxdb_exporter-0.10.0.linux-amd64.tar.gz
hostname $ cd influxdb_exporter-0.10.0.linux-amd64/
hostname $ ./influxdb_exporter
msg="Starting influxdb_exporter" version="(version=0.10.0, branch=
    HEAD, revision=6ce7ff5e3f584eb9c2019be71ecb9e586ba3d83e)"
msg="Build context" context="(go=go1.18.3, user=root@de8ee7c667
    c4, date=20220708-19:34:59)"
```
:::

You can then direct your existing applications that speak the InfluxDB
line protocol to use the InfluxDB Exporter. To send a metric by hand
with labels, you can do:

``` {data-type="programlisting"}
curl -XPOST 'http://localhost:9122/write' --data-binary \
    'example_metric,foo=bar value=43 1517339868000000000'
```

If you then visit *http://localhost:9122/metrics* in your browser, among
the output you will see:

``` {data-type="programlisting"}
# HELP example_metric InfluxDB Metric
# TYPE example_metric untyped
example_metric{foo="bar"} 43
```

You may notice []{#ch11.xhtml#idm45207095527152 primary="timestamps"
secondary="InfluxDB and" data-type="indexterm"}that the timestamp that
you sent to the exporter is not exposed. There are very few valid use
cases for */metrics* to expose timestamps, as scrapes are meant to
synchronously gather metrics representing the application state at
scrape time. When working with other monitoring systems this is often
not the case, and using timestamps would be valid. At the time of
writing only the Java client library supports timestamps for custom
collectors. When metrics are exported without timestamps, Prometheus
will use the time at which the scrape happens. The InfluxDB Exporter
will garbage collect the point after a few minutes and stop exposing
it.[]{#ch11.xhtml#idm45207095525504 primary="scraping"
secondary="prometheus.yml to scrape InfluxDB Exporter"
data-type="indexterm"} These are the challenges you face when you
convert from push to pull. On the other hand, converting from pull to
push is quite simple, as shown in
[Example 4-13](#ch04.xhtml#python_graphite_bridge){data-type="xref"}.

You can scrape the InfluxDB Exporter like any other exporter, as shown
in
[[Example 11-2](#ch11.xhtml#prometheus_yml_influxdb_exporter){data-type="xref"}]{.keep-together}.

::: {#ch11.xhtml#prometheus_yml_influxdb_exporter data-type="example"}
##### [Example 11-2. ]{.label}*prometheus.yml* to scrape a local InfluxDB Exporter

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
scrape_configs:
 - job_name: application_name
   static_configs:
    - targets:
      - localhost:9122
```
:::
:::
:::

::: {.section pdf-bookmark="StatsD" data-type="sect1"}
::: {#ch11.xhtml#idm45207095539104 .sect1}
# StatsD

StatsD takes in events and aggregates them over time into metrics.
[]{#ch11.xhtml#ix_StD primary="StatsD"
data-type="indexterm"}[]{#ch11.xhtml#ix_monsysSD
primary="monitoring systems  (other)" secondary="StatsD"
data-type="indexterm"}You can think of sending an event to StatsD as
like calling `inc` on a counter or `observe` on a summary. The StatsD
Exporter does just that, converting your StatsD events into Prometheus
client library metrics and instrumentation calls.

You can run the StatsD Exporter by following the steps in
[Example 11-3](#ch11.xhtml#statsd_exporter_setup){data-type="xref"}.

::: {#ch11.xhtml#statsd_exporter_setup data-type="example"}
##### [Example 11-3. ]{.label}Downloading and running the StatsD Exporter

``` {data-type="programlisting"}
hostname $ wget https://github.com/prometheus/statsd_exporter/releases/download/
    v0.22.8/statsd_exporter-0.22.8.linux-amd64.tar.gz
hostname $ tar -xzf statsd_exporter-0.22.8.linux-amd64.tar.gz
hostname $ cd statsd_exporter-0.22.8.linux-amd64/
hostname $ ./statsd_exporter
msg="Starting StatsD -> Prometheus Exporter" version="(version=0.22.8, branch=
    HEAD, revision=aecad1a2faf31d4a6c27323a29ca8c7a23d88f6b)"
msg="Build context" context="(go=go1.18.6, user=root@56d5d8c6d
    3d1, date=20220913-14:49:05)"
msg="Accepting StatsD Traffic" udp=:9125 tcp=:9125 unixgram=
msg="Accepting Prometheus Requests" addr=:9102
```
:::

As StatsD uses a custom TCP and UDP protocol, you need different ports
for sending events than for scraping */metrics*.

You can send a gauge by hand
with:^[4](#ch11.xhtml#idm45207095489440){#ch11.xhtml#idm45207095489440-marker
data-type="noteref"}^

``` {data-type="programlisting"}
echo 'example_gauge:123|g' | nc localhost 9125
```

which will appear on *http://localhost:9102/metrics* as:

``` {data-type="programlisting"}
# HELP example_gauge Metric autogenerated by statsd_exporter.
# TYPE example_gauge gauge
example_gauge 123
```

You can also send counter increments and summary/histogram observations:

``` {data-type="programlisting"}
echo 'example_counter_total:1|c' | nc localhost 9125
echo 'example_latency_total:20|ms' | nc localhost 9125
```

The StatsD protocol isn't fully specified; many implementations only
support integer values. While the StatsD Exporter does not have this
limitation, note that many metrics will not be in the base units you are
used to with Prometheus.

You can also extract labels, as []{#ch11.xhtml#idm45207095439680
primary="dotted string notation"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095438976
primary="Graphite" secondary="dotted string notation"
data-type="indexterm"}StatsD is often used with the Graphite *dotted
string* notation, where position indicates meaning.
`app.http.requests.eu-west-1./foo` might, for example, mean what would
be `app_http_requests_total{region="eu-west-1",path="/foo"}` in
Prometheus. To be able to map from such a string, you need to provide a
mapping file in *mapping.yml*, such as:

``` {code-language="yaml" data-type="programlisting"}
mappings:
- match: app.http.requests.*.*
  name: app_http_requests_total
  labels:
    region: "${1}"
    path: "${2}"
```

and then run the StatsD Exporter using it:

``` {data-type="programlisting"}
./statsd_exporter -statsd.mapping-config mapping.yml
```

If you now send requests following that pattern to the StatsD Exporter,
they will be appropriately named and labeled:

``` {data-type="programlisting"}
echo 'app.http.requests.eu-west-1./foo:1|c' | nc localhost 9125
echo 'app.http.requests.eu-west-1./bar:1|c' | nc localhost 9125
```

If you visit *http://localhost:9102/metrics*, it will now contain:

``` {data-type="programlisting"}
# HELP app_http_requests_total Metric autogenerated by statsd_exporter.
# TYPE app_http_requests_total counter
app_http_requests_total{path="/bar",region="eu-west-1"} 1
app_http_requests_total{path="/foo",region="eu-west-1"} 1
```

The Graphite Exporter has a similar mechanism to convert dotted strings
into labels.

You may end up running the StatsD Exporter even after you have completed
your transition to Prometheus if you are using languages such as PHP and
Perl for web applications.[]{#ch11.xhtml#idm45207095397920 primary="PHP"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095397216 primary="Perl"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095396544
primary="multiprocess deployments" data-type="indexterm"} As mentioned
in ["Multiprocess with
Gunicorn"](#ch04.xhtml#multiprocess_gunicorn){data-type="xref"},
Prometheus presumes a multithreaded model with long-lived processes.
[]{#ch11.xhtml#idm45207095394944 primary="processes"
secondary="long-lived and multithreaded in Prometheus"
data-type="indexterm"}You typically use languages like PHP in a way that
is not only multiprocess, but also often with processes that only live
for a single HTTP request. While an approach such as the Python client
uses for multiprocess deployments is theoretically possible for typical
PHP deployments, you may find that the StatsD Exporter is more
practical. There is also the
[prom-aggregation-gateway](https://oreil.ly/qrGYk) in this space.

We would recommend []{#ch11.xhtml#idm45207095392960 primary="exporters"
secondary="other monitoring systems" data-type="indexterm"}for exporters
like the InfluxDB, Graphite, StatsD, and Collectd Exporters that convert
from push to pull that you have one exporter per application instance
and the same lifecycle as the application. You should start, stop, and
restart the exporter at the same time as you start, stop, and restart
the application instance. []{#ch11.xhtml#idm45207095391856
primary="labels"
secondary="changes in, other monitoring system exporters"
data-type="indexterm"}That way is easier to manage, avoids issues with
labels changing, and keeps the exporter from becoming a
bottleneck.^[5](#ch11.xhtml#idm45207095390688){#ch11.xhtml#idm45207095390688-marker
data-type="noteref"}^

While there are hundreds of exporters on offer, you may find yourself
needing to write or extend one yourself. The next chapter will show you
how to write exporters.[]{#ch11.xhtml#idm45207095389776
primary="monitoring systems  (other)" secondary="StatsD"
startref="ix_monsysSD"
data-type="indexterm"}[]{#ch11.xhtml#idm45207095388560 primary="StatsD"
startref="ix_StD" data-type="indexterm"}[]{#ch11.xhtml#idm45207095362016
primary="monitoring systems  (other)" startref="ix_monsys"
data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch11.xhtml#idm45207095574112-marker)^ Version 1.1.0 of Graphite
added tag supports that are ingested by the Graphite Exporter as labels.

^[2](#ch11.xhtml#idm45207095565888-marker)^ Management Information Base,
basically a schema for SNMP objects.

^[3](#ch11.xhtml#idm45207095545040-marker)^ Previously known as Yammer
metrics.

^[4](#ch11.xhtml#idm45207095489440-marker)^ `nc` is a handy networking
utility whose full name is *netcat*. You may need to install it if you
don't have it already.

^[5](#ch11.xhtml#idm45207095390688-marker)^ One of the reasons that
Prometheus exists is due to scaling issues that SoundCloud had with many
applications sending to one StatsD.
:::
:::
:::

[]{#ch12.xhtml}

::: {#ch12.xhtml#sbo-rt-content}
::: {#ch12.xhtml#writing_exporters_chapter .chapter}
# [Chapter 12. ]{.label}Writing Exporters

Sometimes you will not be able to either add direct instrumentation to
an application, nor find an existing exporter that covers it. This
leaves you with having to write an exporter
yourself.[]{#ch12.xhtml#ix_expwr primary="exporters" secondary="writing"
data-type="indexterm"} The good news is that exporters are relatively
easy to write. The hard part is figuring out what the metrics exposed by
applications mean. Units are often unknown, and documentation, if it
exists at all, can be vague. In this chapter you will learn how to write
exporters.[]{#ch12.xhtml#idm45207095358544 primary="writing exporters"
see="exporters" data-type="indexterm"}

::: {.section pdf-bookmark="Consul Telemetry" data-type="sect1"}
::: {#ch12.xhtml#idm45207095357472 .sect1}
# Consul Telemetry

We are going to write a small exporter for Consul to demonstrate the
process.[]{#ch12.xhtml#ix_expwrCT primary="exporters"
secondary="writing" tertiary="Consul Telemetry"
data-type="indexterm"}[]{#ch12.xhtml#ix_Conswrexp primary="Consul"
secondary="writing Consul Telemetry exporter" data-type="indexterm"} We
already saw Consul and the Consul Exporter in
["Consul"](#ch10.xhtml#consul_exporter){data-type="xref"}, so let's
create a simple exporter with metrics from the telemetry
API.^[1](#ch12.xhtml#idm45207095351824){#ch12.xhtml#idm45207095351824-marker
data-type="noteref"}^

While you can write exporters in any programming language, the majority
are written in Go, and that is the language we will use
here.[]{#ch12.xhtml#idm45207095350912 primary="Go"
secondary="writing exporters in" data-type="indexterm"} However, you
will find a small number of exporters written in Python, and an even
smaller number in Java.

If your Consul is not running, start it again following the instructions
in [Example 8-8](#ch08.xhtml#consul_dev_setup){data-type="xref"}. If you
visit *http://localhost:8500/v1/agent/metrics*, you will see the JSON
output that you will be working with, which is similar to
[Example 12-1](#ch12.xhtml#consul_metrics_output){data-type="xref"}.
Conveniently, Consul provides a Go library that you can use, so you
don't have to worry about parsing the JSON
yourself.[]{#ch12.xhtml#idm45207095347104 primary="metrics"
secondary="Consul"
data-type="indexterm"}[]{#ch12.xhtml#idm45207095346160 primary="JSON"
data-type="indexterm"}

::: {#ch12.xhtml#consul_metrics_output data-type="example"}
##### [Example 12-1. ]{.label}An abbreviated example output from a Consul agent's metrics output

``` {code-language="json" data-type="programlisting"}
{
  "Timestamp": "2018-01-31 14:42:10 +0000 UTC",
  "Gauges": [
    {
        "Name": "consul.autopilot.failure_tolerance",
        "Value": 0,
        "Labels": {}
    }
  ],
  "Points": [],
  "Counters": [
    {
        "Name": "consul.raft.apply",
        "Count": 1,
        "Sum": 2, "Min": 1, "Max": 1, "Mean": 1, "Stddev": 0,
        "Labels": {}
    }
  ],
  "Samples": [
    {
        "Name": "consul.fsm.coordinate.batch-update",
        "Count": 1,
        "Sum": 0.13156799972057343,
        "Min": 0.13156799972057343, "Max": 0.13156799972057343,
        "Mean": 0.13156799972057343, "Stddev": 0,
        "Labels": {}
    }
  ]
}
```
:::

You are in luck that Consul has split out the
[]{#ch12.xhtml#idm45207095339728 primary="counters" secondary="Consul"
data-type="indexterm"}[]{#ch12.xhtml#idm45207095338464 primary="gauges"
secondary="Consul" data-type="indexterm"}counters and gauges for
you.^[2](#ch12.xhtml#idm45207095146304){#ch12.xhtml#idm45207095146304-marker
data-type="noteref"}^ The [`Samples`]{.keep-together} also look like you
can use the `Count` and `Sum` in a summary metric. Looking at all the
`Samples` again, we have a suspicion that they are tracking
latency.[]{#ch12.xhtml#idm45207095142368 primary="timers"
data-type="indexterm"} Digging through [the
documentation](https://oreil.ly/6RY1Y) confirms that they are *timers*,
which means a Prometheus summary (see ["The
Summary"](#ch03.xhtml#summary_metric){data-type="xref"}). The timers are
also all in milliseconds, so we can convert them to
seconds.^[3](#ch12.xhtml#idm45207095139536){#ch12.xhtml#idm45207095139536-marker
data-type="noteref"}^ While the JSON has a field for labels, none are
used, so you can ignore that. Aside from that, the only other thing you
need to do is ensure any invalid characters in the metric names are
sanitized.

You now know the logic you need to apply to the metrics that Consul
exposes, so you can write your exporter as in
[Example 12-2](#ch12.xhtml#consul_metrics_go){data-type="xref"}.

::: {#ch12.xhtml#consul_metrics_go data-type="example"}
##### [Example 12-2. ]{.label}*consul_metrics.go*, an exporter for Consul metrics written in Go

``` {code-language="go" data-type="programlisting"}
package main

import (
	"log"
	"net/http"
	"regexp"

	"github.com/hashicorp/consul/api"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
)

var (
	up = prometheus.NewDesc(
		"consul_up",
		"Was talking to Consul successful.",
		nil, nil,
	)
	invalidChars = regexp.MustCompile("[^a-zA-Z0-9:_]")
)

type ConsulCollector struct {
}

// Implements prometheus.Collector.
func (c ConsulCollector) Describe(ch chan<- *prometheus.Desc) {
	ch <- up
}

// Implements prometheus.Collector.
func (c ConsulCollector) Collect(ch chan<- prometheus.Metric) {
	consul, err := api.NewClient(api.DefaultConfig())
	if err != nil {
		ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 0)
		return
	}

	metrics, err := consul.Agent().Metrics()
	if err != nil {
		ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 0)
		return
	}
	ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 1)

	for _, g := range metrics.Gauges {
		name := invalidChars.ReplaceAllLiteralString(g.Name, "_")
		desc := prometheus.NewDesc(name, "Consul metric "+g.Name, nil, 
		    g.Labels)
		ch <- prometheus.MustNewConstMetric(
			desc, prometheus.GaugeValue, float64(g.Value))
	}

	for _, c := range metrics.Counters {
		name := invalidChars.ReplaceAllLiteralString(c.Name, "_")
		desc := prometheus.NewDesc(name+"_total", "Consul metric "+c.Name, 
		    nil, c.Labels)
		ch <- prometheus.MustNewConstMetric(
			desc, prometheus.CounterValue, float64(c.Count))
	}

	for _, s := range metrics.Samples {
		// All samples are times in milliseconds, we convert them to 
		// seconds below.
		name := invalidChars.ReplaceAllLiteralString(s.Name, "_") + 
		    "_seconds"
		countDesc := prometheus.NewDesc(
			name+"_count", "Consul metric "+s.Name, nil, s.Labels)
		ch <- prometheus.MustNewConstMetric(
			countDesc, prometheus.CounterValue, float64(s.Count))
		sumDesc := prometheus.NewDesc(
			name+"_sum", "Consul metric "+s.Name, nil, s.Labels)
		ch <- prometheus.MustNewConstMetric(
			sumDesc, prometheus.CounterValue, s.Sum/1000)
	}
}

func main() {
	c := ConsulCollector{}
	prometheus.MustRegister(c)
	http.Handle("/metrics", promhttp.Handler())
	log.Fatal(http.ListenAndServe(":8000", nil))
}
```
:::

If you have a working []{#ch12.xhtml#idm45207094822496 primary="Go"
secondary="running Consul Telemetry exporter" data-type="indexterm"}Go
development environment, you can run the exporter with:

``` {data-type="programlisting"}
go get -d -u github.com/hashicorp/consul/api
go get -d -u github.com/prometheus/client_golang/prometheus
go run consul_metrics.go
```

If you visit *http://localhost:8000/metrics*, you will see metrics like:

``` {data-type="programlisting"}
# HELP consul_autopilot_failure_tolerance Consul metric
    consul.autopilot.failure_tolerance
# TYPE consul_autopilot_failure_tolerance gauge
consul_autopilot_failure_tolerance 0
# HELP consul_raft_apply_total Consul metric consul.raft.apply
# TYPE consul_raft_apply_total counter
consul_raft_apply_total 1
# HELP consul_fsm_coordinate_batch_update_seconds_count Consul metric
    consul.fsm.coordinate.batch-update
# TYPE consul_fsm_coordinate_batch_update_seconds_count counter
consul_fsm_coordinate_batch_update_seconds_count 1
# HELP consul_fsm_coordinate_batch_update_seconds_sum Consul metric
    consul.fsm.coordinate.batch-update
# TYPE consul_fsm_coordinate_batch_update_seconds_sum counter
consul_fsm_coordinate_batch_update_seconds_sum 1.3156799972057343e-01
```

That's all well and good, but how does the code work? In the next
section we will show you how.[]{#ch12.xhtml#idm45207094818880
primary="Consul" secondary="writing Consul Telemetry exporter"
startref="ix_Conswrexp"
data-type="indexterm"}[]{#ch12.xhtml#idm45207094817664
primary="exporters" secondary="writing" startref="ix_expwrCT"
tertiary="Consul Telemetry" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Custom Collectors" data-type="sect1"}
::: {#ch12.xhtml#custom_collectors .sect1}
# Custom Collectors

With direct instrumentation the client library takes in instrumentation
events and tracks the values of the metrics over
time.[]{#ch12.xhtml#idm45207094813616 primary="client libraries"
data-type="indexterm"}[]{#ch12.xhtml#ix_expwrCC primary="exporters"
secondary="writing" tertiary="custom collectors"
data-type="indexterm"}[]{#ch12.xhtml#ix_collcus primary="collectors"
secondary="custom" data-type="indexterm"} Client libraries provide the
counter, gauge, summary, and histogram metrics for this, which are all
examples of *collectors*. At scrape time each collector in a registry is
*collected*, which is to say, asked for its metrics. These metrics will
then be returned by the scrape of */metrics*. Counters and the other
three standard metric types only ever return one metric family.

If rather than using direct instrumentation you want to provide from
some other source, you use a *custom collector*, which is any collector
that is not one of the standard four. Custom collectors can return any
number of metric families. Collection happens on every single scrape of
a */metrics* page, where each collection is a consistent snapshot of the
metrics from a collector.

In Go your collectors must implement the `prometheus.Collector`
interface. That is to say the collectors must be objects with `Describe`
and `Collect` methods with a specific
signature.[]{#ch12.xhtml#idm45207094279424
primary="prometheus.Collector interface"
data-type="indexterm"}[]{#ch12.xhtml#idm45207094278656 primary="Go"
secondary="collectors written in" data-type="indexterm"}

The `Describe` method returns a description of the metrics it will
produce, in particular the metric name, label names, and help string.
[]{#ch12.xhtml#idm45207094276736 primary="Describe method"
data-type="indexterm"}The `Describe` method is called at registration
time, and is used to avoid duplicate metric registration.

There are two types of metrics an exporter can have: ones where it knows
the names and labels in advance, and ones where they are only determined
at scrape time. []{#ch12.xhtml#idm45207094274736 primary="Desc type"
data-type="indexterm"}[]{#ch12.xhtml#idm45207094274032 primary="NewDesc"
data-type="indexterm"}In this example, `consul_up` is known in advance
so you can create its `Desc` once with `NewDesc` and provide it via
`Describe`. All the other metrics are generated dynamically at scrape
time, so cannot be included:

``` {code-language="go" data-type="programlisting"}
var (
  up = prometheus.NewDesc(
    "consul_up",
    "Was talking to Consul successful.",
    nil, nil,
  )
)
// Implements prometheus.Collector.
func (c ConsulCollector) Describe(ch chan<- *prometheus.Desc) {
  ch <- up
}
```

::: {data-type="tip"}
###### Tip

The Go client requires that at least one `Desc` is provided by
`Describe`. If all your metrics are dynamic, you can provide a dummy
`Desc` to work around this.
:::

At the core of a custom collector is the `Collect` method.
[]{#ch12.xhtml#idm45207094265680 primary="Collect method"
data-type="indexterm"}In this method you fetch all the data you need
from the application instance you are working with, munge it as needed,
and then send the metrics back to the client library. Here you need to
connect to Consul and then fetch its metrics.
[]{#ch12.xhtml#idm45207094264816 primary="consul_up"
data-type="indexterm"}If an error occurs, `consul_up` is returned as
`0`; otherwise, once we know that the collection is going to be
successful, it is returned as `1`. Only returning a metric sometimes is
difficult^[4](#ch12.xhtml#idm45207094262624){#ch12.xhtml#idm45207094262624-marker
data-type="noteref"}^ to deal with in PromQL; having `consul_up` allows
you to alert on issues talking to Consul so you'll know that something
is awry.[]{#ch12.xhtml#idm45207094260576
primary="prometheus.MustNewConstMetric"
data-type="indexterm"}[]{#ch12.xhtml#idm45207094259904
primary="MustNewConstMetric function" data-type="indexterm"}

To return `consul_up`, `prometheus.MustNewConstMetric` is used to
provide a sample for just this scrape. It takes its `Desc`, type, and
value:

``` {code-language="go" data-type="programlisting"}
// Implements prometheus.Collector.
func (c ConsulCollector) Collect(ch chan<- prometheus.Metric) {
  consul, err := api.NewClient(api.DefaultConfig())
  if err != nil {
    ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 0)
    return
  }

  metrics, err := consul.Agent().Metrics()
  if err != nil {
    ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 0)
    return
  }
  ch <- prometheus.MustNewConstMetric(up, prometheus.GaugeValue, 1)
```

There are three possible values: `GaugeValue`, `CounterValue`, and
`UntypedValue`. Gauge and Counter you already know, and Untyped is for
cases where you are not sure whether a metric is a counter or a
gauge.[]{#ch12.xhtml#idm45207094065600 primary="UntypedValue"
data-type="indexterm"}[]{#ch12.xhtml#idm45207094064896
primary="CounterValue"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093964688
primary="GaugeValue" data-type="indexterm"} This is not possible with
direct instrumentation, but it is not unusual for the type of metrics
from other monitoring and instrumentation systems to be unclear and
impractical to determine.

Now that you have the metrics from Consul, you can process the
gauges.[]{#ch12.xhtml#idm45207093963632 primary="gauges"
secondary="processing in custom collector" data-type="indexterm"}
Invalid characters in the metric name, such as dots and hyphens, are
converted to underscores. A `Desc` is created on the fly, and
immediately used in a `MustNewConstMetric`:

``` {code-language="go" data-type="programlisting"}
  for _, g := range metrics.Gauges {
    name := invalidChars.ReplaceAllLiteralString(g.Name, "_")
    desc := prometheus.NewDesc(name, "Consul metric "+g.Name, nil, g.Labels)
    ch <- prometheus.MustNewConstMetric(
        desc, prometheus.GaugeValue, float64(g.Value))
  }
```

::: {.note data-type="note"}
###### Note

We pass `g.Labels` as the last parameter to `prometheus.NewDesc`. It is
a set of labels set by Consul, such as a `datacenter` label.
[]{#ch12.xhtml#idm45207093828064 primary="labels"
secondary="set by Consul" data-type="indexterm"}We have to pass them
because some of the gauges have a cardinality greater than one, and
without those labels, the */metrics* page would error out.
:::

Processing of []{#ch12.xhtml#idm45207093825968 primary="counters"
secondary="processing in custom collector"
data-type="indexterm"}counters is similar, except that a `_total` suffix
is added to the metric name:

``` {code-language="go" data-type="programlisting"}
  for _, c := range metrics.Counters {
    name := invalidChars.ReplaceAllLiteralString(c.Name, "_")
    desc := prometheus.NewDesc(name+"_total", "Consul metric "+c.Name, nil,
        c.Labels)
    ch <- prometheus.MustNewConstMetric(
        desc, prometheus.CounterValue, float64(s.Count))
  }
```

The contents of `metrics.Samples` are more complicated.
[]{#ch12.xhtml#idm45207093773200 primary="metrics.Samples"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093696608 primary="summary"
secondary="metrics.Sample" data-type="indexterm"}While the samples are a
Prometheus summary, the Go client does not currently support those for
`MustNewConstMetric`. []{#ch12.xhtml#idm45207093695040
primary="MustNewConstMetric function" data-type="indexterm"}Instead, you
can emulate it using two counters. `_seconds` is appended to the metric
name, and the sum is divided by one thousand to convert from
milliseconds to seconds:

``` {code-language="go" data-type="programlisting"}
  for _, s := range metrics.Samples {
    // All samples are times in milliseconds, we convert them to seconds below.
    name := invalidChars.ReplaceAllLiteralString(s.Name, "_") + "_seconds"
    countDesc := prometheus.NewDesc(
        name+"_count", "Consul metric "+s.Name, nil, s.Labels)
    ch <- prometheus.MustNewConstMetric(
        countDesc, prometheus.CounterValue, float64(s.Count))
    sumDesc := prometheus.NewDesc(
        name+"_sum", "Consul metric "+s.Name, nil, s.Labels)
    ch <- prometheus.MustNewConstMetric(
        sumDesc, prometheus.CounterValue, s.Sum/1000)
  }
```

::: {.warning data-type="warning"}
###### Warning

`s.Sum` here is a float64, but you must be careful when doing division
with integers to ensure you don't unnecessarily lose
precision.[]{#ch12.xhtml#idm45207093454544 primary="float64"
data-type="indexterm"} If `sum` were an integer, `float64(sum)/1000`
would convert to floating point first and then divide, which is what you
want. On the other hand, `float64(sum/1000)` will first divide the
integer value by one thousand, losing three digits of precision.
:::

Finally, the custom collector object is instantiated and
[]{#ch12.xhtml#idm45207093544832 primary="registry"
secondary="custom collector registered with default registry"
data-type="indexterm"}registered with the default registry, in the same
way you would one of the direct instrumentation metrics:

``` {code-language="go" data-type="programlisting"}
  c := ConsulCollector{}
  prometheus.MustRegister(c)
```

Exposition is performed in the usual way, which
[]{#ch12.xhtml#idm45207093538112 primary="exposition"
secondary="custom collector" data-type="indexterm"}you already saw in
["Go"](#ch04.xhtml#go_exposition){data-type="xref"}:

``` {code-language="go" data-type="programlisting"}
  http.Handle("/metrics", promhttp.Handler())
  log.Fatal(http.ListenAndServe(":8000", nil))
```

This is, of course, a simplified example. In reality you would have some
way to configure the Consul server to talk to, such as a command-line
flag, rather than depending on the client's default. You would also
reuse the client between scrapes, and allow the various authentication
options of the client to be specified.

::: {.note data-type="note"}
###### Note

The `min`, `max`, `mean`, and `stddev` were discarded from the original
output as they are not very useful. You can calculate a mean using the
sum and count. `min`, `max`, and `stddev`, on the other hand, cannot be
aggregated and you don't know over what time period they were measured.
:::

As the default registry is being used, `go_` and `process_` metrics are
included in the result. These provide you with information about the
performance of the exporter itself, and are useful to detect issues such
as file descriptor leaks using the [`process_open_fds`]{.keep-together}.
This saves you from having to scrape the exporter separately for these
metrics.

The only time you might not use the default registry for an exporter
[]{#ch12.xhtml#idm45207093386992 primary="exporters"
secondary="default registry and"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093386016
primary="Blackbox exporters" secondary="default registry and"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093385072
primary="SNMP-style exporters" secondary="default registry and"
data-type="indexterm"}is when writing a Blackbox/SNMP-style exporter,
where some interpretation of URL parameters needs to be performed as
collectors have no access to URL parameters for a scrape. In that case,
you would also scrape the */metrics* of the exporter in order to monitor
the exporter itself.

For comparison, the equivalent exporter written using Python 3 is shown
in [Example 12-3](#ch12.xhtml#consul_metrics_python){data-type="xref"}.
This is largely the same as the one written in Go; the only notable
difference is that a `SummaryMetricFamily` is available to represent a
summary, instead of emulating it with two separate counters.
[]{#ch12.xhtml#idm45207093381760 primary="exporters"
secondary="Consul metrics exporter written in Python 3"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093380816 primary="Python"
secondary="Consul metrics exporter written in" data-type="indexterm"}The
Python client does not have as many sanity checks as the Go client, so
you need to be a little more careful with it.

::: {#ch12.xhtml#consul_metrics_python data-type="example"}
##### [Example 12-3. ]{.label}*consul_metrics.py*, an exporter for Consul metrics written in Python 3

``` {code-language="python" data-type="programlisting"}
import json
import re
import time
from urllib.request import urlopen

from prometheus_client.core import GaugeMetricFamily, CounterMetricFamily
from prometheus_client.core import SummaryMetricFamily, REGISTRY
from prometheus_client import start_http_server


def sanitize_name(s):
    return re.sub(r"[^a-zA-Z0-9:_]", "_", s)

class ConsulCollector(object):
  def collect(self):
    out = urlopen("http://localhost:8500/v1/agent/metrics").read()
    metrics = json.loads(out.decode("utf-8"))

    for g in metrics["Gauges"]:
      yield GaugeMetricFamily(sanitize_name(g["Name"]),
          "Consul metric " + g["Name"], g["Value"])

    for c in metrics["Counters"]:
      yield CounterMetricFamily(sanitize_name(c["Name"]) + "_total",
          "Consul metric " + c["Name"], c["Count"])

    for s in metrics["Samples"]:
      yield SummaryMetricFamily(sanitize_name(s["Name"]) + "_seconds",
          "Consul metric " + s["Name"],
          count_value=c["Count"], sum_value=s["Sum"] / 1000)

if __name__ == '__main__':
  REGISTRY.register(ConsulCollector())
  start_http_server(8000)
  while True:
    time.sleep(1)
```
:::

::: {.section pdf-bookmark="Labels" data-type="sect2"}
::: {#ch12.xhtml#idm45207093210720 .sect2}
## Labels

In the preceding example you only saw metrics without
labels.[]{#ch12.xhtml#idm45207093209344 primary="collectors"
secondary="custom" tertiary="labels for metrics"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093107504 primary="labels"
secondary="for custom collector metrics" secondary-sortas="custom"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093106320
primary="Desc type"
data-type="indexterm"}[]{#ch12.xhtml#idm45207093105648
primary="MustNewConstMetric function"
secondary="specifying label values in" data-type="indexterm"} To provide
labels you need to specify the label names in `Desc` and then the values
in `MustNewConstMetric`.

To expose a metric with the time series
`example_gauge{foo="bar", baz="small"}` and
`example_gauge{foo="quu", baz="far"}`, you could do, with the Go
Prometheus client library:

``` {code-language="go" data-type="programlisting"}
func (c MyCollector) Collect(ch chan<- prometheus.Metric) {
  desc := prometheus.NewDesc(
    "example_gauge",
    "A help string.",
    []string{"foo", "baz"}, nil,
  )
  ch <- prometheus.MustNewConstMetric(
    desc, prometheus.GaugeValue, 1, "bar", "small")
  ch <- prometheus.MustNewConstMetric(
    desc, prometheus.GaugeValue, 2, "quu", "far")
}
```

First, you can provide each time series individually. The registry will
take care of combining all the time series belonging to the same metric
family in the */metrics* output.

::: {.warning data-type="warning"}
###### Warning

The help strings of all metrics with the same name must be identical.
Providing differing `Desc`s will cause the scrape to fail.
:::

The Python client works a little differently; you assemble the metric
family and then return it. While that may sound like more effort, it
usually works out to be the same level of effort in practice:

``` {code-language="python" data-type="programlisting"}
class MyCollector(object):
  def collect(self):
    mf = GaugeMetricFamily("example_gauge", "A help string.",
        labels=["foo", "baz"])
    mf.add_metric(["bar", "small"], 1)
    mf.add_metric(["quu", "far"], 2)
    yield mf
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Guidelines" data-type="sect1"}
::: {#ch12.xhtml#idm45207092893760 .sect1}
# Guidelines

While direct instrumentation tends to be reasonably simple, writing
exporters tends to be murky and involve engineering
trade-offs.[]{#ch12.xhtml#idm45207092874592 primary="exporters"
secondary="writing" startref="ix_expwrCC" tertiary="custom collectors"
data-type="indexterm"}[]{#ch12.xhtml#idm45207092873072
primary="collectors" secondary="custom" startref="ix_collcus"
data-type="indexterm"}[]{#ch12.xhtml#idm45207092871856
primary="exporters" secondary="writing" tertiary="guidelines for"
data-type="indexterm"} Do you want to spend a lot of ongoing effort to
produce perfect metrics, or do something that's good enough and requires
no maintenance? Writing exporters is more of an art than a science.

You should try to follow the metric naming practices, in particular,
avoiding the `_count`, `_sum`, `_total`, `_bucket`, and `_info` suffixes
unless the time series is part of a metric that is meant to contain such
a time series.

It is often not possible or practical to determine whether a bunch of
metrics are gauges, counters, or a mix of the two. In cases where there
is a mix you should mark them as *untyped* rather than using gauge or
counter, which would be incorrect. If a metric is a counter, don't
forget to add the `_total` suffix.

Where practical you should try to provide units for your metrics, and at
the very least try to ensure that the units are in the metric name.
Having to determine what the units are from metrics, as in
[Example 12-1](#ch12.xhtml#consul_metrics_output){data-type="xref"}, is
not fun for anyone, so you should try to remove this burden from your
exporter users. Seconds and bytes are always preferred.

In terms of []{#ch12.xhtml#idm45207092829360 primary="labels"
secondary="use in exporters, gotchas" data-type="indexterm"}using labels
in exporters, there are a few gotchas to look out for. As with direct
instrumentation, cardinality is also a concern for exporters for the
same reasons that were discussed in
["Cardinality"](#ch05.xhtml#cardinality_section){data-type="xref"}.
Metrics with high churn in their labels should be avoided.

Labels should create a partition across a metric, and if you take a sum
or average across a metric it should be meaningful, as discussed in
["When to Use
Labels"](#ch05.xhtml#when_to_use_labels){data-type="xref"}. In
particular, you should look out for any time series that are just totals
of all the other values in a metric, and remove them. If you are ever
unsure as to whether a label makes sense when writing an exporter, then
it is safest not to use one, though keep in mind the discussion in
["Table Exception"](#ch05.xhtml#table_exception){data-type="xref"}. As
with direct instrumentation, you should not apply a label such as
`env="prod"` to all metrics coming from your exporter, as that is what
target labels are for, as discussed in ["Target
Labels"](#ch08.xhtml#target_labels){data-type="xref"}.

It is best to expose raw metrics to Prometheus, rather than doing
calculations on the application side. For example, there is no need to
expose a 5-minute rate when you have a counter, as you can use the
`rate` function to calculate a rate over any period you like. Similarly
with ratios, drop them in favor of the numerator and denominator. If you
have a percentage without its constituent numerator and denominator, at
the least convert it to a
ratio.^[5](#ch12.xhtml#idm45207092823472){#ch12.xhtml#idm45207092823472-marker
data-type="noteref"}^

Beyond multiplication and division to standardize units, you should
avoid math in exporters, as processing raw data in PromQL is preferred.
Race conditions between metrics instrumentation events can lead to
artifacts, particularly when you subtract one metric from another.
Addition of metrics for the purposes of reducing cardinality can be OK,
but if they're counters, make sure there will not be spurious resets due
to some of them disappearing.

Some metrics are not particularly useful given how Prometheus is
intended to be used. Many applications expose metrics such as machine
RAM, CPU, and disk. You should not expose machine-level metrics in your
exporter, as that is the responsibility of the Node
Exporter.^[6](#ch12.xhtml#idm45207092822000){#ch12.xhtml#idm45207092822000-marker
data-type="noteref"}^ Minimums, maximums, and standard deviations cannot
be sanely aggregated so should also be dropped.

You should plan on running one exporter per application
instance,^[7](#ch12.xhtml#idm45207092820816){#ch12.xhtml#idm45207092820816-marker
data-type="noteref"}^ and fetch metrics synchronously for each scrape
without any caching. This keeps the responsibilities of service
discovery and scrape scheduling with Prometheus. Note that you should be
aware that concurrent scrapes can
happen.^[8](#ch12.xhtml#idm45207092820128){#ch12.xhtml#idm45207092820128-marker
data-type="noteref"}^

Just as[]{#ch12.xhtml#idm45207092819216 primary="scraping"
secondary="in custom exporters" secondary-sortas="custom"
data-type="indexterm"} Prometheus adds a `scrape_duration_seconds`
metric when performing a scrape, you may also add a
`myexporter_scrape_duration_seconds` metric for how long it takes your
exporter to pull the data from its application. This helps in
performance debugging, as you can see if it's the application or your
exporter that is getting slow. Additional metrics such as the number of
metrics processed can also be helpful.

It can make sense for you to add direct instrumentation to exporters, in
addition to the custom collectors that provide their core
functionality.[]{#ch12.xhtml#idm45207092816592
primary="Cloudwatch Exporter" data-type="indexterm"} For example, the
CloudWatch Exporter has a `cloudwatch_requests_total` counter tracking
the number of API calls it makes, as each API call costs money. But this
is usually only something that you will see with Blackbox/SNMP-style
exporters.

Now that you know how to get metrics out of both your applications and
third-party code, in the next chapter we will start covering PromQL,
which allows you to work with these
metrics.[]{#ch12.xhtml#idm45207092814992 primary="exporters"
secondary="writing" startref="ix_expwr" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch12.xhtml#idm45207095351824-marker)^ These metrics are also
exported natively by Consul. This example predates these metrics being
natively exposed by Consul.

^[2](#ch12.xhtml#idm45207095146304-marker)^ Just because something is
called a counter does not mean it is a counter. For example, Dropwizard
[]{#ch12.xhtml#idm45207095145664 primary="Dropwizard metrics"
secondary="counters" data-type="indexterm"}has counters that can go
down, so depending on how the counter is used in practice, it may be a
counter, gauge, or untyped in Prometheus terms.

^[3](#ch12.xhtml#idm45207095139536-marker)^ If only some of the
`Samples` were timers, you would have to choose between exposing them as
is or maintaining a list of which metrics are latencies and which
weren't.

^[4](#ch12.xhtml#idm45207094262624-marker)^ See ["or
operator"](#ch15.xhtml#or){data-type="xref"}.

^[5](#ch12.xhtml#idm45207092823472-marker)^ And check that it is
actually a ratio/percentage; it's not unknown for metrics to confuse the
two.

^[6](#ch12.xhtml#idm45207092822000-marker)^ Or Windows Exporter for
Windows users.

^[7](#ch12.xhtml#idm45207092820816-marker)^ Unless writing a
Blackbox/SNMP-style exporter, which is rare.

^[8](#ch12.xhtml#idm45207092820128-marker)^ This can happen when your
exporters are scraped by multiple servers.
:::
:::
:::

[]{#part04.xhtml}

::: {#part04.xhtml#sbo-rt-content}
::: {#part04.xhtml#part4 .part pdf-bookmark="Part IV. PromQL" data-type="part"}
# [Part IV. ]{.label}PromQL

The Prometheus Query Language offers you the ability to do all sorts of
aggregations, analysis, and arithmetic, allowing you to better
understand the performance of your systems from your metrics.

In this part you will be reusing the Prometheus and Node Exporter setup
you created in
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"}, and
using the expression browser to execute queries.

[Chapter 13](#ch13.xhtml#promql_introduction_chapter){data-type="xref"}
covers the basics of PromQL, and how you can use the HTTP API to
evaluate expressions.

[Chapter 14](#ch14.xhtml#promql_aggregation_chapter){data-type="xref"}
looks in depth into how aggregation works.

[Chapter 15](#ch15.xhtml#promql_operators_chapter){data-type="xref"}
covers operators such as addition and comparisons, and how you can join
different metrics.

[Chapter 16](#ch16.xhtml#promql_functions_chapter){data-type="xref"}
goes into the wide variety of functions that PromQL offers you, from
knowing the time of day to predicting when your hard disk will fill up.

[Chapter 17](#ch17.xhtml#promql_rules_chapter){data-type="xref"} covers
the recording rule feature of Prometheus, which allows you to precompute
metrics for faster and more sophisticated querying with PromQL.
:::
:::

[]{#ch13.xhtml}

::: {#ch13.xhtml#sbo-rt-content}
::: {#ch13.xhtml#promql_introduction_chapter .chapter}
# [Chapter 13. ]{.label}Introduction to PromQL

PromQL is []{#ch13.xhtml#idm45207092803232 primary="PromQL"
secondary="about" data-type="indexterm"}the Prometheus Query Language.
While it ends in *QL*, you will find that it is not an SQL-like
language, as SQL languages tend to lack expressive power when it comes
to the sort of calculations you would like to perform on time series.

Labels are a key part of PromQL, and you can use them not only to do
arbitrary aggregations but also to join different metrics together for
arithmetic operations against them. There are a wide variety of
functions available to you from prediction to date and math functions.

This chapter will introduce you to the basic concepts of PromQL,
including aggregation, basic types, and the HTTP API.

::: {.section pdf-bookmark="Aggregation Basics" data-type="sect1"}
::: {#ch13.xhtml#aggregation_basics .sect1}
# Aggregation Basics

Let's get started with some simple aggregation queries.
[]{#ch13.xhtml#ix_PQLaggbsc primary="PromQL"
secondary="aggregation basics"
data-type="indexterm"}[]{#ch13.xhtml#ix_aggPQL primary="aggregation"
secondary="basics of in PromQL" data-type="indexterm"}These queries will
likely cover most of your potential uses for PromQL. While PromQL is as
powerful as it is possible to
be,^[1](#ch13.xhtml#idm45207092796192){#ch13.xhtml#idm45207092796192-marker
data-type="noteref"}^ most of the time your needs will be reasonably
simple.

::: {.section pdf-bookmark="Gauge" data-type="sect2"}
::: {#ch13.xhtml#idm45207092793872 .sect2}
## Gauge

Gauges are a snapshot of state, and usually when aggregating them you
want to take a sum, average, minimum, or
maximum.[]{#ch13.xhtml#ix_PQLaggbscgauge primary="PromQL"
secondary="aggregation basics" tertiary="gauge"
data-type="indexterm"}[]{#ch13.xhtml#ix_gaugeagg primary="gauges"
secondary="aggregating"
data-type="indexterm"}[]{#ch13.xhtml#ix_aggPQLgauge
primary="aggregation" secondary="basics of in PromQL" tertiary="gauge"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092787568 primary="sum"
secondary="using with gauges" data-type="indexterm"}

Consider the metric `node_filesystem_size_bytes` from your Node
Exporter, which reports the size of each of your mounted filesystems,
and has `device`, `fstype`, and `mountpoint` labels.
[]{#ch13.xhtml#idm45207092784176 primary="device labels"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092783440
primary="fstype labels"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092782768
primary="mountpoint labels"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092782096
primary="Node Exporter" secondary="node_filesystem_size_bytes metric"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092781184
primary="filesystems" secondary="node_filesystem_size_bytes metric"
data-type="indexterm"}You can calculate total filesystem size on each
machine with:

``` {data-type="programlisting"}
sum without(device, fstype, mountpoint)(node_filesystem_size_bytes)
```

This works as `without` tells the `sum` aggregator to sum everything up
with the same labels, ignoring those three. So if you had the time
series:

``` {data-type="programlisting"}
node_filesystem_free_bytes{device="/dev/sda1",fstype="vfat",
    instance="localhost:9100",job="node",mountpoint="/boot/efi"} 70300672
node_filesystem_free_bytes{device="/dev/sda5",fstype="ext4",
    instance="localhost:9100",job="node",mountpoint="/"} 30791843840
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run"} 817094656
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/lock"} 5238784
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/1000"} 826912768
```

the result would be:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 32511390720
```

You will notice that the `device`, `fstype`, and `mountpoint` labels are
now gone. The metric name is also no longer present, as this is no
longer `node_filesystem_free_bytes` because math has been performed on
it. Since there is only one Node Exporter being scraped by Prometheus,
there is only one result, but if you were scraping more, then you would
have a result for each of the Node Exporters.

You could go a step further and remove the `instance` label with:

``` {data-type="programlisting"}
sum without(device, fstype, mountpoint, instance)(node_filesystem_size_bytes)
```

This as expected removes the `instance` label, but the value
[]{#ch13.xhtml#idm45207092771616 primary="instance labels"
secondary="removing using sum without" data-type="indexterm"}remains the
same as the previous expression because there is only one Node Exporter
to aggregate metrics from:

``` {data-type="programlisting"}
{job="node"} 32511390720
```

You can use the same approach with other aggregations. `max` would tell
[]{#ch13.xhtml#idm45207092768736 primary="max"
secondary="using with gauges" data-type="indexterm"}you the size of the
biggest mounted filesystem on each machine:

``` {data-type="programlisting"}
max without(device, fstype, mountpoint)(node_filesystem_size_bytes)
```

The outputted labels are exactly the same as when you aggregated using
`sum`:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 30792601600
```

This predictability in what labels are returned is important for vector
matching with operators, as will be discussed in
[Chapter 15](#ch15.xhtml#promql_operators_chapter){data-type="xref"}.

You []{#ch13.xhtml#idm45207092763440 primary="avg without expression"
data-type="indexterm"}are not limited to aggregating metrics about one
type of job. For example, to find the average number of file descriptors
open across all your jobs, you could use:

``` {data-type="programlisting"}
avg without(instance, job)(process_open_fds)
```
:::
:::

::: {.section pdf-bookmark="Counter" data-type="sect2"}
::: {#ch13.xhtml#idm45207092792960 .sect2}
## Counter

Counters track the number or size of events, and the value your
applications expose on their */metrics* is the total since it
started.[]{#ch13.xhtml#idm45207092759712 primary="PromQL"
secondary="aggregation basics" startref="ix_PQLaggbscgauge"
tertiary="gauge" data-type="indexterm"}[]{#ch13.xhtml#idm45207092758192
primary="gauges" secondary="aggregating" startref="ix_gaugeagg"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092756976
primary="aggregation" secondary="basics of in PromQL"
startref="ix_aggPQLgauge" tertiary="gauge"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092755488
primary="counters" secondary="aggregating"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092754544 primary="PromQL"
secondary="aggregation basics" tertiary="counter"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092753328
primary="aggregation" secondary="basics of in PromQL" tertiary="counter"
data-type="indexterm"} But that total is of little use to you on its
own; what you really want to know is how quickly the counter is
increasing over time. This is usually done using the `rate` function,
though the `increase` and `irate` functions also operate on counter
values.[]{#ch13.xhtml#idm45207092750640 primary="rate function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092749936
primary="increase function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092749264
primary="irate function" data-type="indexterm"}

For example, to calculate the amount of network traffic received per
second, you could use:

``` {data-type="programlisting"}
rate(node_network_receive_bytes_total[5m])
```

The `[5m]` says to provide `rate` with 5 minutes of data, so the
returned value will be an average over the last 5 minutes:

``` {data-type="programlisting"}
{device="lo",instance="localhost:9100",job="node"}  1859.389655172414
{device="wlan0",instance="localhost:9100",job="node"} 1314.5034482758622
```

The values here are not integers, as the 5-minute window `rate` is
looking at does not perfectly align with the samples that Prometheus has
scraped. Some estimation is used to fill in the gaps between the data
points you have and the boundaries of the
range.[]{#ch13.xhtml#idm45207092744160 primary="gaps"
secondary="between data points you have and boundaries of the range."
secondary-sortas="data" data-type="indexterm"}

The output of `rate` is a gauge, so the same aggregations apply as for
gauges. The `node_network_receive_bytes_total` metric has a `device`
label, so if you aggregate it away you will get the total bytes received
per []{#ch13.xhtml#idm45207092741072 primary="device labels"
secondary="removing using sum without"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092740032 primary="sum"
secondary="using in counter aggregation" data-type="indexterm"}machine
per second:

``` {data-type="programlisting"}
sum without(device)(rate(node_network_receive_bytes_total[5m]))
```

Running this query will give you a result like:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 3173.8931034482762
```

You can filter down which time []{#ch13.xhtml#idm45207092736512
primary="instance labels" secondary="removing using sum without"
data-type="indexterm"}series to request, so you could only look at
`eth0` and then aggregate it across all machines by aggregating away the
`instance` label:

``` {data-type="programlisting"}
sum without(instance)(rate(node_network_receive_bytes_total{device="eth0"}[5m]))
```

When you run this query the `instance` label is gone, but the `device`
label remains as you did not ask for it to be removed:

``` {data-type="programlisting"}
{device="eth0",job="node"} 3173.8931034482762
```

There is no[]{#ch13.xhtml#idm45207092730960 primary="labels"
secondary="aggregating" data-type="indexterm"} ordering or hierarchy
within labels, allowing you to aggregate by as many or as few labels as
you like.
:::
:::

::: {.section pdf-bookmark="Summary" data-type="sect2"}
::: {#ch13.xhtml#summary_aggregation .sect2}
## Summary

A summary metric will usually contain both a `_sum` and `_count`, and
sometimes a time series with no suffix with a `quantile`
label.[]{#ch13.xhtml#idm45207092725824 primary="aggregation"
secondary="basics of in PromQL" tertiary="summary"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092724544 primary="summary"
secondary="aggregating"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092723600 primary="PromQL"
secondary="aggregation basics" tertiary="summary" data-type="indexterm"}
The `_sum` and `_count` are both [counters]{.keep-together}.

Your Prometheus []{#ch13.xhtml#idm45207092720160
primary="http_response_size_bytes" data-type="indexterm"}exposes an
`http_response_size_bytes` summary for the amount of data some of its
HTTP APIs
return.^[2](#ch13.xhtml#idm45207092718912){#ch13.xhtml#idm45207092718912-marker
data-type="noteref"}^ `http_response_size_bytes_count`
tracks[]{#ch13.xhtml#idm45207092717216
primary="http_response_size_bytes_count"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092716496
primary="rate function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092715824
primary="handler labels, aggregating away"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092715136 primary="sum"
secondary="using with summary" data-type="indexterm"} the number of
requests, and as it is a counter, you must use `rate` before aggregating
away its `handler` label:

``` {data-type="programlisting"}
sum without(handler)(rate(http_response_size_bytes_count[5m]))
```

This gives you the total per-second HTTP request rate, and as the Node
Exporter also returns this metric, you will see both jobs in the result:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 0.26868836781609196
{instance="localhost:9100",job="node"} 0.1
```

Similarly, `http_response_size_bytes_sum` is a
counter[]{#ch13.xhtml#idm45207092710256
primary="http_response_size_bytes_sum" data-type="indexterm"} with the
number of bytes each handle has returned, so the same pattern applies:

``` {data-type="programlisting"}
sum without(handler)(rate(http_response_size_bytes_sum[5m]))
```

This will return results with the same labels as the previous query, but
the values are larger as responses tend to return many bytes:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 796.0015958275862
{instance="localhost:9100",job="node"} 1581.6103448275862
```

The power of a summary is that it allows you to calculate
[]{#ch13.xhtml#idm45207092706880 primary="averages"
secondary="calculating with summary" data-type="indexterm"}the average
size of an event, in this case the average amount of bytes that are
being returned in each response. If you had three responses of size 1,
4, and 7, then the average would be their sum divided by their count,
which is to say 12 divided by 3. The same applies to the
summary.[]{#ch13.xhtml#idm45207092705728 primary="count"
secondary="dividing by sum" data-type="indexterm"} You divide the `_sum`
by the `_count` (after taking a `rate`) to get an average over a time
period:

``` {data-type="programlisting"}
  sum without(handler)(rate(http_response_size_bytes_sum[5m]))
/
  sum without(handler)(rate(http_response_size_bytes_count[5m]))
```

The division operator matches the time series with the same labels, and
divides, giving you the same two time series out but with the average
response size over the past 5 minutes as a value:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 2962.54580091246150133317
{instance="localhost:9100",job="node"} 15816.10344827586200000000
```

When calculating an average, it is important that you first aggregate up
the sum and count, and only as the last step perform the division.
Otherwise, you could end up averaging averages, which is not
statistically valid.

For example, if you []{#ch13.xhtml#idm45207092700688 primary="jobs"
secondary="getting average response size across all instances"
data-type="indexterm"}wanted to get the average response size across all
instances of a job, you could
do:^[3](#ch13.xhtml#idm45207092699456){#ch13.xhtml#idm45207092699456-marker
data-type="noteref"}^

``` {data-type="programlisting"}
  sum without(instance)(
    sum without(handler)(rate(http_response_size_bytes_sum[5m]))
  )
/
  sum without(instance)(
    sum without(handler)(rate(http_response_size_bytes_count[5m]))
  )
```

However, it'd be incorrect to do:

``` {data-type="programlisting"}
avg without(instance)(
    sum without(handler)(rate(http_response_size_bytes_sum[5m]))
  /
    sum without(handler)(rate(http_response_size_bytes_count[5m]))
)
```

It is incorrect to average an average, and both the division and `avg`
would be calculating averages.

::: {.note data-type="note"}
###### Note

It is not[]{#ch13.xhtml#idm45207092693536 primary="quantiles"
data-type="indexterm"} possible for you to aggregate the quantiles of a
summary (the time series with the `quantile` label) from a statistical
standpoint.
:::
:::
:::

::: {.section pdf-bookmark="Histogram" data-type="sect2"}
::: {#ch13.xhtml#histogram_intro .sect2}
## Histogram

Histogram metrics allow you to track the distribution of the size of
events, allowing you to calculate quantiles from
them.[]{#ch13.xhtml#ix_PQLaggbschst primary="PromQL"
secondary="aggregation basics" tertiary="histogram"
data-type="indexterm"}[]{#ch13.xhtml#ix_hstagg primary="histograms"
data-type="indexterm"} For example, you can use histograms to calculate
the 0.9 quantile (which is also known as the 90th percentile)
latency.[]{#ch13.xhtml#idm45207092687600 primary="quantiles"
secondary="calculating with histograms" data-type="indexterm"}

Prometheus 2.37.1 exposes a histogram metric called
`prometheus_tsdb_compaction_duration_seconds` that tracks how many
seconds compaction takes for the time series database.
[]{#ch13.xhtml#idm45207092685312 primary="buckets (in histograms)"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092684640
primary="compaction duration, time series database"
data-type="indexterm"}This histogram metric has time series with a
`_bucket` suffix called
`prometheus_tsdb_compaction_duration_seconds_bucket`. Each bucket has a
`le` label, which is a counter of how many events have a size less than
or equal to the bucket boundary.[]{#ch13.xhtml#idm45207092682320
primary="histogram_quantile function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092681600
primary="le labels" data-type="indexterm"} This is an implementation
detail you largely need not worry about as the `histogram_quantile`
function takes care of this when calculating quantiles. For example, the
0.90 quantile would be:

``` {data-type="programlisting"}
histogram_quantile(
    0.90,
    rate(prometheus_tsdb_compaction_duration_seconds_bucket[1d]))
```

As `prometheus_tsdb_compaction_duration_seconds_bucket` is a counter,
you must first take a `rate`. Compaction usually only happens every two
hours, so a one-day time range is used here and you
[]{#ch13.xhtml#idm45207092678112 primary="rate function"
data-type="indexterm"}will see a result in the expression browser such
as:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 7.720000000000001
```

This indicates that the 90th percentile latency of compactions is around
7.72 seconds. As there will usually only be 12 compactions in a day, the
90th percentile says that 10% of compactions take longer than this,
which is to say one or two compactions. This is something to be aware of
when using quantiles. For example, if you want to calculate a 0.999
quantile, you should have several thousand data points to work with in
order to produce a reasonably accurate answer. If you have fewer than
that, single outliers could greatly affect the result, and you should
consider using lower quantiles to avoid making statements about your
system for which you have insufficient data to back up.

::: {.note data-type="note"}
###### Note

Usually you would use a 5- or 10-minute `rate` with histograms. All the
bucket time series combined with any labels, and a long range on the
`rate`, can make for a lot of samples that need to be processed. Be wary
of PromQL expressions using ranges that are hours or days, as they can
be relatively expensive to
calculate.^[4](#ch13.xhtml#idm45207092673952){#ch13.xhtml#idm45207092673952-marker
data-type="noteref"}^
:::

Similar to when taking averages, using `histogram_quantile` should be
the last step in a query expression. Quantiles cannot be aggregated, or
have arithmetic performed upon them, from a statistical standpoint.
[]{#ch13.xhtml#idm45207092672304 primary="sum"
secondary="using in histogram of aggregates"
data-type="indexterm"}Accordingly, when you want to take a histogram of
an aggregate, first aggregate up with `sum` and then use
`histogram_quantile`:

``` {data-type="programlisting"}
histogram_quantile(
  0.90,
  sum without(instance)(rate(prometheus_tsdb_compaction_duration_bucket[1d])))
```

This calculates the 0.9 quantile compaction duration across all of your
Prometheus servers, and will produce a result without an `instance`
label:

``` {data-type="programlisting"}
{job="prometheus"} 7.720000000000001
```

Histogram metrics also include `_sum` and `_count` metrics, which work
exactly the same as for the summary metric.
[]{#ch13.xhtml#idm45207092666320 primary="count"
secondary="histogram metric"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092665264
primary="averages"
secondary="calculating average event size using histograms"
data-type="indexterm"}You can use these to calculate average event
sizes, such as the average compaction duration:

``` {data-type="programlisting"}
  sum without(instance)(rate(prometheus_tsdb_compaction_duration_sum[1d]))
/
  sum without(instance)(rate(prometheus_tsdb_compaction_duration_count[1d]))
```

This would produce a result like:

``` {data-type="programlisting"}
{job="prometheus"} 3.1766430400714287
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Selectors" data-type="sect1"}
::: {#ch13.xhtml#selectors .sect1}
# Selectors

Working with all the different time series with different
[]{#ch13.xhtml#idm45207092659776 primary="histograms"
startref="ix_hstagg"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092658720 primary="PromQL"
secondary="aggregation basics" startref="ix_PQLaggbschst"
tertiary="histogram"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092657232 primary="PromQL"
secondary="aggregation basics" startref="ix_PQLaggbsc"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092656016
primary="aggregation" secondary="basics of in PromQL"
startref="ix_aggPQL" data-type="indexterm"}[]{#ch13.xhtml#ix_slct
primary="selectors" data-type="indexterm"}[]{#ch13.xhtml#ix_PQLsel
primary="PromQL" secondary="selectors" data-type="indexterm"}label
values for a metric can be a bit overwhelming, and potentially confusing
if a metric is coming from multiple different types of
servers.^[5](#ch13.xhtml#idm45207092652352){#ch13.xhtml#idm45207092652352-marker
data-type="noteref"}^ Usually you will want to narrow down which time
series you are working on. You almost always will want to limit by `job`
label, and depending on what you are up to, you might want to only look
at one `instance` or one `handler`, for example.

This limiting by labels is done using *selectors*. You have seen
selectors in every example thus far, and now we are going to explain
them to you in detail.[]{#ch13.xhtml#idm45207092648128 primary="labels"
secondary="limiting by, using selectors" data-type="indexterm"} For
example:

``` {data-type="programlisting"}
process_resident_memory_bytes{job="node"}
```

is a selector that will return all time series with the name
`process_resident_​`[`memory_bytes`]{.keep-together} and a `job` label of
`node`. This particular selector []{#ch13.xhtml#idm45207092643584
primary="instant vector selector" data-type="indexterm"}is most properly
called an *instant vector selector*, as it returns the values of the
given time series at a given instant. *Vector* here basically means a
one-dimensional list, as a selector can return zero or more time series,
and each time series will have one
sample.[]{#ch13.xhtml#idm45207092641792 primary="vectors"
seealso="instant vector selector" data-type="indexterm"}

The `job="node"` is called a *matcher*, and you can have many matchers
in one selector that are ANDed together.[]{#ch13.xhtml#idm45207092639568
primary="regular expressions" secondary="matchers"
data-type="indexterm"}

::: {.section pdf-bookmark="Matchers" data-type="sect2"}
::: {#ch13.xhtml#idm45207092638464 .sect2}
## Matchers

There are four []{#ch13.xhtml#idm45207092636992 primary="matchers"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092636256
primary="selectors" secondary="matchers"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092635312 primary="PromQL"
secondary="selectors" tertiary="matchers" data-type="indexterm"}matchers
(you have already seen the *equality matcher*, which is also the most
commonly used):

`=`

:   This is the *equality matcher*; for example, `job="node"`. With this
    you can specify that the returned time series has a label name with
    exactly the given label value. []{#ch13.xhtml#idm45207092630096
    primary="= (equality matcher)"
    data-type="indexterm"}[]{#ch13.xhtml#idm45207092629392
    primary="equality matcher (=)" data-type="indexterm"}As an empty
    label, value is the same as not having that label, so you could use
    `foo=""` to specify that the `foo` label not be present.

`!=`

:   This is the *negative equality matcher*; for example, `job!="node"`.
    With this you can specify that the returned time series do not have
    a label name with exactly the given label
    value.[]{#ch13.xhtml#idm45207092625232
    primary="negative equality matcher (!=)"
    data-type="indexterm"}[]{#ch13.xhtml#idm45207092624560
    primary="!= (negative equality matcher)" data-type="indexterm"}

`=~`

:   This is the *regular expression matcher*; for example, `job=~"n.*"`.
    With this you specify that for the returned time series, the given
    label's value will be matched by the regular
    expression.[]{#ch13.xhtml#idm45207092621072
    primary="=~ (regular expression matcher)"
    data-type="indexterm"}[]{#ch13.xhtml#idm45207092620352
    primary="regular expression matcher (=~)"
    data-type="indexterm"}[]{#ch13.xhtml#idm45207092619664
    primary="fully anchored regular expressions" data-type="indexterm"}
    The regular expression is fully anchored, which is to say that the
    regular expression `a` will only match the string `a`, and not `xa`
    or `ax`. []{#ch13.xhtml#idm45207092617152
    primary=".* prefixing/suffixing regular expressions"
    data-type="indexterm"}You can prepend or suffix your regular
    expression with `.*` if you do not want this
    behavior.^[6](#ch13.xhtml#idm45207092615760){#ch13.xhtml#idm45207092615760-marker
    data-type="noteref"}^ As with relabeling, the RE2 regular expression
    engine is used, as covered in ["Regular
    Expressions"](#ch08.xhtml#regex){data-type="xref"}.

`!~`

:   This is the *negative regular expression matcher*. RE2 does not
    support negative lookahead expressions, so this provides you with an
    alternative way to exclude label values based on a regular
    expression.[]{#ch13.xhtml#idm45207092612480
    primary="negative regular expression matcher (!~)"
    data-type="indexterm"}[]{#ch13.xhtml#idm45207092611712
    primary="!~ (negative regular expression matcher)"
    data-type="indexterm"}

You can have multiple matchers with the same label name in a selector,
which can be a substitute for negative lookahead expressions. For
example, to find the size of all filesystems mounted under `/run` but
not `/run/user`, you could
use:^[7](#ch13.xhtml#idm45207092609296){#ch13.xhtml#idm45207092609296-marker
data-type="noteref"}^

``` {data-type="programlisting"}
node_filesystem_size_bytes{job="node",mountpoint=~"/run/.*",
    mountpoint!~"/run/user/.*"}
```

Internally, the metric name is stored in a label called `__name__` (as
discussed in ["Reserved Labels and
\_\_name\_\_"](#ch05.xhtml#name_label){data-type="xref"}), so
`process_resident_` [`memory_bytes{job="node"}`]{.keep-together} is
syntactic sugar for `{`*`name`*`="process_resident_`
[`memory_bytes",job="node"}`]{.keep-together}. You can even do regular
expressions on the metric name, but this is unwise outside of when you
are debugging the performance of the Prometheus
server.[]{#ch13.xhtml#idm45207092602432 primary="name labels"
data-type="indexterm"}

::: {data-type="tip"}
###### Tip

Having to use regular expression matchers is a little bit of a smell. If
you find yourself using them a lot on a given label, consider if you
should instead combine the matched label values into one. For example,
for HTTP status codes instead of doing `code~="4.."` to catch 401s,
404s, 405s, etc., you might combine them into a label value `4xx` and
use the equality matcher `code="4xx"`.
:::

The selector `{}` returns an error, which is a safety measure to avoid
accidentally returning all the time series inside the Prometheus server
as that could be expensive. To be more precise, at least one of the
matchers in a selector must not match the empty string. So `{foo=""}`
and `{foo=~".*"}` will return an error, while `{foo="",bar="x"}`,
`{foo!=""}`, or `{foo=~".+"}` are
permitted.^[8](#ch13.xhtml#idm45207092595904){#ch13.xhtml#idm45207092595904-marker
data-type="noteref"}^
:::
:::

::: {.section pdf-bookmark="Instant Vector" data-type="sect2"}
::: {#ch13.xhtml#instant_vector .sect2}
## Instant Vector

An instant vector selector returns an `instant vector` of the most
recent samples before the query evaluation time, which is to say a list
of zero or more time series.[]{#ch13.xhtml#idm45207092592320
primary="instant vector selector" data-type="indexterm"} Each of these
time series will have one sample, and a sample contains both a value and
a timestamp. []{#ch13.xhtml#idm45207092591376 primary="timestamps"
secondary="returned by instant vector selector"
data-type="indexterm"}While the instant vector returned by an instant
vector selector has the timestamp of the original
data,^[9](#ch13.xhtml#idm45207092590128){#ch13.xhtml#idm45207092590128-marker
data-type="noteref"}^ any instant vectors returned by other operations
or functions will have the timestamp of the query evaluation time for
all of their [values]{.keep-together}.

When you ask for current memory usage, you do not want samples from an
instance that was turned down days ago to be included, a concept known
as *staleness*. []{#ch13.xhtml#idm45207092587296 primary="staleness"
data-type="indexterm"}In Prometheus 1.x this was handled by returning
time series that had a sample no more than 5 minutes before the query
evaluation time. This largely worked but had downsides such as double
counting if an instance restarted with a new `instance` label within
that 5-minute window.

Prometheus 2.x has a []{#ch13.xhtml#idm45207092585600
primary="stale markers" data-type="indexterm"}more sophisticated
approach. If a time series disappears from one scrape to the next, or if
a target is no longer returned from service discovery, a special type of
sample called a *stale
marker*^[10](#ch13.xhtml#idm45207092584288){#ch13.xhtml#idm45207092584288-marker
data-type="noteref"}^ is appended to the time series. When evaluating an
instant vector selector, all time series satisfying all the matchers are
first found, and the most recent sample in the 5 minutes before the
query evaluation time is still considered. If the sample is a normal
sample, then it is returned in the instant vector, but if it is a stale
marker, then that time series will not be included in that instant
vector.

The outcome of all of this is that when you use an instant vector
selector, time series that have gone stale are not returned.

::: {.note data-type="note"}
###### Note

If you have an exporter exposing timestamps, as described in
["Timestamps"](#ch04.xhtml#format_timestamps){data-type="xref"}, then
stale markers and the Prometheus 2.x staleness logic will not apply.
[]{#ch13.xhtml#idm45207092581152 primary="timestamps"
secondary="exporters exposing, staleness and"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092580192
primary="exporters" secondary="exposing timestamps, staleness and"
data-type="indexterm"}The affected time series will work instead with
the older logic that looks back 5 minutes.
:::
:::
:::

::: {.section pdf-bookmark="Range Vector" data-type="sect2"}
::: {#ch13.xhtml#idm45207092578736 .sect2}
## Range Vector

There is a second type of selector you have already seen, called the
*range vector selector*. []{#ch13.xhtml#idm45207092576384
primary="selectors" secondary="range vector"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092575376 primary="PromQL"
secondary="selectors" tertiary="range vector"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092574160
primary="range vector selector" data-type="indexterm"}Unlike an instant
vector selector, which returns one sample per time series, a range
vector selector can return many samples for each time
series.^[11](#ch13.xhtml#idm45207092573200){#ch13.xhtml#idm45207092573200-marker
data-type="noteref"}^ Range vectors are always used with the `rate`
function, for example:

``` {data-type="programlisting"}
rate(process_cpu_seconds_total[1m])
```

The `[1m]` turns the instant vector selector into a range vector
selector, and instructs PromQL to return for all time series matching
the selector all samples for the minute up to the query evaluation time.
If you execute just `process_cpu_seconds_​`[`total[1m]`]{.keep-together}
in the Console tab of the expression browser, you will see something
like [Figure 13-1](#ch13.xhtml#range_vector_eb){data-type="xref"}.

In this case, each time series happens to have six samples in the past
minute. You will notice that while the samples for each time series
happen to be perfectly 10 seconds
apart^[12](#ch13.xhtml#idm45207092565504){#ch13.xhtml#idm45207092565504-marker
data-type="noteref"}^ in line with the scrape interval you configured,
the two time series timestamps are not aligned with each other. One time
series has a sample with a timestamp of `1517925155.087` and the other
`1517925156.245`.

<figure>
<div id="ch13.xhtml#range_vector_eb" class="figure">
<img src="assets/pur2_1301.png" width="600" height="363"
alt="Two time series with several points each." />
<h6><span class="label">Figure 13-1. </span>A range vector in the
Console tab of the expression browser</h6>
</div>
</figure>

This is []{#ch13.xhtml#idm45207092561312 primary="expression browser"
secondary="range vector in Console tab" data-type="indexterm"}because
range vectors preserve the actual timestamps of the samples, and the
scrapes for different targets are distributed in order to spread load
more evenly. While you can control the frequency of scrapes and rule
evaluations, you cannot control their phase or alignment. If you have a
10-second scrape interval and hundreds of targets, then all those
targets will be scraped at different points in a given 10-second window.
Put another way, your time series all have slightly different ages. This
generally won't matter to you in practice, but can lead to artifacts as
fundamentally metrics-based monitoring systems like Prometheus produce
(quite good) estimates rather than exact answers.

You will very rarely look at range vectors directly. It only comes up
when you need to see raw samples when debugging. Almost always you will
use a range vector with a function such as `rate` or `avg_over_time`
that takes a range vector as an
argument.[]{#ch13.xhtml#idm45207092558704 primary="avg_over_time"
data-type="indexterm"}

Staleness and stale markers have no impact on range vectors; you will
get all the normal samples in a given range.
[]{#ch13.xhtml#idm45207092557488 primary="staleness"
secondary="range vectors and" data-type="indexterm"}Any stale markers
also in that range are not returned by a range vector selector.

```{=html}
<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar">
```
::: {#ch13.xhtml#idm45207092556256 .sidebar}
# Durations

Durations in Prometheus as used in PromQL and the configuration file
support several units. []{#ch13.xhtml#idm45207092553984
primary="durations" secondary="in Prometheus as used in PromQL"
secondary-sortas="Prometheus"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092552768
primary="milliseconds"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092552096 primary="time"
secondary="durations in Prometheus as used in PromQL"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092551056 primary="seconds"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092550384 primary="minutes"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092549712 primary="hours"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092549040 primary="days"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092548368 primary="weeks"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092547696 primary="years"
data-type="indexterm"}You have already seen `m` for minute.

  Suffix   Meaning
  -------- ----------------------------------------
  `ms`     Milliseconds
  `s`      Seconds, which have 1,000 milliseconds
  `m`      Minutes, which have 60 seconds
  `h`      Hours, which have 60 minutes
  `d`      Days, which have 24 hours
  `w`      Weeks, which have 7 days
  `y`      Years, which have 365 days

You can combine multiple units with integers, as long as they are
ordered, so `90m` is valid, `1h30m` and `1.5h` are also valid, but 30m1h
is not valid.

Leap years and leap seconds are ignored; `1y` is always `60*60*24*365`
seconds.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Subqueries" data-type="sect2"}
::: {#ch13.xhtml#idm45207092578144 .sect2}
## Subqueries

While range vectors act on time series, they cannot be used in
combination with functions.[]{#ch13.xhtml#idm45207092526624
primary="selectors" secondary="subqueries"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092525648 primary="PromQL"
secondary="selectors" tertiary="subqueries"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092524432
primary="subqueries"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092523760
primary="range vectors" data-type="indexterm"}

If you want to combine `max_over_time` with `rate`, you can either use
recording rules, which would record the result of the `rate` function
and pass it to the vector function, or you can use a
subquery.[]{#ch13.xhtml#idm45207092521392 primary="rate function"
secondary="using with max_over_time"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092520448
primary="max_over_time" data-type="indexterm"}

A subquery is a part of a query that allows you to do a range query
within a query. The syntax for a subquery uses square brackets, like
range selectors. But it takes two different durations: the range and the
resolution.

The range is the range returned by the subquery, and the resolution acts
as a step:

``` {data-type="programlisting"}
max_over_time( rate(http_requests_total[5m])[30m:1m] )
```

The preceding query runs `rate(http_requests_total[5m])` every minute
(`1m`) for the last 30 minutes (`30m`), then feeds the result in a
`max_over_time()` function.

The resolution can be omitted, such as in `[30m:]`. In this case, the
global evaluation interval is used as resolution.
:::
:::

::: {.section pdf-bookmark="Offset" data-type="sect2"}
::: {#ch13.xhtml#idm45207092514944 .sect2}
## Offset

There is a modifier you can use with[]{#ch13.xhtml#idm45207092513200
primary="offset" data-type="indexterm"}[]{#ch13.xhtml#idm45207092512496
primary="selectors" secondary="offset modifier"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092511552 primary="PromQL"
secondary="selectors" tertiary="offset" data-type="indexterm"} either
type of vector selector called `offset`. `offset` allows you to take the
evaluation time for a query, and on a per-selector basis put it further
back in time. For example:

``` {data-type="programlisting"}
process_resident_memory_bytes{job="node"} offset 1h
```

would get memory usage an hour before the query evaluation time.

`offset` is not used much in simple queries like this, as it would be
easier to change the evaluation time for the whole query instead. Where
this can be useful is when you only want to adjust one selector in a
query expression. For example:

``` {data-type="programlisting"}
  process_resident_memory_bytes{job="node"}
-
  process_resident_memory_bytes{job="node"} offset 1h
```

would give the change in memory usage in the Node Exporter over the past
hour.^[13](#ch13.xhtml#idm45207092506016){#ch13.xhtml#idm45207092506016-marker
data-type="noteref"}^

The same approach works with range vectors:

``` {data-type="programlisting"}
  rate(process_cpu_seconds_total{job="node"}[5m])
-
  rate(process_cpu_seconds_total{job="node"}[5m] offset 1h)
```

`offset` allows you to look further back into the past, but also in the
future, using a negative offset. This can be used when doing prediction
or when the sample of the metrics is unaligned with the reality:

``` {data-type="programlisting"}
  rate(process_cpu_seconds_total{job="node"}[5m]) offset -1h
-
  rate(process_cpu_seconds_total{job="node"}[5m])
```

Note that this query will likely not return anything for the last hour.

::: {data-type="tip"}
###### Tip

Grafana has a feature to shift in time a panel to a different time range
than the rest of the dashboard it is a part
of.[]{#ch13.xhtml#idm45207092497696 primary="Grafana"
secondary="time shifting panel to different time range"
data-type="indexterm"} In Grafana 5.0.0 you can find this in the Time
range tab of the panel editor.
:::
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="At Modifier" data-type="sect2"}
::: {#ch13.xhtml#at_modifier .sect2}
## At Modifier

Similar to the `offset` modifier, PromQL supports an `@` modifier that
lets you change the evaluation of vector selectors, range selectors, and
subqueries to a fixed [revaluation
time]{.keep-together}.[]{#ch13.xhtml#idm45207092492464
primary="selectors" secondary="at (@) modifier"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092491456 primary="PromQL"
secondary="selectors" tertiary="at (@) modifier"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092490240
primary="@ (at) modifier"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092489568
primary="at (@) modifier" data-type="indexterm"}

The `@` modifier can be used with a Unix timestamp.
[]{#ch13.xhtml#idm45207092488000 primary="http_requests_total"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092487264
primary="timestamps" secondary="Unix, use of @ modifier with"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092486352
primary="rate function" secondary="using at (@) modifier with"
data-type="indexterm"}The query
[`http_`]{.keep-together}`requests_total @ 1667491200` returns the value
of `http_requests_total` at `2022-11-03T16:00:00+00:00`. The query
`rate(http_requests_total[5m] @ 1667491200)` returns the 5-minute rate
of `http_requests_total` at the same time.

Additionally, `start()` and `end()` can be used as values for the `@`
modifier. For a range query, they resolve respectively with the start
and the end of the range query. []{#ch13.xhtml#idm45207092480624
primary="start function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092479920
primary="end function" data-type="indexterm"}For an instant query, they
both resolve to the evaluation time.

In practice, it is possible to use the `@` modifier to graph the
evolution of the `http_request_total` that has a high rate at the end of
the evaluation interval:

``` {data-type="programlisting"}
  rate(http_requests_total[1m])
    and
  topk(5, rate(http_requests_total[1h] @ end()))
```

The `topk(5, rate(http_requests_total[1h] @ end()))` acts as a ranking
function, filtering only the higher values at the end of the evaluation
interval.[]{#ch13.xhtml#idm45207092475808 primary="topk operator"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092475136 primary="PromQL"
secondary="selectors" startref="ix_PQLsel"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092473920
primary="selectors" startref="ix_slct" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="HTTP API" data-type="sect1"}
::: {#ch13.xhtml#idm45207092661408 .sect1}
# HTTP API

Prometheus offers a number of HTTP APIs. []{#ch13.xhtml#ix_PQLHTTP
primary="PromQL" secondary="HTTP API"
data-type="indexterm"}[]{#ch13.xhtml#ix_HTTPAPI primary="HTTP API"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092469312 primary="APIs"
seealso="HTTP API" data-type="indexterm"}The ones you will mostly
interact with are `query` and `query_range`, which give you access to
PromQL and can be used by dashboarding tools or custom reporting
scripts.

All the endpoints of interest are under `/api/v1/`, and beyond executing
PromQL you can also look up time series metadata and perform
administrative actions, such as taking snapshots and deleting time
series. These other APIs are mainly of interest to dashboarding tools
such as Grafana, which can use metadata to enhance its UI, and to those
administering Prometheus, but are not relevant to PromQL execution.

::: {.section pdf-bookmark="query" data-type="sect2"}
::: {#ch13.xhtml#query_api .sect2}
## query

The *query endpoint*, or more formally `/api/v1/query`, executes a
PromQL expression at a given time and returns the result.
[]{#ch13.xhtml#ix_PQLHTTPqry primary="PromQL" secondary="HTTP API"
tertiary="query" data-type="indexterm"}[]{#ch13.xhtml#ix_HTTPAPIqry
primary="HTTP API" secondary="query or query endpoint"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092460336
primary="query endpoint" data-type="indexterm"}[]{#ch13.xhtml#ix_qry
primary="query" data-type="indexterm"}For example,
*http://localhost:9090/api/v1/query?query=process_resident_memory_bytes*
will return results
like:^[14](#ch13.xhtml#idm45207092458176){#ch13.xhtml#idm45207092458176-marker
data-type="noteref"}^

``` {code-language="json" data-type="programlisting"}
{
  "status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "process_resident_memory_bytes",
          "instance": "localhost:9090",
          "job": "prometheus"
        },
        "value": [1517929228.782, "91656192"]
      },
      {
        "metric": {
          "__name__": "process_resident_memory_bytes",
          "instance": "localhost:9100",
          "job": "node"
        },
        "value": [1517929228.782, "15507456"]
      }
    ]
  }
}
```

The `status` is `success`, meaning that the query worked. If it had
failed, the `status` would be `error`, and an `error` field would
provide more details.

This particular result is an instant vector, which
you[]{#ch13.xhtml#idm45207092352736 primary="resultType"
secondary="vector"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092351760
primary="instant vectors" data-type="indexterm"} can tell from
`"resultType": "vector"`. For each of the samples in the result, the
labels are in the `metric` map, and the sample value is in the `value`
list. The first number in the `value` list is the timestamp of the
sample, in seconds, and the second is the actual value of the sample.
The value is inside a string, as JSON cannot represent nonreal values
such as `NaN` and `+Inf`.

The time of all the samples will be the query evaluation time, even if
the expression consisted of only an instant vector
selector.[]{#ch13.xhtml#idm45207092348112 primary="time"
secondary="time URL parameter" data-type="indexterm"} Here the query
evaluation time defaulted to the current time, but you can specify a
time with the `time` URL parameter, which can be a Unix time, in
seconds, or an RFC 3339 time. For example,
*http://localhost:9090/api/v1/query?query=process_resident_memory_bytes&time=1514764800*
would evaluate the query at midnight of January 1st,
2018.^[15](#ch13.xhtml#idm45207092345984){#ch13.xhtml#idm45207092345984-marker
data-type="noteref"}^

You can also use range vectors with the `query` endpoint.
[]{#ch13.xhtml#idm45207092344384 primary="range vectors"
secondary="use with query endpoint" data-type="indexterm"}For example,
*http://localhost:9090/api/v1/query?query=prometheus_tsdb_head_samples_appended_total\[1m\]*
will return results like:

``` {code-language="json" data-type="programlisting"}
{
  "status": "success",
  "data": {
    "resultType": "matrix",
    "result": [
      {
        "metric": {
          "__name__": "process_resident_memory_bytes",
          "instance": "localhost:9090",
          "job": "prometheus"
        },
        "values": [
          [1518008453.662, "87318528"],
          [1518008463.662, "87318528"],
          [1518008473.662, "87318528"]
        ]
      },
      {
        "metric": {
          "__name__": "process_resident_memory_bytes",
          "instance": "localhost:9100",
          "job": "node"
        },
        "values": [
          [1518008444.819, "17043456"],
          [1518008454.819, "17043456"],
          [1518008464.819, "17043456"]
        ]
      }
    ]
  }
}
```

This is different than the previous instant vector result, as
`resultType` is now `matrix`, and each time series has multiple
values.[]{#ch13.xhtml#idm45207092122448 primary="resultType"
secondary="matrix" data-type="indexterm"} When used with a range vector,
the `query` endpoint returns the raw
samples,^[16](#ch13.xhtml#idm45207092158960){#ch13.xhtml#idm45207092158960-marker
data-type="noteref"}^ but be wary of asking for too much data at once
because one end or the other may run out of memory.

There is one[]{#ch13.xhtml#idm45207092157712 primary="resultType"
secondary="scalar" data-type="indexterm"} other type of result called a
*scalar*. []{#ch13.xhtml#idm45207092156192 primary="scalars"
data-type="indexterm"}Scalars don't have labels, they are just
numbers.^[17](#ch13.xhtml#idm45207092155328){#ch13.xhtml#idm45207092155328-marker
data-type="noteref"}^ *http://localhost:9090/api/v1/query?query=42*
would produce:

``` {.pagebreak-before code-language="json" data-type="programlisting"}
{
  "status": "success",
  "data": {
    "resultType": "scalar",
    "result": [1518008879.023, "42"]
  }
}
```
:::
:::

::: {.section pdf-bookmark="query_range" data-type="sect2"}
::: {#ch13.xhtml#query_range .sect2}
## query_range

The *query range endpoint* at `/api/v1/query_range` is the main HTTP
endpoint of Prometheus you will use, as it is the endpoint to use for
graphing.[]{#ch13.xhtml#idm45207091977728 primary="PromQL"
secondary="HTTP API" startref="ix_PQLHTTPqry" tertiary="query"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091976208
primary="HTTP API" secondary="query or query endpoint"
startref="ix_HTTPAPIqry"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091968624 primary="query"
startref="ix_qry" data-type="indexterm"}[]{#ch13.xhtml#ix_HTTPAPIqryrng
primary="HTTP API" secondary="query range endpoint or query_range"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091966688
primary="query range endpoint" seealso="query_range"
data-type="indexterm"}[]{#ch13.xhtml#ix_PQLHTTPqryrng primary="PromQL"
secondary="HTTP API" tertiary="query_range"
data-type="indexterm"}[]{#ch13.xhtml#ix_qryrng primary="query_range"
data-type="indexterm"} Under the covers, `query_range` is syntactic
sugar (plus some performance optimizations) for multiple calls to the
`query` endpoint.

In addition to a `query` URL parameter, you provide `query_range` with a
`start` time, an `end` time, and a `step`.
[]{#ch13.xhtml#idm45207091959792 primary="step"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091959056
primary="start time"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091958384 primary="end"
data-type="indexterm"}The query is first executed at the `start` time.
Then it is [executed]{.keep-together} `step` seconds after the start
time. Then it is executed twice `step` seconds after the `start` time
and so on, stopping when the query evaluation time would exceed the
`end` time. []{#ch13.xhtml#idm45207091954752 primary="instant vectors"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091954016 primary="scalars"
secondary="conversion to instant vector" data-type="indexterm"}All the
instant
vector^[18](#ch13.xhtml#idm45207091952976){#ch13.xhtml#idm45207091952976-marker
data-type="noteref"}^ results from the different executions are combined
into a range vector and returned.[]{#ch13.xhtml#idm45207091951872
primary="range vectors" secondary="use with query_result"
data-type="indexterm"}

For example, if you wanted to query the number of samples Prometheus
ingested in the first 15 minutes of 2018, you could run the following:
*http://localhost:9090/api/v1/query_range?query=rate(prometheus_tsdb_head_samples_appended_total\[5m\])&start​=1514764800&end=1514765700&step=60*,
which would produce a result like:

``` {code-language="json" data-type="programlisting"}
{
  "status": "success",
  "data": {
    "resultType": "matrix",
    "result": [
      {
        "metric": {
          "instance": "localhost:9090",
          "job": "prometheus"
        },
        "values": [
          [1514764800, "85.07241379310345"],
          [1514764860, "102.6793103448276"],
          [1514764920, "120.30344827586208"],
          [1514764980, "137.93103448275863"],
          [1514765040, "146.7586206896552"],
          [1514765100, "146.7793103448276"],
          [1514765160, "146.8"],
          [1514765220, "146.8"],
          [1514765280, "146.8"],
          [1514765340, "146.8"],
          [1514765400, "146.8"],
          [1514765460, "146.8"],
          [1514765520, "146.8"],
          [1514765580, "146.8"],
          [1514765640, "146.8"],
          [1514765700, "146.8"],
        ]
      }
    ]
  }
}
```

There are a few aspects of this that you should take note
of.[]{#ch13.xhtml#idm45207091738128 primary="timestamps"
secondary="aligning with start time and step" data-type="indexterm"} The
first is that the sample timestamps align with the start time and step,
as each result comes from a [different]{.keep-together} instant query
evaluation and instant query results always use their evaluation time as
the timestamp of results.

The second is that the last sample here is at the end time, which is to
say that the range is inclusive and the last point will be the end time
if it happens to line up with the step.

The third is that we selected a range of 5 minutes for the `rate`
function, which is larger than the step. Since `query_range` is doing
repeated instant query evaluations, there is no state being passed
between the evaluations. If the range was smaller than the step, then we
would have been skipping over data. For example, a 1-minute range with a
[5-minute]{.keep-together} step would have ignored 80% of the samples.
To prevent this you should use ranges that are at least one or two
scrape intervals larger than the `step` you are using.

::: {.warning data-type="warning"}
###### Warning

When using range vectors with `query_range`, you should usually use a
range that is longer than your `step` in order to not skip data.
:::

The fourth is that some of the samples are not particularly round, and
that any numbers are round at all is due to this being a simple setup of
the sample values. When working with metrics your data is rarely
perfectly clean; different targets are scraped at different times and
scrapes can be delayed. When performing queries that are not perfectly
aligned with the underlying data or aggregating across multiple hosts,
you will rarely get round results. In addition, the nature of
floating-point calculations can lead to numbers that are almost round.

Here, there is a sample for each step. If it happened that there was no
result for a given time series for a step, then that sample would simply
be missing in the end result.

::: {.note data-type="note"}
###### Note

If there are more than 11,000 steps for a `query_range`, Prometheus will
reject the query with an error.[]{#ch13.xhtml#idm45207091813360
primary="step" secondary="maximum number for query_range"
data-type="indexterm"} This is to prevent accidentally sending extremely
large queries to Prometheus, such as a 1-second step for a week. As
monitors with a horizontal resolution of over 11,000 pixels are rare,
you are unlikely to run into this when graphing.

If you are writing reporting scripts, you can split up `query_range`
requests that would hit this limit. This limit allows for a minute
resolution for a week, or an hour of resolution for a year, so most of
the time it should not apply.[]{#ch13.xhtml#idm45207091811296
primary="query_range" startref="ix_qryrng"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091810320 primary="PromQL"
secondary="HTTP API" startref="ix_PQLHTTPqryrng" tertiary="query_range"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091808832
primary="HTTP API" secondary="query range endpoint or query_range"
startref="ix_HTTPAPIqryrng" data-type="indexterm"}
:::

::: {.section pdf-bookmark="Aligned data" data-type="sect3"}
::: {#ch13.xhtml#idm45207091807392 .sect3}
### Aligned data

When using tools like Grafana it's common for the alignment of
`query_range` to be based on the current time, and so your results will
not align perfectly with minutes, hours, or
days.[]{#ch13.xhtml#idm45207091805552 primary="Grafana"
secondary="aligned data"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091804576
primary="aligned data"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091803904
primary="HTTP API" secondary="aligned data"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091802960 primary="PromQL"
secondary="HTTP API" tertiary="aligned data" data-type="indexterm"}
While this is fine when you are looking at dashboards, it is rarely what
you want with reporting scripts.[]{#ch13.xhtml#idm45207091801488
primary="start" data-type="indexterm"}

`query_range` does not have an option to specify alignment, instead it
is up to you to specify a `start` parameter with the right alignment.
For example, if you wanted to have samples every hour on the hour in
Python, the expression `(time.time() // 3600) * 3600` will return the
start of the current
hour,^[19](#ch13.xhtml#idm45207091799024){#ch13.xhtml#idm45207091799024-marker
data-type="noteref"}^ which you can adjust in steps of 3,600 and use as
the `start` and `end` URL parameters, and then use a `step` parameter of
`3600`.[]{#ch13.xhtml#idm45207091665440 primary="end"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091664832 primary="step"
data-type="indexterm"}

Now that you know the basics of how to use PromQL and execute queries
via the HTTP APIs, we will go into more detail on
aggregation.[]{#ch13.xhtml#idm45207091663968 primary="PromQL"
secondary="HTTP API" startref="ix_PQLHTTP"
data-type="indexterm"}[]{#ch13.xhtml#idm45207091662720
primary="HTTP API" startref="ix_HTTPAPI" data-type="indexterm"}
:::
:::
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch13.xhtml#idm45207092796192-marker)^ Brian has demonstrated
PromQL to be Turing Complete in [two](https://oreil.ly/TQWlz)
[different](https://oreil.ly/kikcz) ways. Don't try this in production.

^[2](#ch13.xhtml#idm45207092718912-marker)^ In Prometheus 2.3.0 this was
renamed to `prometheus_http_response_size_bytes_count`.

^[3](#ch13.xhtml#idm45207092699456-marker)^ This can of course be more
simply calculated as `sum without(instance, handler)(…​)`, but with the
recording rules covered in
[Chapter 17](#ch17.xhtml#promql_rules_chapter){data-type="xref"}, such
an expression could end up split into several expressions.

^[4](#ch13.xhtml#idm45207092673952-marker)^ The day-long range is only
being used here due to the limited number of histograms that Prometheus
and the Node Exporter offer for us to use as examples.

^[5](#ch13.xhtml#idm45207092652352-marker)^ Such as
`process_cpu_seconds_total`, which most []{#ch13.xhtml#idm45207092651328
primary="process_cpu_seconds_total" data-type="indexterm"}exporters and
client libraries will expose.

^[6](#ch13.xhtml#idm45207092615760-marker)^ It works this way to avoid
accidentally overmatching. This way you usually get immediate feedback
if your regular expression is under matching, while an unanchored
expression might cause subtle issues down the line.

^[7](#ch13.xhtml#idm45207092609296-marker)^ The Node Exporter has a
`--collector.filesystem.ignored-mount-points` flag you could use if you
didn't want these filesystems exported in the first place.

^[8](#ch13.xhtml#idm45207092595904-marker)^ If you do want to return all
time series, you can use `{__name__=~".+"}`, but beware of the expense
of this expression.

^[9](#ch13.xhtml#idm45207092590128-marker)^ You can extract the samples'
timestamps using the `timestamp` function.

^[10](#ch13.xhtml#idm45207092584288-marker)^ Internally, stale markers
are a special type of NaN value. They are an implementation detail, and
you cannot access them directly via any of the query APIs that use
PromQL. But you could see them if you looked at the Prometheus server's
storage directly, such as via Prometheus's remote read endpoint.

^[11](#ch13.xhtml#idm45207092573200-marker)^ You may
[]{#ch13.xhtml#idm45207092572672 primary="matrix"
see="range vector selector" data-type="indexterm"}also see it referred
to as a *matrix* in places, as it is[]{#ch13.xhtml#idm45207092571152
primary="rate function" secondary="use with range vectors"
data-type="indexterm"} a two-dimensional data structure.

^[12](#ch13.xhtml#idm45207092565504-marker)^ This is a very lightly
loaded Prometheus, so there is no jitter.

^[13](#ch13.xhtml#idm45207092506016-marker)^ This
[]{#ch13.xhtml#idm45207092505360 primary="deriv function"
data-type="indexterm"}[]{#ch13.xhtml#idm45207092504624
primary="rate function" secondary="offset and" data-type="indexterm"}is
susceptible to outliers as it is using only two data points; the `deriv`
function discussed in ["deriv"](#ch16.xhtml#deriv){data-type="xref"} is
more robust.

^[14](#ch13.xhtml#idm45207092458176-marker)^ We have pretty printed
these JSON results for readability.

^[15](#ch13.xhtml#idm45207092345984-marker)^ Unless your Prometheus has
been running since then, this will produce an empty result.

^[16](#ch13.xhtml#idm45207092158960-marker)^ Excluding stale markers.

^[17](#ch13.xhtml#idm45207092155328-marker)^ This is different from
`{}`, which is the identity of a time series with no labels.

^[18](#ch13.xhtml#idm45207091952976-marker)^ A scalar result is
converted into an instant vector with a single time series with no
labels with the same value, as if the `vector` function was used. Range
vector results are not supported.

^[19](#ch13.xhtml#idm45207091799024-marker)^ `//` performs integer
division in Python.
:::
:::
:::

[]{#ch14.xhtml}

::: {#ch14.xhtml#sbo-rt-content}
::: {#ch14.xhtml#promql_aggregation_chapter .chapter}
# [Chapter 14. ]{.label}Aggregation Operators

You already learned about aggregation in ["Aggregation
Basics"](#ch13.xhtml#aggregation_basics){data-type="xref"}; however,
this is only a small taste of what is possible. Aggregation is
important.[]{#ch14.xhtml#ix_aggops primary="aggregation operators"
data-type="indexterm"}[]{#ch14.xhtml#ix_PQLaggop primary="PromQL"
secondary="aggregation operators" data-type="indexterm"} With
applications with thousands or even just tens of instances it's not
practical for you to sift through each instance's metrics individually.
Aggregation allows you to summarize metrics not just within one
application, but across applications too.

There are 12 aggregation operators in PromQL, with 2 optional clauses,
`without` and `by`. In this chapter you'll learn about the different
ways you can use aggregation.

::: {.section pdf-bookmark="Grouping" data-type="sect1"}
::: {#ch14.xhtml#grouping .sect1}
# Grouping

Before talking about the aggregation operators themselves, you need to
know about how time series are grouped.[]{#ch14.xhtml#ix_aggopsgrp
primary="aggregation operators" secondary="grouping"
data-type="indexterm"}[]{#ch14.xhtml#ix_PQLaggopgrp primary="PromQL"
secondary="aggregation operators" tertiary="grouping"
data-type="indexterm"}[]{#ch14.xhtml#ix_grpng primary="grouping"
data-type="indexterm"} Aggregation operators work only on instant
vectors, and they also output instant vectors.

Let's say you []{#ch14.xhtml#idm45207091649232
primary="node_filesystem_size_bytes" data-type="indexterm"}have the
following time series in Prometheus:

``` {data-type="programlisting"}
node_filesystem_size_bytes{device="/dev/sda1",fstype="vfat",
    instance="localhost:9100",job="node",mountpoint="/boot/efi"} 100663296
node_filesystem_size_bytes{device="/dev/sda5",fstype="ext4",
    instance="localhost:9100",job="node",mountpoint="/"} 90131324928
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run"} 826961920
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/lock"} 5242880
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/1000"} 826961920
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/119"} 826961920
```

There are three instrumentation labels: `device`, `fstype`, and
`mountpoint`. There are also two target labels: `job` and `instance`.
[]{#ch14.xhtml#idm45207091644800 primary="instrumentation labels"
secondary="device, fstype, and mountpoint"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091643824
primary="device labels"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091643152
primary="fstype labels"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091642480
primary="mountpoint labels" data-type="indexterm"}Target and
instrumentation labels are a notion that you and we have, but which
PromQL knows nothing about. []{#ch14.xhtml#idm45207091641552
primary="target labels" secondary="job and instance"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091640608
primary="job labels"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091639936
primary="instance labels" data-type="indexterm"}All labels are the same
when it comes to PromQL, no matter where they originated from.

::: {.section pdf-bookmark="without" data-type="sect2"}
::: {#ch14.xhtml#idm45207091639200 .sect2}
## without

Generally you will always know the instrumentation labels, as they
rarely change. []{#ch14.xhtml#idm45207091637408 primary="grouping"
secondary="without clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091636496
primary="without clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091635792
primary="target labels" secondary="variances in"
data-type="indexterm"}But you do not always know the target labels in
play, as an expression you write might be used by someone else on
metrics originating from different scrape configs, or Prometheus servers
that might also have added in other target labels across a job, such as
an `env` or `cluster` label. []{#ch14.xhtml#idm45207091633888
primary="env labels"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091633152
primary="cluster labels" data-type="indexterm"}You might even add in
such target labels yourself at some point, and it'd be nice not to have
to update all your expressions.

When aggregating metrics you should usually try to preserve such target
labels, and thus you should use the `without` clause when aggregating to
specify the labels you want to remove. []{#ch14.xhtml#idm45207091631456
primary="sum" secondary="without clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091630480
primary="without clause" secondary="sum without"
data-type="indexterm"}For example, the query:

``` {data-type="programlisting"}
sum without(fstype, mountpoint)(node_filesystem_size_bytes)
```

will group the time series, ignoring the `fstype` and `mountpoint`
labels, into three groups:

``` {data-type="programlisting"}
# Group {device="/dev/sda1",instance="localhost:9100",job="node"}
node_filesystem_size_bytes{device="/dev/sda1",fstype="vfat",
    instance="localhost:9100",job="node",mountpoint="/boot/efi"} 100663296

# Group {device="/dev/sda5",instance="localhost:9100",job="node"}
node_filesystem_size_bytes{device="/dev/sda5",fstype="ext4",
    instance="localhost:9100",job="node",mountpoint="/"} 90131324928

# Group {device="tmpfs",instance="localhost:9100",job="node"}
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run"} 826961920
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/lock"} 5242880
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/1000"} 826961920
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/119"} 826961920
```

and the `sum` aggregator will apply within each of these groups, adding
up the values of the time series and returning one sample per group:

``` {data-type="programlisting"}
{device="/dev/sda1",instance="localhost:9100",job="node"} 100663296
{device="/dev/sda5",instance="localhost:9100",job="node"} 90131324928
{device="tmpfs",instance="localhost:9100",job="node"} 2486128640
```

Notice that the `instance` and `job` labels are preserved, as would be
any other labels that had been present. []{#ch14.xhtml#idm45207091623520
primary="alerting"
secondary="preservation of job and instance labels for"
data-type="indexterm"}This is useful because any alerts you created that
included this expression somehow would have additional target labels
like `env` or `cluster` [preserved]{.keep-together}. This provides
context for your alerts and makes them more useful (also useful when
graphing).

The metric name has also been removed, as
[]{#ch14.xhtml#idm45207091620400 primary="node_filesystem_size_bytes"
secondary="aggregation of" data-type="indexterm"}this is an aggregation
of the `node_filesystem_size_bytes` metric rather than the original
metric. When a PromQL operator or function could change the value or
meaning of a time series, the metric name is removed.

It is valid to provide no labels to the `without`. For example:

``` {data-type="programlisting"}
sum without()(node_filesystem_size_bytes)
```

will give you the same result as:

``` {data-type="programlisting"}
node_filesystem_size_bytes
```

with the only difference being the metric name is removed.
:::
:::

::: {.section pdf-bookmark="by" data-type="sect2"}
::: {#ch14.xhtml#by .sect2}
## by

In addition to `without` there is also the `by`
clause.[]{#ch14.xhtml#idm45207091612048 primary="grouping"
secondary="by clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091611040
primary="by clause" data-type="indexterm"} Where `without` specifies the
labels to remove, `by` specifies the labels to keep.
[]{#ch14.xhtml#idm45207091609408 primary="labels"
secondary="specifying to keep using by clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091608464 primary="sum"
secondary="by clause" data-type="indexterm"}Accordingly, some care is
required when using `by` to ensure you don't remove target labels that
you would like to propagate in your alerts or use in your dashboards.
You cannot use both `by` and `without` in the same aggregation.

The query:

``` {data-type="programlisting"}
sum by(job, instance, device)(node_filesystem_size_bytes)
```

will produce the same result as the query in the preceding section using
`without`:

``` {data-type="programlisting"}
{device="/dev/sda1",instance="localhost:9100",job="node"} 100663296
{device="/dev/sda5",instance="localhost:9100",job="node"} 90131324928
{device="tmpfs",instance="localhost:9100",job="node"} 2486128640
```

However, if `instance` or `job` had not been specified, then they
wouldn't have defined the group and would not be in the output.
Generally, you should prefer to use `without` rather than `by` for this
reason.[]{#ch14.xhtml#idm45207091600624 primary="without clause"
secondary="by clause versus"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091599616
primary="by clause" secondary="versus without clause"
secondary-sortas="without" data-type="indexterm"}

There are two cases where you might find `by` more useful. The first is
that unlike `without`, `by` does keep the `__name__` label if told
explicitly. []{#ch14.xhtml#idm45207091596160
primary="sort_desc function" secondary="using by clause"
data-type="indexterm"}This allows you to use expressions like:

``` {data-type="programlisting"}
sort_desc(count by(__name__)({__name__=~".+"}))
```

to investigate how many time series[]{#ch14.xhtml#idm45207091594016
primary="time series" secondary="finding how many have same name"
data-type="indexterm"} have the same metric
names.^[1](#ch14.xhtml#idm45207091592944){#ch14.xhtml#idm45207091592944-marker
data-type="noteref"}^

The second is cases where you do want to remove any labels you do not
know about.[]{#ch14.xhtml#idm45207091591920 primary="labels"
secondary="removing any you don't know about with by clause"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091590848 primary="count"
secondary="by clause" data-type="indexterm"} For example, info metrics,
as discussed in ["Info"](#ch05.xhtml#info_metrics){data-type="xref"},
are expected to add more labels over time. To count how many machines
were running each kernel version, you could use:

``` {data-type="programlisting"}
count by(release)(node_uname_info)
```

which on our single machine test setup returns:

``` {data-type="programlisting"}
{release="4.4.0-101-generic"} 1
```

You can use `sum` with an empty `by`, and can even omit the `by`.
[]{#ch14.xhtml#idm45207091585184 primary="sum" secondary="by clause"
tertiary="using empty by or omitting by" data-type="indexterm"}That is
to say that:

``` {data-type="programlisting"}
sum by()(node_filesystem_size_bytes)
```

and:

``` {data-type="programlisting"}
sum(node_filesystem_size_bytes)
```

are exactly equivalent and will give a result like:

``` {data-type="programlisting"}
{} 92718116864
```

This is a single time series, and that time series has no labels.

If you executed the expression:

``` {data-type="programlisting"}
sum(non_existent_metric)
```

the result would be an instant vector with no time series, which will
show up in the expression browser's Console tab as "*no data*."

::: {data-type="tip"}
###### Tip

If the input to an aggregation operator is an empty instant vector, it
will output an empty instant vector.[]{#ch14.xhtml#idm45207091576720
primary="instant vectors"
secondary="empty, input to aggregation operator" data-type="indexterm"}
Thus, `count by(foo)(non_existent_metric)` will be empty rather than
`0`, as `count` and other aggregators don't have any labels to work
with. `count(non_existent_metric)` is consistent with this, and also
returns an empty instant vector.[]{#ch14.xhtml#idm45207091573760
primary="aggregation operators" secondary="grouping"
startref="ix_aggopsgrp"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091572464 primary="PromQL"
secondary="aggregation operators" startref="ix_PQLaggopgrp"
tertiary="grouping"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091570976
primary="grouping" startref="ix_grpng" data-type="indexterm"}
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Operators" data-type="sect1"}
::: {#ch14.xhtml#idm45207091614656 .sect1}
# Operators

All 11 aggregation operators use the same grouping logic. You can
control this with one of `without` or `by`. What differs between
aggregation operators is what they do with the grouped data.

::: {.section pdf-bookmark="sum" data-type="sect2"}
::: {#ch14.xhtml#idm45207091567632 .sect2}
## sum

`sum` is the most common aggregator; it adds up all the values in a
group and returns that as the value for the
group.[]{#ch14.xhtml#idm45207091565616 primary="sum" secondary="about"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091564640
primary="aggregation operators" secondary="sum"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091563696 primary="PromQL"
secondary="aggregation operators" tertiary="sum" data-type="indexterm"}
For example:

``` {data-type="programlisting"}
sum without(fstype, mountpoint, device)(node_filesystem_size_bytes)
```

would return the []{#ch14.xhtml#idm45207091561168 primary="filesystems"
secondary="returning total filesystem size on each machine"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091560128
primary="rate function" secondary="using before sum with counters"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091559216
primary="counters" secondary="using rate before sum with"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091558256 primary="summary"
secondary="using rate before sum on"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091557296 primary="count"
secondary="using rate before sum on"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091556336
primary="buckets (in histograms)" secondary="using rate before sum on"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091555376
primary="histograms" secondary="buckets"
tertiary="using rate before sum on" data-type="indexterm"}total size of
the filesystems of each of your machines.

When dealing with
counters,^[2](#ch14.xhtml#idm45207091553632){#ch14.xhtml#idm45207091553632-marker
data-type="noteref"}^ it is important that you take a `rate` before
aggregating with `sum`:

``` {data-type="programlisting"}
sum without(device)(rate(node_disk_read_bytes_total[5m]))
```

If you were to take a `sum` across counters directly, the result would
be meaningless, as different counters could have been initialized at
different times depending on when the exporter started, restarted, or
any particular children were first used.
:::
:::

::: {.section pdf-bookmark="count" data-type="sect2"}
::: {#ch14.xhtml#idm45207091548768 .sect2}
## count

The `count` aggregator counts the number of time series in a group, and
returns it as the value for the group.[]{#ch14.xhtml#idm45207091546704
primary="aggregation operators" secondary="count"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091545728 primary="count"
secondary="about" data-type="indexterm"}[]{#ch14.xhtml#idm45207091544784
primary="PromQL" secondary="aggregation operators" tertiary="count"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091543568
primary="without clause" secondary="count without"
data-type="indexterm"} For example:

``` {data-type="programlisting"}
count without(device)(node_disk_read_bytes_total)
```

would return the number of disk devices a machine has. Our machine only
has one disk, so we get:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 1
```

Here it is OK not to use `rate` with a counter, as you care about the
existence of the time series rather than its
value.[]{#ch14.xhtml#idm45207091539520 primary="rate function"
secondary="not using with a counter" data-type="indexterm"}

::: {.section pdf-bookmark="Unique label values" data-type="sect3"}
::: {#ch14.xhtml#idm45207091538352 .sect3}
### Unique label values

You can also use `count` to count how many unique values a label
has.[]{#ch14.xhtml#idm45207091536208 primary="count"
secondary="counting unique label values"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091535216 primary="labels"
secondary="counting unique label values"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091534256 primary="CPUs"
secondary="counting number in each machine" data-type="indexterm"} For
example, to count the number of CPUs in each of your machines, you could
use:

``` {data-type="programlisting"}
count without(cpu)(count without (mode)(node_cpu_seconds_total))
```

The inner
`count`^[3](#ch14.xhtml#idm45207091531696){#ch14.xhtml#idm45207091531696-marker
data-type="noteref"}^ removes the other []{#ch14.xhtml#idm45207091529392
primary="mode labels" secondary="counting without"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091528416
primary="instrumentation labels" secondary="mode"
data-type="indexterm"}instrumentation label, `mode`, returning one time
series per CPU per instance:

``` {data-type="programlisting"}
{cpu="0",instance="localhost:9100",job="node"} 8
{cpu="1",instance="localhost:9100",job="node"} 8
{cpu="2",instance="localhost:9100",job="node"} 8
{cpu="3",instance="localhost:9100",job="node"} 8
```

The outer `count` then returns the number of CPUs that each instance
has:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 4
```

If you didn't want a per-machine breakdown, such as if you were
investigating whether certain labels had high cardinality, you could
[]{#ch14.xhtml#idm45207091523920 primary="by clause"
secondary="count by"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091522944 primary="count"
secondary="count by" data-type="indexterm"}use the `by` modifier to look
at only one label:

``` {data-type="programlisting"}
count(count by(cpu)(node_cpu_seconds_total))
```

which would produce a single sample with no labels, such as:

``` {data-type="programlisting"}
{} 4
```
:::
:::
:::
:::

::: {.section pdf-bookmark="avg" data-type="sect2"}
::: {#ch14.xhtml#idm45207091537760 .sect2}
## avg

The `avg` aggregator returns the average of the
values^[4](#ch14.xhtml#idm45207091517344){#ch14.xhtml#idm45207091517344-marker
data-type="noteref"}^ of the time series in the group as the value for
the group.[]{#ch14.xhtml#idm45207091509344 primary="ln function"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091508624
primary="exp function"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091507952
primary="without clause" secondary="avg without" data-type="indexterm"}
For example:

``` {data-type="programlisting"}
avg without(cpu)(rate(node_cpu_seconds_total[5m]))
```

would give you the average usage of each CPU mode for each Node Exporter
instance with a result such as:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node",mode="idle"} 0.9095948275861836
{instance="localhost:9100",job="node",mode="iowait"} 0.005543103448275879
{instance="localhost:9100",job="node",mode="irq"} 0
{instance="localhost:9100",job="node",mode="nice"} 0.0013620689655172522
{instance="localhost:9100",job="node",mode="softirq"} 0.0001465517241379329
{instance="localhost:9100",job="node",mode="steal"} 0
{instance="localhost:9100",job="node",mode="system"} 0.015836206896552414
{instance="localhost:9100",job="node",mode="user"} 0.06054310344827549
```

This gives you the exact[]{#ch14.xhtml#idm45207091504432 primary="sum"
data-type="indexterm"} same result as:

``` {data-type="programlisting"}
  sum without(cpu)(rate(node_cpu_seconds_total[5m]))
/
  count without(cpu)(rate(node_cpu_seconds_total[5m]))
```

but it is both more succinct and more efficient to use `avg`.

When using `avg`, sometimes you may find that a `NaN` in the input is
causing the entire result to become `NaN`.
[]{#ch14.xhtml#idm45207091499872 primary="NaN (not a number)"
secondary="input to avg operator" data-type="indexterm"}This is because
any floating-point arithmetic that involves `NaN` will have `NaN` as a
result.

You may wonder how to filter out these NaNs in the input, but that is
the wrong question to ask.[]{#ch14.xhtml#idm45207091497424
primary="averages" secondary="attempt to average averages"
data-type="indexterm"} Usually this is due to attempting to average
averages, and one of the denominators of the first averages was
0.^[5](#ch14.xhtml#idm45207091496128){#ch14.xhtml#idm45207091496128-marker
data-type="noteref"}^ Averaging averages is not statistically valid, so
what you should do instead is aggregate using `sum` and then finally
divide, as shown in
["Summary"](#ch13.xhtml#summary_aggregation){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="group" data-type="sect2"}
::: {#ch14.xhtml#idm45207091518928 .sect2}
## group

The `group` aggregator returns 1 for each of the time series in the
group as the value for the group. []{#ch14.xhtml#idm45207091491600
primary="group operator"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091490896
primary="aggregation operators" secondary="group"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091489952 primary="PromQL"
secondary="aggregation operators" tertiary="group"
data-type="indexterm"}For example:

``` {data-type="programlisting"}
count by (instance)(
  group by (fstype,instance) (node_filesystem_files)
)
```

That query would return the number of different filesystem types for
each instance.[]{#ch14.xhtml#idm45207091487504 primary="by clause"
secondary="group by" data-type="indexterm"}

In this case, any aggregation could have worked (`sum`, `count`) in
place of `group`. However, using `group` makes it clear for anyone
reading the query that we are interested in the grouping and the
resulting labels themselves rather than the value produced by the inner
aggregation operator.
:::
:::

::: {.section pdf-bookmark="stddev and stdvar" data-type="sect2"}
::: {#ch14.xhtml#idm45207091484096 .sect2}
## stddev and stdvar

The *standard deviation* is a statistical measure of how spread out a
set of numbers is. []{#ch14.xhtml#idm45207091481760 primary="PromQL"
secondary="aggregation operators" tertiary="stddev and stdvar"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091480432 primary="stddev"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091479760
primary="standard deviation" seealso="stddev"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091478816
primary="aggregation operators" secondary="stddev"
data-type="indexterm"}For example, if you had the numbers `[2,4,6]`,
then the standard deviation would be
`1.633`.^[6](#ch14.xhtml#idm45207091476944){#ch14.xhtml#idm45207091476944-marker
data-type="noteref"}^ The numbers `[3,4,5]` have the same average of
`4`, but a standard deviation of
`0.816`.[]{#ch14.xhtml#idm45207091473936
primary="population standard deviation"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091473136
primary="sample standard deviation" data-type="indexterm"}

The main use of the standard deviation in monitoring is to detect
outliers.[]{#ch14.xhtml#idm45207091472320 primary="outliers, detecting"
data-type="indexterm"} In normally distributed data you would expect
that about 68% of samples would be within one standard deviation of the
mean, and 95% within two standard
deviations.^[7](#ch14.xhtml#idm45207091471552){#ch14.xhtml#idm45207091471552-marker
data-type="noteref"}^ If one instance in a job has a metric several
standard deviations away from the average, that's a good indication that
something is wrong with it.

For example, you could find all instances that were at least two
standard deviations above the average using an expression such as:

``` {data-type="programlisting"}
  some_gauge
> ignoring (instance) group_left()
  (
      avg without(instance)(some_gauge)
    +
      2 * stddev without(instance)(some_gauge)
  )
```

This uses one-to-many vector matching, which will be discussed in
["Many-to-One and
group_left"](#ch15.xhtml#group_left){data-type="xref"}. If your values
are all tightly bunched, then this may return some time series that are
more than two standard deviations away, but still operating normally and
close to the average. You could add an additional filter that the value
has to be at least, say, 20% higher than the average to protect against
this. This is also a rare case where it is OK to take an average of an
average, such as if you applied this to average latency.

The *standard variance* is the []{#ch14.xhtml#idm45207091465712
primary="standard variance" seealso="stdvar"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091464704 primary="stdvar"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091464032
primary="aggregation operators" secondary="stdvar"
data-type="indexterm"}standard deviation
squared^[8](#ch14.xhtml#idm45207091462912){#ch14.xhtml#idm45207091462912-marker
data-type="noteref"}^ and has statistical uses.
:::
:::

::: {.section pdf-bookmark="min and max" data-type="sect2"}
::: {#ch14.xhtml#idm45207091460512 .sect2}
## min and max

The `min` and `max` aggregators return the minimum or maximum value
within a group as the value of the group,
respectively.[]{#ch14.xhtml#idm45207091457792 primary="PromQL"
secondary="aggregation operators" tertiary="min and max"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091456544 primary="min"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091455872 primary="max"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091455200
primary="aggregation operators" secondary="max and min"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091454256
primary="topk operator"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091453584
primary="bottomk operator" data-type="indexterm"} The same grouping
rules apply as elsewhere, so the output time series will have the labels
of the
group.^[9](#ch14.xhtml#idm45207091452656){#ch14.xhtml#idm45207091452656-marker
data-type="noteref"}^ For example:

``` {data-type="programlisting"}
max without(device, fstype, mountpoint)(node_filesystem_size_bytes)
```

will return the size of the biggest filesystem on each instance, which
for us returns:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 90131324928
```

The `max` and `min` aggregators []{#ch14.xhtml#idm45207091447696
primary="NaN (not a number)" secondary="return by max and min"
data-type="indexterm"}will only return `NaN` if all values in a group
are
`NaN`.^[10](#ch14.xhtml#idm45207091445728){#ch14.xhtml#idm45207091445728-marker
data-type="noteref"}^
:::
:::

::: {.section pdf-bookmark="topk and bottomk" data-type="sect2"}
::: {#ch14.xhtml#idm45207091460208 .sect2}
## topk and bottomk

`topk` and `bottomk` are different from the other aggregators discussed
so far in three ways.[]{#ch14.xhtml#idm45207091439360
primary="aggregation operators" secondary="topk and bottomk"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091438384 primary="PromQL"
secondary="aggregation operators" tertiary="topk and bottomk"
data-type="indexterm"} First, the labels of time series they return for
a group are not the labels of the group; second, they can return more
than one time series per group; and third, they take an additional
parameter.

`topk` returns the `k` time series with the biggest values, so for
example:

``` {data-type="programlisting"}
topk without(device, fstype, mountpoint)(2, node_filesystem_size_bytes)
```

would return up to
two^[11](#ch14.xhtml#idm45207091434576){#ch14.xhtml#idm45207091434576-marker
data-type="noteref"}^ time series per group, such as:

``` {data-type="programlisting"}
node_filesystem_size_bytes{device="/dev/sda5",fstype="ext4",
    instance="localhost:9100",job="node",mountpoint="/"} 90131324928
node_filesystem_size_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run"} 826961920
```

As you can see, `topk` returns input time series with all their labels,
including the `__name__` label, which holds the metric name. The result
is also sorted.

`bottomk` is the same as `topk`, except that it returns the `k` time
series with the smallest values rather than the `k` biggest
values.[]{#ch14.xhtml#idm45207091428464 primary="bottomk operator"
data-type="indexterm"} Both aggregators will, where possible, avoid
returning time series with `NaN` values.

There is a gotcha when using these aggregators with the `query_range`
HTTP API endpoint.[]{#ch14.xhtml#idm45207091426208 primary="query_range"
secondary="gotcha when using with topk and bottomk"
data-type="indexterm"} As was discussed in
["query_range"](#ch13.xhtml#query_range){data-type="xref"}, the
evaluation of each step is independent. If you use `topk`, it is
possible that the top time series will change from step to step. So a
`topk(5, some_gauge)` for a `query_range` with 1,000 steps could in the
worst case return 5,000 different time series.

The way to[]{#ch14.xhtml#idm45207091422560 primary="@ (at) modifier"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091421824
primary="at (@) modifier" data-type="indexterm"} handle this is to use
the at (`@`) modifier, as discussed in ["At
Modifier"](#ch13.xhtml#at_modifier){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="quantile" data-type="sect2"}
::: {#ch14.xhtml#idm45207091441104 .sect2}
## quantile

The `quantile` aggregator returns the specified quantile of the values
of the group as the group's return
value.[]{#ch14.xhtml#idm45207091417296 primary="quantile operator"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091416560
primary="aggregation operators" secondary="quantile"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091415616 primary="PromQL"
secondary="aggregation operators" tertiary="quantile"
data-type="indexterm"} As with `topk`, `quantile` takes a parameter.

So, for example, if we wanted to know across the different CPUs in each
of our machines what the 90th percentile of
[]{#ch14.xhtml#idm45207091412992 primary="without clause"
secondary="quantile without"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091412016 primary="CPUs"
secondary="calculating 90th percentile of system mode CPU usage"
data-type="indexterm"}the system mode CPU usage is, we could use:

``` {data-type="programlisting"}
quantile without(cpu)(0.9, rate(node_cpu_seconds_total{mode="system"}[5m]))
```

which produces a result like:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node",mode="system"} 0.024558620689654007
```

This means that 90% of our CPUs are spending at least 0.02 seconds per
second in the system mode. This would be a more useful query if we had
tens of CPUs in our machine, rather than the four it actually
has.[]{#ch14.xhtml#idm45207091408544 primary="percentiles"
secondary="median, 25th, and 75th" data-type="indexterm"}

In addition to the mean, you could use `quantile` to show the median,
25th, and 75th
percentiles^[12](#ch14.xhtml#idm45207091406608){#ch14.xhtml#idm45207091406608-marker
data-type="noteref"}^ on your graphs.[]{#ch14.xhtml#idm45207091405408
primary="quartiles (1st and 3rd)" data-type="indexterm"} For example,
for process CPU usage the expressions would be:

``` {data-type="programlisting"}
# average, arithmetic mean
avg without(instance)(rate(process_cpu_seconds_total[5m]))

# 0.25 quantile, 25th percentile, 1st or lower quartile
quantile without(instance)(0.25, rate(process_cpu_seconds_total[5m]))

# 0.5 quantile, 50th percentile, 2nd quartile, median
quantile without(instance)(0.5, rate(process_cpu_seconds_total[5m]))

# 0.75 quantile, 75th percentile, 3rd or upper quartile
quantile without(instance)(0.75, rate(process_cpu_seconds_total[5m]))
```

This would give you a sense of how your different instances for a job
are behaving, without having to graph each instance individually. This
allows you to keep your dashboards readable as the number of underlying
instances grows. Personally we find that per-instance graphs break down
somewhere around three to five
instances.[]{#ch14.xhtml#idm45207091403392 primary="quantile_over_time"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091402688
primary="histogram_quantile function" data-type="indexterm"}

```{=html}
<aside data-type="sidebar" epub:type="sidebar">
```
::: {#ch14.xhtml#three_quantiles .sidebar}
# quantile, histogram_quantile, and quantile_over_time

As you may have noticed by now, there is more than one PromQL function
or operator with quantile in the name.

The `quantile` aggregator works across an instant vector
[]{#ch14.xhtml#idm45207091399040 primary="instant vectors"
secondary="quantile working across in aggregation group"
data-type="indexterm"}in an aggregation group.

The `quantile_over_time` function works across a single time series at a
time in a range vector.

The `histogram_quantile` function works across the buckets of one
histogram metric child at a time in an instant vector.
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="count_values" data-type="sect2"}
::: {#ch14.xhtml#idm45207091395776 .sect2}
## count_values

The final aggregation operator is `count_values`. Like `topk` it takes a
parameter and can return more than one time series from a
group.[]{#ch14.xhtml#idm45207091392704 primary="count_values operator"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091392000
primary="frequency histograms"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091391328
primary="histograms" secondary="frequency histogram"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091390384
primary="aggregation operators" secondary="count_values"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091389440 primary="PromQL"
secondary="aggregation operators" tertiary="count_values"
data-type="indexterm"} What it does is build a *frequency histogram* of
the values of the time series in the group, with the count of each value
as the value of the output time series and the original value as a new
label.

That's a bit of a mouthful, so we will show you an example. Say you had
a time series called `software_version` with the following values:

``` {data-type="programlisting"}
software_version{instance="a",job="j"} 7
software_version{instance="b",job="j"} 4
software_version{instance="c",job="j"} 8
software_version{instance="d",job="j"} 4
software_version{instance="e",job="j"} 7
software_version{instance="f",job="j"} 4
```

If you evaluated the query:

``` {data-type="programlisting"}
count_values without(instance)("version", software_version)
```

on these time series, you would get the result:

``` {data-type="programlisting"}
{job="j",version="7"} 2
{job="j",version="8"} 1
{job="j",version="4"} 3
```

There were two time series in the group with a value of `7`, so a time
series with a `version="7"` plus the group labels was returned with the
value `2`. The result is similar for the other time series.

There is no bucketing involved when the frequency histogram is created;
the exact values of the time series are used. Thus this is only really
useful with integer values and where there will not be too many unique
values.

This is most useful with version
numbers,^[13](#ch14.xhtml#idm45207091380432){#ch14.xhtml#idm45207091380432-marker
data-type="noteref"}^ or with the number of objects of some type that
each instance of your application sees. If you have too many versions
deployed at once, or different applications are continuing to see
different numbers of objects, something might be stuck somewhere.

`count_values` can be combined with `count` to calculate the number of
unique values for a given aggregation
group.[]{#ch14.xhtml#idm45207091377664 primary="count"
secondary="using count_values with"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091376688
primary="count_values operator" secondary="use with count"
data-type="indexterm"} For example, the number of versions of software
that are deployed can be calculated with:

``` {data-type="programlisting"}
count without(version)(
  count_values without(instance)("version", software_version)
)
```

which in this case would return:

``` {data-type="programlisting"}
{job="j"} 3
```

You could also combine `count_values` with `count` in the other
direction; for example, to see how many of your machines had how many
disk devices:

``` {data-type="programlisting"}
count_values without(instance)(
  "devices",
  count without(device) (node_disk_io_now)
)
```

In our case we have one machine with five disk devices:

``` {data-type="programlisting"}
{devices="5",job="node"} 1
```

Now that you understand aggregators, we will look at binary operators,
like addition and subtraction, and how vector matching
works.[]{#ch14.xhtml#idm45207091369472 primary="aggregation operators"
startref="ix_aggops"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091368496 primary="PromQL"
secondary="aggregation operators" startref="ix_PQLaggop"
data-type="indexterm"}
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch14.xhtml#idm45207091592944-marker)^ This is potentially an
expensive query as it touches every active time series; use it
carefully.

^[2](#ch14.xhtml#idm45207091553632-marker)^ Including the `_sum`,
`_count`, and `_bucket` of histograms and summary metrics.

^[3](#ch14.xhtml#idm45207091531696-marker)^ The inner aggregation does
not have to be `count`; anything that returns the same set of time
series, such as `sum`, would also work. This is because the outer
`count` ignores the values of these time series.

^[4](#ch14.xhtml#idm45207091517344-marker)^ Technically it is called an
*arithmetic mean*. []{#ch14.xhtml#idm45207091516192
primary="arithmetic mean"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091515456
primary="avg operator"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091514784
primary="aggregation operators" secondary="avg"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091513840 primary="PromQL"
secondary="aggregation operators" tertiary="avg"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091512624 primary="mean"
data-type="indexterm"}[]{#ch14.xhtml#idm45207091511952
primary="geometric mean" data-type="indexterm"}In the unlikely event you
need a *geometric mean*, the `ln` and `exp` functions combined with the
`avg` aggregator can be used to calculate that.

^[5](#ch14.xhtml#idm45207091496128-marker)^ This is as `1 / 0 = NaN`.

^[6](#ch14.xhtml#idm45207091476944-marker)^ Prometheus uses the
*population standard deviation* rather than the *sample standard
deviation*, as you will usually be looking at all the values you are
interested in rather than a random subset.

^[7](#ch14.xhtml#idm45207091471552-marker)^ For nonnormally
[]{#ch14.xhtml#idm45207091470928 primary="Chebyshev's inequality"
data-type="indexterm"}distributed data, *Chebyshev's inequality*
provides a weaker bound.

^[8](#ch14.xhtml#idm45207091462912-marker)^ If the exponentiation
operator had existed at the time we were adding `stdvar` and `stddev`,
then `stdvar` would probably not have been added.

^[9](#ch14.xhtml#idm45207091452656-marker)^ If you want the input time
series returned, use `topk` or `bottomk`.

^[10](#ch14.xhtml#idm45207091445728-marker)^ In floating-point math, any
comparison with `NaN` always returns false.
[]{#ch14.xhtml#idm45207091444480 primary="floating-point math"
secondary="comparisons with NaN" data-type="indexterm"}Aside from
causing oddities such as `NaN != NaN` returning false, a naive
implementation of `min` and `max` would (and once did) get stuck on a
`NaN` if it was the first value examined.

^[11](#ch14.xhtml#idm45207091434576-marker)^ The `k` is `2` in this
case.

^[12](#ch14.xhtml#idm45207091406608-marker)^ Also known as the 1st and
3rd *quartiles*.

^[13](#ch14.xhtml#idm45207091380432-marker)^ For versions that cannot be
represented as floating-point values, you can use an info metric, as
discussed in ["Info"](#ch05.xhtml#info_metrics){data-type="xref"}.
:::
:::
:::

[]{#ch15.xhtml}

::: {#ch15.xhtml#sbo-rt-content}
::: {#ch15.xhtml#promql_operators_chapter .chapter}
# [Chapter 15. ]{.label}Binary Operators

You will want to do more with your metrics than simply aggregate them,
which is where the *binary operators* come in.[]{#ch15.xhtml#ix_PQLbinop
primary="PromQL" secondary="binary operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_binop primary="binary operators"
data-type="indexterm"} Binary operators are operators that take two
operands,^[1](#ch15.xhtml#idm45207091363104){#ch15.xhtml#idm45207091363104-marker
data-type="noteref"}^ such as the addition and equality
operators.[]{#ch15.xhtml#idm45207091360912 primary="unary operators"
data-type="indexterm"}

Binary operators in allow for more than simple arithmetic on instant
vectors; you can also apply a binary operator to two instant vectors
with grouping based on labels. []{#ch15.xhtml#idm45207091359632
primary="instant vectors" secondary="binary operators applied to"
data-type="indexterm"}This is where the real power of PromQL comes out,
allowing classes of analysis that few other metrics systems offer.

PromQL has three sets of binary operators: arithmetic operators,
comparison operations, and logical operators. This chapter will show you
how to use them.

::: {.section pdf-bookmark="Working with Scalars" data-type="sect1"}
::: {#ch15.xhtml#idm45207091357920 .sect1}
# Working with Scalars

In addition to instant vectors and range vectors, there is another type
of value []{#ch15.xhtml#ix_PQLbinopsclr primary="PromQL"
secondary="binary operators" tertiary="working with scalars"
data-type="indexterm"}[]{#ch15.xhtml#ix_sclr primary="scalars"
secondary="working with"
data-type="indexterm"}[]{#ch15.xhtml#ix_binopsclr
primary="binary operators" secondary="working with scalars"
data-type="indexterm"}known as a
*scalar*.^[2](#ch15.xhtml#idm45207091351520){#ch15.xhtml#idm45207091351520-marker
data-type="noteref"}^ Scalars are single numbers with no dimensionality.
[]{#ch15.xhtml#idm45207091348128 primary="instant vectors"
secondary="{} 0" data-type="indexterm"}For example, `0` is a scalar with
the value zero, while `{} 0` is an instant vector containing a single
sample with no labels and the value
zero.^[3](#ch15.xhtml#idm45207091346128){#ch15.xhtml#idm45207091346128-marker
data-type="noteref"}^

::: {.section pdf-bookmark="Arithmetic Operators" data-type="sect2"}
::: {#ch15.xhtml#idm45207091345008 .sect2}
## Arithmetic Operators

You can use scalars in arithmetic with an instant vector to change the
values in the instant vector.[]{#ch15.xhtml#ix_binopsclrarith
primary="binary operators" secondary="working with scalars"
tertiary="arithmetic operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_arith
primary="arithmetic operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_sclrmath primary="scalars"
secondary="working with" tertiary="arithmetic operators"
data-type="indexterm"} For example:

``` {data-type="programlisting"}
process_resident_memory_bytes / 1024
```

would return:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 21376
{instance="localhost:9100",job="node"} 13316
```

which is the process memory usage, in
kilobytes.^[4](#ch15.xhtml#idm45207091336768){#ch15.xhtml#idm45207091336768-marker
data-type="noteref"}^ You will note that the division operator was
applied to all time series in the instant vector returned by the
[`process_resident_memory_bytes`]{.keep-together} selector, and that the
metric name was removed as it is no longer the metric
`process_resident_memory_bytes`.

::: {.note data-type="note"}
###### Note

Even when you are using arithmetic operators in a way that doesn't
change the value, the metric name will still be removed for consistency.
For example, the result of `some_gauge + 0` will not have a metric name.
:::

All six arithmetic operations work similarly, with the semantics you'd
expect from other programming languages.
[]{#ch15.xhtml#idm45207091331968 primary="arithmetic operators"
secondary="summary of" data-type="indexterm"}They are:

-   `+` addition

-   `-` subtraction

-   `*` multiplication

-   `/` division

-   `%` modulo

-   `^` exponentiation

The *modulo* operator is a floating-point modulo and can return
noninteger results if you provide it with noninteger
input.[]{#ch15.xhtml#idm45207091322256 primary="modulo operator (%)"
data-type="indexterm"} For example:

``` {data-type="programlisting"}
5 % 1.5
```

will return:

``` {data-type="programlisting"}
0.5
```

As this example demonstrates, you can also use binary arithmetic
operators when both operands are scalars. The result will be a scalar.
This is mostly useful for readability, as it is much easier to
understand the intent of `(1024 * 1024 * 1024)` than it is `1073741824`.

In addition, you can put the scalar operand on the left side of the
operator and an instant vector on the right, so for example:

``` {data-type="programlisting"}
1e9 - process_resident_memory_bytes
```

would subtract the process memory from a billion.

You can also[]{#ch15.xhtml#idm45207091316048 primary="binary operators"
secondary="working with scalars" startref="ix_binopsclrarith"
tertiary="arithmetic operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091314496 primary="scalars"
secondary="working with" startref="ix_sclrmath"
tertiary="arithmetic operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091313008
primary="arithmetic operators" startref="ix_arith"
data-type="indexterm"} use arithmetic operators with instant vectors on
both sides, which is covered in ["Vector
Matching"](#ch15.xhtml#vector_matching){data-type="xref"}.
:::
:::

::: {.section pdf-bookmark="Trigonometric Operator" data-type="sect2"}
::: {#ch15.xhtml#idm45207091344416 .sect2}
## Trigonometric Operator

The `atan2` operator returns the []{#ch15.xhtml#idm45207091309328
primary="scalars" secondary="working with"
tertiary="trigonometric operator atan2"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091308032
primary="trigonometric operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091307360
primary="binary operators" secondary="working with scalars"
tertiary="trigonometric operator atan2"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091306128
primary="atan2 operator" data-type="indexterm"}arc tangent of the
division of two vectors, using the signs of the two to determine the
quadrant of the return value:

``` {data-type="programlisting"}
x atan2 y
```

This operator allows you to execute `atan2` on two vectors using vector
matching, which isn't available with normal functions. It acts in the
same manner as arithmetic operators (`+`, `-`, `*`, ...​).
:::
:::

::: {.section pdf-bookmark="Comparison Operators" data-type="sect2"}
::: {#ch15.xhtml#idm45207091301952 .sect2}
## Comparison Operators

The *comparison operators* are as follows, with
[]{#ch15.xhtml#ix_binopsclrcmp primary="binary operators"
secondary="working with scalars" tertiary="comparison operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_cmpops
primary="comparison operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_sclrcmp primary="scalars"
secondary="working with" tertiary="comparison operators"
data-type="indexterm"}the usual meanings:

-   `==` equals

-   `!=` not equals

-   `>` greater than

-   `<` less than

-   `>=` greater than or equal to

-   `<=` less than or equal to

What is a little different is that the comparison operators in PromQL
are *filtering*. []{#ch15.xhtml#idm45207091287280 primary="filtering"
data-type="indexterm"}That is to say that if you had the samples:

``` {data-type="programlisting"}
process_open_fds{instance="localhost:9090",job="prometheus"} 14
process_open_fds{instance="localhost:9100",job="node"} 7
```

and used an instant vector in a comparison with a scalar, such as in the
expression:

``` {data-type="programlisting"}
process_open_fds > 10
```

then you would get the result:

``` {data-type="programlisting"}
process_open_fds{instance="localhost:9090",job="prometheus"}  14
```

As the value can't change, the metric name has been preserved. When
comparing a scalar and an instant vector, it doesn't matter which side
each is on; it is always elements of the instant vector that are
returned.

::: {.note data-type="note"}
###### Note

As PromQL deals with floating-point numbers, some care is required when
using `==` and `!=`. []{#ch15.xhtml#idm45207091280624
primary="floating-point numbers"
secondary="using equality comparisons with"
data-type="indexterm"}Floating-point calculations can produce results
that are very slightly different depending on exactly what the values
are and in what order the operations are [performed]{.keep-together}.

If you want to do equality []{#ch15.xhtml#idm45207091278320
primary="epsilon" data-type="indexterm"}on noninteger values, it is
better to instead check that their difference is less than some small
number which is called an *epsilon*. For example, you could do:

``` {data-type="programlisting"}
(some_gauge - 1) < 1e-6 > -1e-6
```

to check if a gauge has a value of 1 allowing for inaccuracy of one in a
million.
:::

You cannot do a filtering comparison between two scalars, as to be
consistent with arithmetic operations between two scalars it'd have to
return a scalar. This doesn't allow for filtering, as there's no way to
have an empty scalar like you can have an empty instant vector.

::: {.section pdf-bookmark="bool modifier" data-type="sect3"}
::: {#ch15.xhtml#bool .sect3}
### bool modifier

Filtering comparisons are primarily useful in
[]{#ch15.xhtml#idm45207091272752 primary="filtering"
secondary="bool modifier and"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091271776
primary="bool modifier"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091271104
primary="comparison operators" secondary="bool modifier and"
data-type="indexterm"}alerting rules, as discussed in
[Chapter 18](#ch18.xhtml#alerting_rules_chapter){data-type="xref"}, and
generally to be avoided
elsewhere.^[5](#ch15.xhtml#idm45207091269120){#ch15.xhtml#idm45207091269120-marker
data-type="noteref"}^ We will show you why.

Continuing on from the preceding example, say you wanted to see how many
of your processes for each job had more than 10 open file descriptors.
The obvious way to do this would be:

``` {data-type="programlisting"}
count without(instance)(process_open_fds > 10)
```

which would return the result:

``` {data-type="programlisting"}
{job="prometheus"}  1
```

This correctly indicates that there is 1 Prometheus process with more
than 10 open file descriptions. It does not report that the Node
Exporter has zero such processes. This is can be a subtle gotcha because
as long as one time series is not filtered away, everything seems to be
OK.

What you need is some way to do the comparison but not have it filter.
This is what the `bool` modifier does; for each comparison it returns a
`0` for `false` or a `1` for `true`.

For example:

``` {data-type="programlisting"}
process_open_fds > bool 10
```

will return:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 1
{instance="localhost:9100",job="node"} 0
```

which as expected has one output sample per sample in the input instant
vector.

From there you can `sum` up to get the number of processes for each job
that have more than 10 open file descriptors:

``` {data-type="programlisting"}
sum without(instance)(process_open_fds > bool 10)
```

which produces the result you originally wanted:

``` {data-type="programlisting"}
{job="prometheus"} 1
{job="node"} 0
```

You could use a similar approach to find the proportion of machines with
more than four disk devices:

``` {data-type="programlisting"}
avg without(instance)(
  count without(device)(node_disk_io_now) > bool 4
)
```

This works by first using a `count` aggregation to find the number of
disks reported by each Node Exporter, then seeing how many have more
than four, and finally averaging across machines to get the proportion.
The trick here is that the values returned by the `bool` modifier are
all `0` and `1`, so the count is the total number of machines, and the
sum is the number of machines meeting the criteria. The `avg` is the
count divided by the sum, giving you a ratio or proportion.

The `bool` modifier is the only []{#ch15.xhtml#idm45207091251056
primary="bool modifier" secondary="using to compare scalars"
data-type="indexterm"}way you can compare scalars, as:

``` {data-type="programlisting"}
42 <= bool 13
```

will return:

``` {data-type="programlisting"}
0
```

where the `0` indicates `false`.[]{#ch15.xhtml#idm45207091246736
primary="scalars" secondary="working with" startref="ix_sclrcmp"
tertiary="comparison operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091245184
primary="binary operators" secondary="working with scalars"
startref="ix_binopsclrcmp" tertiary="comparison operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091243696
primary="comparison operators" startref="ix_cmpops"
data-type="indexterm"}
:::
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Vector Matching" data-type="sect1"}
::: {#ch15.xhtml#vector_matching .sect1}
# Vector Matching

Using operators between scalars and instant vectors will cover many of
your needs, but using operators between two instant vectors is where
PromQL's power really starts to shine.[]{#ch15.xhtml#idm45207091241424
primary="PromQL" secondary="binary operators" startref="ix_PQLbinopsclr"
tertiary="working with scalars"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091239872
primary="binary operators" secondary="working with scalars"
startref="ix_binopsclr"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091238656 primary="scalars"
secondary="working with" startref="ix_sclr"
data-type="indexterm"}[]{#ch15.xhtml#ix_PQLbinopvec primary="PromQL"
secondary="binary operators" tertiary="vector matching"
data-type="indexterm"}[]{#ch15.xhtml#ix_vecmtch
primary="vector matching"
data-type="indexterm"}[]{#ch15.xhtml#ix_binopvec
primary="binary operators" secondary="vector matching"
data-type="indexterm"}

When you have a scalar and an instant vector, it is obvious that the
scalar can be applied to each sample in the vector. With two instant
vectors, which samples should apply to which other samples?
[]{#ch15.xhtml#idm45207091233408 primary="instant vectors"
secondary="matching" see="vector matching" data-type="indexterm"}This
matching of the instant vectors is known as *vector matching*.

::: {.section pdf-bookmark="One-to-One" data-type="sect2"}
::: {#ch15.xhtml#idm45207091231456 .sect2}
## One-to-One

In the simplest cases there will be a one-to-one mapping between your
two vectors.[]{#ch15.xhtml#idm45207091229440 primary="vector matching"
secondary="one-to-one"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091228464
primary="binary operators" secondary="vector matching"
tertiary="one-to-one"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091227248
primary="one-to-one vector matching" data-type="indexterm"} Say that you
had the following samples:

``` {data-type="programlisting"}
process_open_fds{instance="localhost:9090",job="prometheus"} 14
process_open_fds{instance="localhost:9100",job="node"} 7
process_max_fds{instance="localhost:9090",job="prometheus"} 1024
process_max_fds{instance="localhost:9100",job="node"} 1024
```

Then when you evaluated the expression:

``` {data-type="programlisting"}
  process_open_fds
/
  process_max_fds
```

you would get the result:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 0.013671875
{instance="localhost:9100",job="node"} 0.0068359375
```

What has happened here is that samples with exactly the same labels,
except for the metric name in the label `__name__`, were matched
together. That is to say that the two samples with the labels
`{instance="localhost:9090",job="prometheus"}` got matched together, and
the two samples with the labels `{instance="localhost:9100",job="node"}`
got matched together.

In this case there was a perfect match, with each sample on both sides
of the operator being matched. If a sample on one side had no match on
the other side, then it would not be present in the result, as binary
operators need two operands.

::: {data-type="tip"}
###### Tip

If a binary operator returns an empty instant vector when you were
expecting a result, it is probably because the labels of the samples in
the operands don't match. []{#ch15.xhtml#idm45207091219712
primary="instant vectors" secondary="empty, returned by binary operator"
data-type="indexterm"}This is often due to a label that is present on
one side of the operator but not the other.
:::

Sometimes you will want to match two instant vectors whose labels do not
quite match. Similar to how aggregation allows you to specify which
labels matter, as discussed in
["Grouping"](#ch14.xhtml#grouping){data-type="xref"}, vector matching
also has clauses controlling which labels are
considered.[]{#ch15.xhtml#idm45207091216896 primary="ignoring clause"
data-type="indexterm"}

You can use the `ignoring` clause to ignore certain labels when
matching, similar to how `without` works for aggregation. Say you were
working with `node_cpu_​`[`seconds_total`]{.keep-together}, which has
`cpu` and `mode` as instrumentation labels, and wanted to know what
proportion of time was being spent in the `idle` mode for each instance.
You could use the expression:

``` {data-type="programlisting"}
  sum without(cpu)(rate(node_cpu_seconds_total{mode="idle"}[5m]))
/ ignoring(mode)
  sum without(mode, cpu)(rate(node_cpu_seconds_total[5m]))
```

This will give you a result such as:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node"} 0.8423353718871361
```

Here the[]{#ch15.xhtml#idm45207091209104 primary="mode labels"
data-type="indexterm"} first `sum` produces an instant vector with a
`mode="idle"` label, whereas the second `sum` produces an instant vector
with no `mode` label. Usually vector matching will fail to match the
samples, but with `ignoring(mode)` the `mode` label is discarded when
the vectors are being grouped, and matching succeeds. As the `mode`
label was not in the match group, it is not in the
output.^[6](#ch15.xhtml#idm45207091205152){#ch15.xhtml#idm45207091205152-marker
data-type="noteref"}^

::: {data-type="tip"}
###### Tip

You can tell the preceding expression is correct in terms of vector
matching by inspection, without having to know anything about the
underlying time series. The removal of `cpu` is balanced on both sides,
and `ignoring(mode)` handles one side having a `mode` and the other not.

This can be trickier when there are different time series with different
labels in play, but looking at expressions in terms of how the labels
flow is a handy way for you to spot errors.
:::

The `on` clause allows you to consider only the labels you provide,
similar to how `by` works for
aggregation.[]{#ch15.xhtml#idm45207091199136 primary="on clause"
data-type="indexterm"} The expression:

``` {data-type="programlisting"}
  sum by(instance, job)(rate(node_cpu_seconds_total{mode="idle"}[5m]))
/ on(instance, job)
  sum by(instance, job)(rate(node_cpu_seconds_total[5m]))
```

will produce the same result as the previous
expression,^[7](#ch15.xhtml#idm45207091197008){#ch15.xhtml#idm45207091197008-marker
data-type="noteref"}^ but as with `by`, the `on` clause has the
disadvantage that you need to know all labels that are currently on the
time series or that may be present in the future in other contexts.

The value that is []{#ch15.xhtml#idm45207091193456
primary="arithmetic operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091192720
primary="comparison operators" data-type="indexterm"}returned for the
arithmetic operators is the result of the calculation, but you may be
wondering what happens for the comparison operators when there are two
instant vectors. The answer is that the value from the lefthand side is
returned. For example, the expression:

``` {data-type="programlisting"}
  process_open_fds
>
  (process_max_fds * .5)
```

will return for you the value of `process_open_fds` for all instances
whose open file descriptors are more than halfway to the
maximum.^[8](#ch15.xhtml#idm45207091190320){#ch15.xhtml#idm45207091190320-marker
data-type="noteref"}^

If you had instead used:

``` {data-type="programlisting"}
  (process_max_fds * .5)
<
  process_open_fds
```

you would get half the maximum file descriptors as the return value.
While the result will have the same labels, this value might be
semantically less useful when
alerting^[9](#ch15.xhtml#idm45207091187808){#ch15.xhtml#idm45207091187808-marker
data-type="noteref"}^ or when used in a dashboard! In general, a current
value is more informative than the limit, so you should try to structure
your math so that the most interesting number is on the lefthand side of
a comparison.
:::
:::

::: {.section pdf-bookmark="Many-to-One and group_left" data-type="sect2"}
::: {#ch15.xhtml#group_left .sect2}
## Many-to-One and group_left

If you were to remove the matcher on `mode` from the
preceding[]{#ch15.xhtml#ix_vecmtchmtoo primary="vector matching"
secondary="many-to-one and group_left"
data-type="indexterm"}[]{#ch15.xhtml#ix_mnyone
primary="many-to-one vector matching"
data-type="indexterm"}[]{#ch15.xhtml#ix_binopvecmtoone
primary="binary operators" secondary="vector matching"
tertiary="many-to-one and group_left" data-type="indexterm"} section and
try to evaluate:

``` {data-type="programlisting"}
  sum without(cpu)(rate(node_cpu_seconds_total[5m]))
/ ignoring(mode)
  sum without(mode, cpu)(rate(node_cpu_seconds_total[5m]))
```

you would get the error:

``` {data-type="programlisting"}
multiple matches for labels:
  many-to-one matching must be explicit (group_left/group_right)
```

This []{#ch15.xhtml#ix_grplft primary="group_left"
data-type="indexterm"}is because the samples no longer match one-to-one,
as there are multiple samples with different mode labels on the lefthand
side for each sample on the righthand side. This can be a subtle failure
mode, as a time series may appear later on that breaks your expression.
[]{#ch15.xhtml#idm45207091176240 primary="mode labels"
data-type="indexterm"}You can see that this is a potential issue, as
looking at the label flow there's nothing restricting the `mode` label
to one potential
value^[10](#ch15.xhtml#idm45207091174880){#ch15.xhtml#idm45207091174880-marker
data-type="noteref"}^ on the lefthand side.

Errors like this are usually due to incorrectly written expressions, so
PromQL does not attempt to do anything smart by default. Instead, you
must specifically request that you want to do many-to-one matching using
the `group_left` modifier.

`group_left` lets you specify that there can be multiple matching
samples in the group of the lefthand
operand.^[11](#ch15.xhtml#idm45207091171856){#ch15.xhtml#idm45207091171856-marker
data-type="noteref"}^ For example:

``` {data-type="programlisting"}
  sum without(cpu)(rate(node_cpu_seconds_total[5m]))
/ ignoring(mode) group_left
  sum without(mode, cpu)(rate(node_cpu_seconds_total[5m]))
```

will produce one output sample for each different `mode` label within
each group on the lefthand side:

``` {data-type="programlisting"}
{instance="localhost:9100",job="node",mode="irq"} 0
{instance="localhost:9100",job="node",mode="nice"} 0
{instance="localhost:9100",job="node",mode="softirq"} 0.00005226389784152013
{instance="localhost:9100",job="node",mode="steal"} 0
{instance="localhost:9100",job="node",mode="system"} 0.01720353303949279
{instance="localhost:9100",job="node",mode="user"} 0.10345203045243238
{instance="localhost:9100",job="node",mode="idle"} 0.8608691486211044
{instance="localhost:9100",job="node",mode="iowait"} 0.01842302398912871
```

`group_left` always takes all of its labels from samples of your operand
on the lefthand side. This ensures that the extra labels that are on the
left side that require this to be many-to-one vector matching are
preserved.^[12](#ch15.xhtml#idm45207091167440){#ch15.xhtml#idm45207091167440-marker
data-type="noteref"}^

This is much easier than having to run a one-to-one expression with a
matcher for each potential `mode` label: `group_left` does it all for
you in one expression. You can use this approach to determine the
proportion each label value within a metric represents of the whole, as
shown in the preceding example, or to compare a metric from a leader of
a cluster against the replicas.

There is another use for `group_left`---adding labels from info metrics
to other metrics from a target. Instrumentation with info metrics was
covered in ["Info"](#ch05.xhtml#info_metrics){data-type="xref"}. The
role of info metrics is to allow you to provide labels that would be
useful for a target or metric to have but that would clutter up the
metric if you were to use it as a normal
label.[]{#ch15.xhtml#idm45207091163520 primary="prometheus_build_info"
data-type="indexterm"}

The `prometheus_build_info` metric, for example, provides you with build
information from Prometheus:

``` {data-type="programlisting"}
prometheus_build_info{branch="HEAD",goversion="go1.10",
    instance="localhost:9090",job="prometheus",
    revision="bc6058c81272a8d938c05e75607371284236aadc",version="2.2.1"}
```

You can join this with metrics []{#ch15.xhtml#idm45207091160672
primary="up" data-type="indexterm"}such as `up`:

``` {data-type="programlisting"}
  up
* on(instance) group_left(version)
  prometheus_build_info
```

which will produce a result like:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus",version="2.2.1"} 1
```

You can[]{#ch15.xhtml#idm45207091156624 primary="version labels"
data-type="indexterm"} see that the `version` label has been copied over
from the righthand operand to the lefthand operand as was requested by
`group_left(version)`, in addition to returning all the labels from the
lefthand operand as `group_left` usually does. [You can]{.keep-together}
specify as many labels as you like to `group_left`, but usually it's
only one or
two.^[13](#ch15.xhtml#idm45207091153088){#ch15.xhtml#idm45207091153088-marker
data-type="noteref"}^ This approach works no matter how many
instrumentation labels the lefthand side has, as the vector matching is
many-to-one.

The preceding[]{#ch15.xhtml#idm45207091151616 primary="on clause"
data-type="indexterm"} expression used `on(instance)`, which relies on
each `instance` label only being used for one target within your
Prometheus.[]{#ch15.xhtml#idm45207091149888 primary="instance labels"
secondary="on clause with" data-type="indexterm"} While this is often
the case, it isn't always, so you may also need to add other labels such
as `job` to the `on` clause.

`prometheus_build_info` applies to a whole target. There are also
info-style^[14](#ch15.xhtml#idm45207091146896){#ch15.xhtml#idm45207091146896-marker
data-type="noteref"}^ metrics []{#ch15.xhtml#idm45207091145696
primary="child metrics" secondary="of node_hwmon_sensor_label"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091144752
primary="node_hwmon_sensor_label"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091144080
primary="sensor labels" data-type="indexterm"}such as
`node_hwmon_sensor_label` mentioned in ["Hwmon
Collector"](#ch07.xhtml#hwmon){data-type="xref"} that apply to children
of a different metric:

``` {data-type="programlisting"}
node_hwmon_sensor_label{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_0",sensor="temp2"} 1
node_hwmon_sensor_label{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_1",sensor="temp3"} 1

node_hwmon_temp_celsius{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",sensor="temp1"} 42
node_hwmon_temp_celsius{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",sensor="temp2"} 42
node_hwmon_temp_celsius{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",sensor="temp3"} 41
```

The `node_hwmon_sensor_label` metric has children that match with some
(but not all) of the time series in `node_hwmon_temp_celsius`. In this
case you know that there is only one additional label (which is called
`label`), so you can use `ignoring` with `group_left` to add this
[]{#ch15.xhtml#idm45207091138464 primary="ignoring clause"
secondary="using with group_left" data-type="indexterm"}label to the
`node_hwmon_temp_celsius` samples:

``` {data-type="programlisting"}
  node_hwmon_temp_celsius
* ignoring(label) group_left(label)
  node_hwmon_sensor_label
```

which will produce results such as:

``` {data-type="programlisting"}
{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_0",sensor="temp2"} 42
{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_1",sensor="temp3"} 41
```

Notice that there is no sample with `sensor="temp1"` as there was no
such sample in `node_hwmon_sensor_label` (how to match sparse instant
vectors will be covered in ["or
operator"](#ch15.xhtml#or){data-type="xref"}).

There is also a `group_right` modifier that works in the same way as
`group_left` except that the one and the many sides are switched, with
the many side now being your operand on the righthand
side.[]{#ch15.xhtml#idm45207091131168 primary="group_right"
data-type="indexterm"} Any labels you specify in the `group_right`
modifier are copied from the left to the right. For the sake of
consistency, you should prefer
`group_left`.[]{#ch15.xhtml#idm45207091129424 primary="group_left"
startref="ix_grplft"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091128416
primary="vector matching" secondary="many-to-one and group_left"
startref="ix_vecmtchmtoo"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091127232
primary="binary operators" secondary="vector matching"
startref="ix_binopvecmtoone" tertiary="many-to-one and group_left"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091125728
primary="many-to-one vector matching" startref="ix_mnyone"
data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Many-to-Many and Logical Operators" data-type="sect2"}
::: {#ch15.xhtml#logical_operators .sect2}
## Many-to-Many and Logical Operators

There are three logical or set []{#ch15.xhtml#ix_mnymny
primary="many-to-many vector matching"
data-type="indexterm"}[]{#ch15.xhtml#ix_vecmthmnymnylog
primary="vector matching" secondary="many-to-many and logical operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_binopvecmtom
primary="binary operators" secondary="vector matching"
tertiary="many-to-many and logical operators"
data-type="indexterm"}[]{#ch15.xhtml#ix_logop
primary="logical operators" data-type="indexterm"}operators you can use:

-   `or` union

-   `and` intersection

-   `unless` set subtraction

There is no *not* operator, but the `absent` function discussed in
["Missing Series, absent, and
absent_over_time"](#ch16.xhtml#absent){data-type="xref"} serves a
similar role.

All the logical operators work in a many-to-many fashion, and they are
the only operators that work many-to-many. They are different from the
arithmetic and comparison operators you have already seen in that no
math is performed; all that matters is whether a group contains samples.

::: {.section pdf-bookmark="or operator" data-type="sect3"}
::: {#ch15.xhtml#or .sect3}
### or operator

In the preceding section, `node_hwmon_sensor_label` did not have a
sample to go with every `node_hwmon_temp_celsius`, so results were only
returned for samples that were present in both instant vectors.
[]{#ch15.xhtml#idm45207091108640 primary="or operator"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091107936
primary="logical operators" secondary="or" data-type="indexterm"}Metrics
with inconsistent children, or whose children are not always present,
are tricky to work with, but you can deal with them using the `or`
operator.

How the `or` operator works is that for each group where the group on
the lefthand side has samples, then they are returned; otherwise, the
samples in the group on the righthand side are returned. If you are
familiar with SQL, this operator can be used in a similar way as the SQL
`COALESCE` function, but with labels.

Continuing the example from the preceding section, `or` can be used to
substitute the missing time series from `node_hwmon_sensor_label`.
[]{#ch15.xhtml#idm45207091103344 primary="node_hwmon_sensor_label"
secondary="using or operator to substitute missing time series"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091102240
primary="ignoring clause" secondary="use with or operator"
data-type="indexterm"}All you need is some other time series that has
the labels you need, which in this case is `node_hwmon_temp_celsius`.
`node_hwmon_temp_celsius` does not have the `label` label, but all the
other labels match up so you can ignore this using `ignoring`:

``` {data-type="programlisting"}
  node_hwmon_sensor_label
or ignoring(label)
  (node_hwmon_temp_celsius * 0 + 1)
```

The vector matching produced three groups of labels. The first two
groups had a sample from `node_hwmon_sensor_label` so that was what was
returned, including the metric name as there was nothing to change it.
For the third group, however, which included
[`sensor="temp1"`]{.keep-together}, there was no sample in the group for
the lefthand side, so the values in the group from the righthand side
were used. Because arithmetic operators were used on the value, the
metric name was removed.[]{#ch15.xhtml#idm45207091096640
primary="sensor labels" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

`x * 0 + 1` will change
all^[15](#ch15.xhtml#idm45207091094480){#ch15.xhtml#idm45207091094480-marker
data-type="noteref"}^ the values of the `x` instant vector to `1`. This
is also useful when you want to use `group_left` to copy labels, as `1`
is the identity element for multiplication, which is to say it does not
change the value you are multiplying.[]{#ch15.xhtml#idm45207091089088
primary="group_left" data-type="indexterm"}
:::

This expression can now be used in the place of
`node_hwmon_sensor_label`:

``` {data-type="programlisting"}
  node_hwmon_temp_celsius
* ignoring(label) group_left(label)
  (
      node_hwmon_sensor_label
    or ignoring(label)
      (node_hwmon_temp_celsius * 0 + 1)
  )
```

which will produce:

``` {data-type="programlisting"}
{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",sensor="temp1"} 42
{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_0",sensor="temp2"} 42
{chip="platform_coretemp_0",instance="localhost:9100",
    job="node",label="core_1",sensor="temp3"} 41
```

The sample with `sensor="temp1"` is now present in your result. It has
no label called `label`, which is the same as saying that that `label`
label has the empty string as a value.

In simpler cases you will be working with metrics without any
instrumentation labels. []{#ch15.xhtml#idm45207091083072
primary="instrumentation labels"
secondary="metrics without, working with" data-type="indexterm"}For
example, you might be using the textfile collector, as covered in
["Textfile
Collector"](#ch07.xhtml#textfile_collector){data-type="xref"}, and
expecting it to expose a metric called `node_custom_metric`. In the
event that metric doesn't exist, you would like to return `0` instead.
In cases like this, you can use the `up` metric that is associated with
every target:

``` {data-type="programlisting"}
  node_custom_metric
or
  up * 0
```

This has a small problem in that it will return a value even for a
failed scrape, which is not how scraped metrics
work.^[16](#ch15.xhtml#idm45207091078512){#ch15.xhtml#idm45207091078512-marker
data-type="noteref"}^ It will also return results for other jobs. You
can fix this with a matcher and some filtering:

``` {data-type="programlisting"}
  node_custom_metric
or
  (up{job="node"} == 1) * 0
```

Another way you can use the `or` operator is to return the larger of two
series:

``` {data-type="programlisting"}
(a >= b) or b
```

If `a` is larger it will be returned by the comparison, and then the
`or` operator since the group on the lefthand side was not empty. If, on
the other hand, `b` is larger, then the comparison will return nothing,
and `or` will return `b` as the group on the lefthand side was empty.
:::
:::

::: {.section pdf-bookmark="unless operator" data-type="sect3"}
::: {#ch15.xhtml#unless .sect3}
### unless operator

The `unless` operator does vector matching in the same way as the `or`
operator, working based on whether groups from the right and left
operands are empty or have samples.[]{#ch15.xhtml#idm45207091068400
primary="unless operator"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091067696
primary="logical operators" secondary="unless" data-type="indexterm"}
The `unless` operator returns the lefthand group, unless the righthand
group has members, in which case it returns no samples for that group.

You can use `unless` to restrict what time series are returned based on
an expression. For example, if you wanted to know the average CPU usage
of processes except those using less than 100 MB of resident memory, you
could use the expression:

``` {data-type="programlisting"}
  rate(process_cpu_seconds_total[5m])
unless
  process_resident_memory_bytes < 100 * 1024 * 1024
```

`unless` can also be used to spot when a metric is missing from a
target.[]{#ch15.xhtml#idm45207091063456 primary="targets"
secondary="spotting metric missing from target using unless"
data-type="indexterm"} For example:

``` {data-type="programlisting"}
  up{job="node"} == 1
unless
  node_custom_metric
```

would return a sample for every instance that was missing the
`node_custom_metric` metric, which you could use in alerting.

By default, as with all binary operators, `unless` looks at all labels
when grouping. If `node_custom_metric` had instrumentation labels, you
could use `on` or `ignoring` to check that at least one relevant time
series existed without having to know the values of the other labels:

``` {data-type="programlisting"}
  up == 1
unless on (job, instance)
  node_custom_metric
```

Even if there are multiple samples from the right operand in a group,
this is OK as `unless` uses many-to-many matching.
:::
:::

::: {.section pdf-bookmark="and operator" data-type="sect3"}
::: {#ch15.xhtml#and_operator .sect3}
### and operator

The `and` operator is the opposite of the `unless` operator.
[]{#ch15.xhtml#idm45207091053600 primary="and operator"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091052864
primary="logical operators" secondary="and" data-type="indexterm"}It
returns a group from the lefthand operand only if the matching righthand
group has samples; otherwise, it returns no samples for that match
group. You can think of it as an *if*
operator.^[17](#ch15.xhtml#idm45207091051216){#ch15.xhtml#idm45207091051216-marker
data-type="noteref"}^

You will use the `and` operator most commonly in alerting to specify
more than one condition. For example, you might want to return when both
latency is high and there is more than a trickle of user requests. To do
this for Prometheus for handlers that were taking over a second on
average and had at least one request per second, you could use:

``` {data-type="programlisting"}
  (
      rate(http_request_duration_seconds_sum{job="prometheus"}[5m])
    /
      rate(http_request_duration_seconds_count{job="prometheus"}[5m])
  ) > 1
and
  rate(http_request_duration_seconds_count{job="prometheus"}[5m]) > 1
```

This will return a sample for every individual handler on every
`prometheus` job, so it could get a little spammy even with the one
request per second restriction. Usually you would want to aggregate
across a job when alerting.

You can use `on` and `ignoring` with the `and` operator, as you can with
the other binary operators. []{#ch15.xhtml#idm45207091044384
primary="on clause" secondary="use with and operator"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091043408
primary="ignoring clause" secondary="use with and operator"
data-type="indexterm"}In particular, `on()` can be used to have a
condition that has no common labels at all between the two operands. You
can use this, for example, to limit the time of day an expression will
return results for:

``` {data-type="programlisting"}
  (
      rate(http_request_duration_microseconds_sum{job="prometheus"}[5m])
    /
      rate(http_request_duration_microseconds_count{job="prometheus"}[5m])
  ) > 1000000
and
  rate(http_request_duration_microseconds_count{job="prometheus"}[5m]) > 1
and on()
  hour() >= 9 < 17
```

The `hour` function is covered in ["minute, hour, day_of_week,
day_of_month, day_of_year, days_in_month, month, and
year"](#ch16.xhtml#date_functions){data-type="xref"}; it returns an
instant vector with one sample with no labels and the hour of the UTC
day of the query evaluation time as the
value.[]{#ch15.xhtml#idm45207091039296 primary="hour function"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091038624
primary="logical operators" startref="ix_logop"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091037680
primary="binary operators" secondary="vector matching"
startref="ix_binopvecmtom" tertiary="many-to-many and logical operators"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091036224
primary="vector matching" secondary="many-to-many and logical operators"
startref="ix_vecmthmnymnylog"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091035040
primary="many-to-one vector matching" startref="ix_mnymny"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091034080 primary="PromQL"
secondary="binary operators" startref="ix_PQLbinopvec"
tertiary="vector matching"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091032592
primary="binary operators" secondary="vector matching"
startref="ix_binopvec"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091031376
primary="vector matching" startref="ix_vecmtch" data-type="indexterm"}
:::
:::
:::
:::
:::
:::

::: {.section pdf-bookmark="Operator Precedence" data-type="sect1"}
::: {#ch15.xhtml#idm45207091030432 .sect1}
# Operator Precedence

When evaluating an expression with multiple binary operators, PromQL
does not simply go from left to right.[]{#ch15.xhtml#idm45207091028896
primary="binary operators" secondary="operator precedence"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091027920
primary="precedence (operator)"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091027248 primary="PromQL"
secondary="binary operators" tertiary="operator precedence"
data-type="indexterm"} Instead, there is an order of operators that is
largely the same as the order used in other languages:

1.  `^`

2.  `*` `/` `%` `atan2`

3.  `+` `-`

4.  `==` `!=` `>` `<` `>=` `<=`

5.  `unless` `and`

6.  `or`

For example, `a or b * c + d` is the same as `a or ((b * c) + d)`.

All operators except `^` are
left-associative.[]{#ch15.xhtml#idm45207091011808
primary="left-associative (operators)" data-type="indexterm"} That means
that `a / b * c` is the same as `(a / b) * c`, but `a ^ b ^ c` is
`a ^ (b ^ c)`.

You can use parentheses to change the order of
evaluation.[]{#ch15.xhtml#idm45207091008928
primary="() (parentheses), using to change order of evaluation"
data-type="indexterm"} We also recommend adding parentheses where the
evaluation order may not be immediately clear for an expression, as not
everyone will have memorized the operator precedence.

Now that you understand both aggregators and operators, let's look at
the final part of PromQL: functions.[]{#ch15.xhtml#idm45207091007184
primary="binary operators" startref="ix_binop"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091006208 primary="PromQL"
secondary="binary operators" startref="ix_PQLbinop"
data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch15.xhtml#idm45207091363104-marker)^ In contrast to *unary
operators*, which only take one operand. PromQL has `+` and `-` unary
operators.

^[2](#ch15.xhtml#idm45207091351520-marker)^ Internally, PromQL also has
a *string* type, but this is []{#ch15.xhtml#idm45207091350336
primary="string type" data-type="indexterm"}only used as an argument to
`count_values`, `label_replace`, and `label_join`.

^[3](#ch15.xhtml#idm45207091346128-marker)^ You may also see the
convention `{}: 0` to represent a single sample.

^[4](#ch15.xhtml#idm45207091336768-marker)^ If you are using a
dashboarding tool like Grafana, it's generally best to let it handle
creating human-readable units for metrics that are already in base
units, such as bytes.

^[5](#ch15.xhtml#idm45207091269120-marker)^ It is possible to use
filtering correctly with careful application of the `or` operator, but
it's more complicated and error prone.

^[6](#ch15.xhtml#idm45207091205152-marker)^ The `cpu` label was
aggregated away by both `sum`s, so is not present in the output either.

^[7](#ch15.xhtml#idm45207091197008-marker)^ You could exclude the
`on(instance, job)` here as the left- and righthand side both have only
`instance` and `job` labels.

^[8](#ch15.xhtml#idm45207091190320-marker)^ Running out of file
descriptors can break applications in fun ways, and you should usually
try to ensure that your applications always have enough.

^[9](#ch15.xhtml#idm45207091187808-marker)^ Alert templates have ready
access to the value of an alert's PromQL expression. This is discussed
in ["Annotations and
Templates"](#ch18.xhtml#alert_templates){data-type="xref"}.

^[10](#ch15.xhtml#idm45207091174880-marker)^ A missing `mode` label due
to aggregating it away would count as a single label value of the empty
string.

^[11](#ch15.xhtml#idm45207091171856-marker)^ There can still only be one
sample per group on the righthand side of the operand, as `group_left`
only enables many-to-one matching, not many-to-many matching.

^[12](#ch15.xhtml#idm45207091167440-marker)^ If the labels from the
righthand side were used, you would get the same labels for each sample
from the groups on the left, which would clash.

^[13](#ch15.xhtml#idm45207091153088-marker)^ There's no way for you to
request all the labels to be copied over, as then you would no longer
know what labels the output metric had.

^[14](#ch15.xhtml#idm45207091146896-marker)^ The convention for whether
a metric that has a single info-style label should have an `_info`
suffix is not fully resolved yet.

^[15](#ch15.xhtml#idm45207091094480-marker)^ `NaN` will stay as `NaN`,
but in practice there will be another time series with the same labels
and no `NaN` values that []{#ch15.xhtml#idm45207091092640
primary="multiplication, 1 as identity element for"
data-type="indexterm"}[]{#ch15.xhtml#idm45207091091872
primary="NaN (not a number)" data-type="indexterm"}you could use
instead.

^[16](#ch15.xhtml#idm45207091078512-marker)^ `up` is not a scraped
metric; Prometheus[]{#ch15.xhtml#idm45207091077568 primary="up"
secondary="added by Prometheus after each scrape" data-type="indexterm"}
adds it after every scrape whether the scrape succeeds or fails.

^[17](#ch15.xhtml#idm45207091051216-marker)^ Prior to Prometheus 2.x,
PromQL had an `IF` keyword that was used in alerting, so while Brian had
wondered if renaming the `and` operator to `if` would have been a good
idea, it was not possible.
:::
:::
:::

[]{#ch16.xhtml}

::: {#ch16.xhtml#sbo-rt-content}
::: {#ch16.xhtml#promql_functions_chapter .chapter}
# [Chapter 16. ]{.label}Functions

PromQL has 69 functions as of 2.37.0, and offers you a wide variety of
functionality, from common math to functions specifically for dealing
with counter and histogram metrics.[]{#ch16.xhtml#ix_PQLfnc
primary="PromQL" secondary="functions"
data-type="indexterm"}[]{#ch16.xhtml#ix_fnct primary="functions"
data-type="indexterm"} In this chapter you will learn about how all the
functions work and how they can be used.

Almost all PromQL functions return instant vectors, and the three that
don't (`time`, `pi`, and `scalar`) return scalars.
[]{#ch16.xhtml#idm45207090998144 primary="instant vectors"
secondary="return by PromQL functions"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090997168
primary="range vectors" secondary="functions and"
data-type="indexterm"}No functions return range vectors, though multiple
functions, including `rate` and `avg_over_time` that you have already
seen, take a range vector as input.[]{#ch16.xhtml#idm45207090995264
primary="avg_over_time" secondary="range vectors as input"
data-type="indexterm"}

Put another way, functions generally work either across the samples of a
single time series at a time or across the samples of an instant vector.
If you want to process an entire range vector at once, you would need to
use subqueries.[]{#ch16.xhtml#idm45207090993952 primary="types"
secondary="static typing in PromQL" data-type="indexterm"}

PromQL is statically typed, functions[]{#ch16.xhtml#idm45207090992592
primary="static typing (PromQL)" data-type="indexterm"} do not change
their return value based on the input types. In fact, the input types
for each function are also fixed. Where a function needs to work with
two different types, two different names are used. For example, you use
the `avg` aggregator on instant vectors and the `avg_over_time` function
on range vectors.

There are no official categories for the functions, but we have grouped
related functions together.

::: {.section pdf-bookmark="Changing Type" data-type="sect1"}
::: {#ch16.xhtml#idm45207090989968 .sect1}
# Changing Type

At times you will have a vector but need a scalar, or vice versa. There
are two functions that allow you to do so: `vector` and
`scalar`.[]{#ch16.xhtml#idm45207090987552 primary="PromQL"
secondary="functions" tertiary="changing type"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090986272 primary="types"
secondary="changing"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090985328
primary="functions" secondary="changing type" tertiary="vector"
data-type="indexterm"}

::: {.section pdf-bookmark="vector" data-type="sect2"}
::: {#ch16.xhtml#idm45207090983984 .sect2}
## vector

The `vector` function takes a scalar value, and converts it into an
instant vector with one sample without a label and the given
value.[]{#ch16.xhtml#idm45207090981920 primary="vector function"
data-type="indexterm"} For example, the expression:

``` {data-type="programlisting"}
vector(1)
```

will produce:

``` {data-type="programlisting"}
{} 1
```

This is useful if you need to ensure an expression returns a result, but
can't depend on any particular time series to exist. For example:

``` {data-type="programlisting"}
sum(some_gauge) or vector(0)
```

will always return one sample, even if `some_gauge` has no samples.
Depending on the use case, the `bool` modifier, as discussed in ["bool
modifier"](#ch15.xhtml#bool){data-type="xref"}, may be a better choice
than the `or` operator (see ["or
operator"](#ch15.xhtml#or){data-type="xref"}).
:::
:::

::: {.section pdf-bookmark="scalar" data-type="sect2"}
::: {#ch16.xhtml#idm45207090974416 .sect2}
## scalar

The `scalar` function takes an instant vector with a single sample and
converts it to a scalar with the value the input sample had.
[]{#ch16.xhtml#idm45207090972160 primary="scalar function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090971456
primary="functions" secondary="changing type" tertiary="scalar"
data-type="indexterm"}If there is not exactly one sample, then `NaN`
will be returned to you.

This is mostly useful when working with scalar constants, but you should
use math functions that only work on instant vectors. For example, if
you wanted the natural logarithm of two as a scalar, rather than typing
out `0.6931471805599453` and hoping anyone reading it recognized the
significance of number, you could use:

``` {data-type="programlisting"}
scalar(ln(vector(2)))
```

This can also make certain expressions simpler to write. For example, if
you wanted to see which servers were started in the current year, you
could do:

``` {data-type="programlisting"}
  year(process_start_time_seconds)
==
  scalar(year())
```

rather than:

``` {data-type="programlisting"}
  year(process_start_time_seconds)
== on() group_left
  year()
```

as scalar comparisons are a little easier to understand than vector
matching with `group_left`, and this is OK because you know that `year`
here will only ever return one sample.[]{#ch16.xhtml#idm45207090964048
primary="year function" data-type="indexterm"}

But use of the `scalar` function should be limited because using
`scalar` loses all of your labels and with it your ability to do vector
matching. For example:

``` {data-type="programlisting"}
  sum(rate(node_cpu_seconds_total{mode!="idle",instance="localhost:9090"}[5m]))
/
  scalar(count(node_cpu_seconds_total{mode="idle",instance="localhost:9090"))
```

will give you the proportion of time a machine's CPU is not idle, but
you would then have to alter and reevaluate this expression for every
single instance.

Taking advantage of the full power of PromQL, you can do:

``` {data-type="programlisting"}
  sum without (cpu, mode)(
     rate(node_cpu_seconds_total{mode!="idle"}[5m])
  )
/
  count without(cpu, mode)(node_cpu_seconds_total{mode="idle"})
```

and calculate the proportion of nonidle CPU for all your machines at
once.
:::
:::
:::
:::

::: {.section pdf-bookmark="Math" data-type="sect1"}
::: {#ch16.xhtml#idm45207090958768 .sect1}
# Math

The math functions perform standard mathematical operations on instant
vectors, such as calculating absolute values or taking a
logarithm.[]{#ch16.xhtml#ix_PQLfncmth primary="PromQL"
secondary="functions" tertiary="math functions"
data-type="indexterm"}[]{#ch16.xhtml#ix_math primary="math functions"
data-type="indexterm"}[]{#ch16.xhtml#ix_fnctmth primary="functions"
secondary="math" data-type="indexterm"} Each sample in the instant
vector is handled independently, and the metric name is removed in the
return value.

::: {.section pdf-bookmark="abs" data-type="sect2"}
::: {#ch16.xhtml#idm45207090952912 .sect2}
## abs

`abs` takes an instant vector and returns
the[]{#ch16.xhtml#idm45207090950816 primary="abs function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090950112
primary="math functions" secondary="abs"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090949168
primary="functions" secondary="math" tertiary="abs"
data-type="indexterm"} absolute value for each of its values, which is
to say any negative numbers are changed to positive numbers.

The expression:

``` {data-type="programlisting"}
abs(process_open_fds - 15)
```

will return how far away each process's open file descriptors count is
from 15. Counts of 5 and 25 would both return 10.
:::
:::

::: {.section pdf-bookmark="ln, log2, and log10" data-type="sect2"}
::: {#ch16.xhtml#idm45207090945776 .sect2}
## ln, log2, and log10

The functions `ln`, `log2`, and `log10` take an instant vector, return
the logarithm of the values, and use different bases for the logarithm,
Euler's number *e*, 2, and 10, respectively. `ln` is also known as the
*natural logarithm*.[]{#ch16.xhtml#idm45207090941104
primary="math functions" secondary="ln, log2, and log10"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090940096
primary="functions" secondary="math" tertiary="ln, log2, and log10"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090938880
primary="ln function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090938208
primary="log2 function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090937536
primary="log10 function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090936864
primary="natural logarithm" data-type="indexterm"}

These functions can be used to get an idea of the different orders of
magnitude of numbers. For example, to calculate the number of
9s^[1](#ch16.xhtml#idm45207090935792){#ch16.xhtml#idm45207090935792-marker
data-type="noteref"}^ of successes an API endpoint had over the past
hour, you could do:

``` {data-type="programlisting"}
log10(
    sum without(instance)(rate(requests_failed_total[1h]))
  /
    sum without(instance)(rate(requests_total[1h]))
) * -1
```

::: {data-type="tip"}
###### Tip

If you want a logarithm to a different base, you can use the *change of
base* formula.[]{#ch16.xhtml#idm45207090932624
primary="change of base function" data-type="indexterm"} For example,
for a logarithm base three on the instant vector `x`, you would use:

``` {data-type="programlisting"}
ln(x) / ln(3)
```
:::

These can also be useful for graphing in certain circumstances where
normal linear graphs can't suitably represent a large variance in
values. However, it is usually best to rely on the built-in logarithm
graphing options in tools such as Grafana rather than using these
functions, as they tend to gracefully handle edge cases such as negative
logarithms returning `NaN`.
:::
:::

::: {.section pdf-bookmark="exp" data-type="sect2"}
::: {#ch16.xhtml#idm45207090929456 .sect2}
## exp

The `exp` function provides the natural exponent, and is the inverse to
the `ln` function. []{#ch16.xhtml#idm45207090927008
primary="exp function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090926272
primary="math functions" secondary="exp"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090925328
primary="functions" secondary="math" tertiary="exp"
data-type="indexterm"}For example:

``` {data-type="programlisting"}
exp(vector(1))
```

returns:

``` {data-type="programlisting"}
{} 2.718281828459045
```

which is Euler's number, *e*.
:::
:::

::: {.section pdf-bookmark="sqrt" data-type="sect2"}
::: {#ch16.xhtml#idm45207090921088 .sect2}
## sqrt

The `sqrt` function returns a square root of
[]{#ch16.xhtml#idm45207090919104 primary="sqrt function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090918400
primary="functions" secondary="math" tertiary="sqrt"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090917184
primary="math functions" secondary="sqrt" data-type="indexterm"}the
values in an instant vector. For [example]{.keep-together}:

``` {data-type="programlisting"}
sqrt(vector(9))
```

will return:

``` {data-type="programlisting"}
{} 3
```

`sqrt` predates the[]{#ch16.xhtml#idm45207090912784
primary="^ (exponent) operator"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090912048
primary="exponent operator (^)" data-type="indexterm"} exponent operator
`^`, so this is equivalent to:

``` {data-type="programlisting"}
vector(9) ^ 0.5
```

::: {data-type="tip"}
###### Tip

If you need other roots, you can use the same approach. For example, the
cube or third []{#ch16.xhtml#idm45207090909056 primary="cube root"
data-type="indexterm"}root can be calculated with:

``` {data-type="programlisting"}
vector(9) ^ (1/3)
```
:::
:::
:::

::: {.section pdf-bookmark="ceil and floor" data-type="sect2"}
::: {#ch16.xhtml#idm45207090907264 .sect2}
## ceil and floor

`ceil` and `floor` allow you to round the values in an instant
vector.[]{#ch16.xhtml#idm45207090904736 primary="functions"
secondary="math" tertiary="ceil and floor"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090903488
primary="ceil function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090902816
primary="math functions" secondary="ceil and floor"
data-type="indexterm"} `ceil` always rounds up to the nearest integer,
and `floor` always rounds down. For example:

``` {data-type="programlisting"}
ceil(vector(0.1))
```

will return:

``` {data-type="programlisting"}
{} 1
```
:::
:::

::: {.section pdf-bookmark="round" data-type="sect2"}
::: {#ch16.xhtml#idm45207090898992 .sect2}
## round

`round` rounds the values in an instant vector to the nearest integer.
If you provide a value that is exactly halfway between two integers, it
is rounded up. []{#ch16.xhtml#idm45207090896848 primary="math functions"
secondary="round" data-type="indexterm"}[]{#ch16.xhtml#idm45207090895872
primary="round function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090895200
primary="functions" secondary="math" tertiary="round"
data-type="indexterm"}That is to say that:

``` {data-type="programlisting"}
round(vector(5.5))
```

will return:

``` {data-type="programlisting"}
{} 6
```

`round` is also one of the functions that you can optionally provide
with an additional argument. The additional argument is a scalar, and
the values will be rounded to the nearest multiple of this number:

``` {data-type="programlisting"}
round(vector(2446), 1000)
```

will return:

``` {data-type="programlisting"}
{} 2000
```

for example. This is equivalent to:

``` {data-type="programlisting"}
round(vector(2446) / 1000) * 1000
```

but easier for you to use and understand.
:::
:::

::: {.section pdf-bookmark="clamp, clamp_max, and clamp_min" data-type="sect2"}
::: {#ch16.xhtml#idm45207090898336 .sect2}
## clamp, clamp_max, and clamp_min

Sometimes you will find that a metric returns spurious values well
outside the normal range, such as a gauge that you expect to be positive
occasionally being massively negative.[]{#ch16.xhtml#idm45207090885504
primary="functions" secondary="math"
tertiary="clamp, clamp_max, and clamp_min"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090884288
primary="clamp function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090883616
primary="clamp_max function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090882944
primary="clamp_min function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090882272
primary="math functions" secondary="clamp, clamp_max, and clamp_min"
data-type="indexterm"} `clamp_max` and `clamp_min` allow you to put
upper and lower bounds, respectively, on the values in an instant
vector.

For example, if you didn't believe that your processes could have fewer
than 10 open file descriptors, you could use:

``` {data-type="programlisting"}
clamp_min(process_open_fds, 10)
```

which would produce a result like:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 46
{instance="localhost:9100",job="node"} 10
```

`clamp` enables you to put upper and lower bounds into a single query.
On the same data, the following query:

``` {data-type="programlisting"}
clamp(process_open_fds, 10, 20)
```

would produce a result like:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 20
{instance="localhost:9100",job="node"} 10
```
:::
:::

::: {.section pdf-bookmark="sgn" data-type="sect2"}
::: {#ch16.xhtml#idm45207090886976 .sect2}
## sgn

`sgn` returns a vector with all sample values converted to their sign,
defined as this: 1 if the value is positive, --1 if value is negative,
and 0 if the value is equal to zero.[]{#ch16.xhtml#idm45207090872880
primary="sgn function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090872176
primary="math functions" secondary="sgn"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090871232
primary="functions" secondary="math" tertiary="sgn"
data-type="indexterm"}

``` {data-type="programlisting"}
sgn(vector(100))
```

will return:

``` {data-type="programlisting"}
{} 1
```
:::
:::

::: {.section pdf-bookmark="Trigonometric Functions" data-type="sect2"}
::: {#ch16.xhtml#idm45207090868064 .sect2}
## Trigonometric Functions

There are 12 trigonometric functions available.
[]{#ch16.xhtml#idm45207090866496 primary="functions" secondary="math"
tertiary="trigonometric functions"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090865248
primary="math functions" secondary="trigonometric functions"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090864304
primary="trigonometric functions" data-type="indexterm"}They work in
radians:

-   `acos` calculates the arccosine of the values.

-   `acosh` calculates the inverse hyperbolic cosine of the values.

-   `asin` calculates the arcsine of the values.

-   `asinh` calculates the inverse hyperbolic sine of the values.

-   `atan` calculates the arctangent of the values.

-   `atanh` calculates the inverse hyperbolic tangent of the values.

-   `cos` calculates the cosine of the values.

-   `cosh` calculates the hyperbolic cosine of the values.

-   `sin` calculates the sine of the values.

-   `sinh` calculates the hyperbolic sine of the values.

-   `tan` calculates the tangent of the values.

-   `tanh` calculates the hyperbolic tangent of the values.

There are three additional []{#ch16.xhtml#idm45207090847936
primary="degrees and radians, converting between"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090847168
primary="radians and degrees, converting between"
data-type="indexterm"}functions that are useful for converting between
degrees and radians:

-   `deg` converts values passed as radians to degrees.

-   `pi` returns pi.

-   `rad` converts values passed as degrees to radians.

``` {data-type="programlisting"}
sin(vector(pi()/2))
```

returns:

``` {data-type="programlisting"}
{} 1
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Time and Date" data-type="sect1"}
::: {#ch16.xhtml#idm45207090958176 .sect1}
# Time and Date

Prometheus offers you several functions
dealing[]{#ch16.xhtml#idm45207090838720 primary="PromQL"
secondary="functions" startref="ix_PQLfncmth" tertiary="math functions"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090837200
primary="functions" secondary="math" startref="ix_fnctmth"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090835984
primary="math functions" startref="ix_math"
data-type="indexterm"}[]{#ch16.xhtml#ix_fncttmda primary="functions"
secondary="time and date"
data-type="indexterm"}[]{#ch16.xhtml#ix_tmdafnc
primary="time and date functions"
data-type="indexterm"}[]{#ch16.xhtml#ix_PQLfnctmda primary="PromQL"
secondary="functions" tertiary="time and date" data-type="indexterm"}
with time, most of which are convenience functions around `time` to save
you from having to implement date-related logic yourself. Prometheus
works entirely in UTC, and has no notion of time zones.

::: {.section pdf-bookmark="time" data-type="sect2"}
::: {#ch16.xhtml#idm45207090830528 .sect2}
## time

The `time` function is the most basic time-related function. It returns
the evaluation time of the query as seconds since the Unix
epoch^[2](#ch16.xhtml#idm45207090828272){#ch16.xhtml#idm45207090828272-marker
data-type="noteref"}^ as a scalar.[]{#ch16.xhtml#idm45207090827584
primary="functions" secondary="time and date" tertiary="time function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090826336
primary="time function" data-type="indexterm"} For example:

``` {data-type="programlisting"}
time()
```

might return:

``` {data-type="programlisting"}
1652911202.529
```

If you were to use `time` with the `query_range` endpoint, then every
result would be different, as each step has a different evaluation
time.[]{#ch16.xhtml#idm45207090822224 primary="query_range"
secondary="using time function with" data-type="indexterm"}

The Prometheus best practice is to expose the Unix time in seconds at
which something of interest happened, and not how long it has been since
it happened. This is more reliable, as it's not susceptible to failure
to update the metric. The `time` function then lets you convert these to
durations. For example, if you wanted to see how long your processes
have been running, you would use:

``` {data-type="programlisting"}
time() - process_start_time_seconds
```

which will return a result such as:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 313.5699999332428
{instance="localhost:9100",job="node"} 322.25999999046326
```

Here both Node Exporter and Prometheus have been running for a bit over
5 minutes. If you had a batch job pushing the last time it succeeded to
the Pushgateway, as discussed in
["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}, you could
find jobs that hadn't succeeded in the past hour with:

``` {data-type="programlisting"}
time() - my_job_last_success_seconds > 3600
```
:::
:::

::: {.section pdf-bookmark="minute, hour, day_of_week, day_of_month, day_of_year, days_in_month, month, and year" data-type="sect2"}
::: {#ch16.xhtml#date_functions .sect2}
## minute, hour, day_of_week, day_of_month, day_of_year, days_in_month, month, and year

`time` covers most use cases, but sometimes you will want to have logic
based on the clock or calendar.[]{#ch16.xhtml#idm45207090813632
primary="date functions"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090812928
primary="minute function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090812256
primary="hour function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090811584
primary="day_of_week function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090810912
primary="day_of_year function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090810240
primary="days_in_month function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090809568
primary="month function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090808896
primary="year function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090808224
primary="functions" secondary="time and date" tertiary="minute function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090807008
primary="functions" secondary="time and date" tertiary="hour function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090805792
primary="functions" secondary="time and date" tertiary="day_of_week"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090804576
primary="functions" secondary="time and date" tertiary="day_of_month"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090803360
primary="functions" secondary="time and date" tertiary="day_of_year"
data-type="indexterm"} Converting to minutes and hours from `time` isn't
too
difficult,^[3](#ch16.xhtml#idm45207090801632){#ch16.xhtml#idm45207090801632-marker
data-type="noteref"}^ but beyond that you have to consider issues like
leap days.

All of these functions return the given value for the query evaluation
time as an instant vector with one sample and no
labels.[]{#ch16.xhtml#idm45207090799936 primary="functions"
secondary="time and date" tertiary="days_in_month"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090798688
primary="functions" secondary="time and date" tertiary="month function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090797472
primary="functions" secondary="time and date" tertiary="year function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090796256
primary="UTC (Coordinated Universal Time)" data-type="indexterm"} As we
write this it is currently 13:37 on Saturday, November 5, 2022, in the
UTC time zone. The outputs of these functions when evaluated at this
time are:

  Expression          Result
  ------------------- ---------
  `minute()`          {} 37
  `hour()`            {} 13
  `day_of_week()`     {} 6
  `day_of_month()`    {} 5
  `day_of_year()`     {} 309
  `days_in_month()`   {} 30
  `month()`           {} 11
  `year()`            {} 2022

`day_of_week` starts with `0` for Sunday, so the `6` here is Saturday.
If you wanted to check if today was the last day of the month, you could
compare the output of `day_of_month` to
`days_in_month`.[]{#ch16.xhtml#idm45207090775728 primary="day_of_month"
data-type="indexterm"}

You may be wondering why these functions don't return scalars, as that'd
seem more convenient to work with. The answer is that these functions
all take an optional
argument^[4](#ch16.xhtml#idm45207090774544){#ch16.xhtml#idm45207090774544-marker
data-type="noteref"}^ so that you can pass in instant vectors. For
example, to see what year your processes started in, you could use:

``` {data-type="programlisting"}
year(process_start_time_seconds)
```

which would produce a result such as:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 2022
{instance="localhost:9100",job="node"} 2022
```

This could also be used to count how many processes were started this
month:

``` {data-type="programlisting"}
sum(
    (year(process_start_time_seconds) == bool scalar(year()))
  *
    (month(process_start_time_seconds) == bool scalar(month()))
)
```

Here we are taking advantage of the fact that the multiplication
operator acts like an *and operator* when used on booleans with the
value `1` for `true` and `0` for `false`.
:::
:::

::: {.section pdf-bookmark="timestamp" data-type="sect2"}
::: {#ch16.xhtml#idm45207090815504 .sect2}
## timestamp

The `timestamp` function is different from the other time functions in
that it looks at the timestamp of the samples in an instant vector
rather than the values.[]{#ch16.xhtml#idm45207090764880
primary="timestamp function" data-type="indexterm"} As was mentioned in
["Instant Vector"](#ch13.xhtml#instant_vector){data-type="xref"} and
["query"](#ch13.xhtml#query_api){data-type="xref"}, the timestamps for
samples returned from all operators, functions, the `query_range` HTTP
API, and `query` HTTP API when it returns an instant vector will be the
query evaluation time.[]{#ch16.xhtml#idm45207090761520 primary="query"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090760816
primary="query_range" data-type="indexterm"}

However, the timestamp of samples in an instant vector from an instant
vector selector will be the actual
timestamps.^[5](#ch16.xhtml#idm45207090759760){#ch16.xhtml#idm45207090759760-marker
data-type="noteref"}^ The `timestamp` function allows you to access
these. For example, you can see when the last scrape started for each
target with:

``` {data-type="programlisting"}
timestamp(up)
```

This is because the default timestamp for data from a scrape is the time
that the scrape started. Similarly the timestamp for samples from
recording rules, as covered in
[Chapter 17](#ch17.xhtml#promql_rules_chapter){data-type="xref"}, is the
rule group execution time.

If you want to see raw data with samples for debugging, using a range
vector selector with the `query` HTTP API is best, but `timestamp` does
have some uses. For example:

``` {data-type="programlisting"}
node_time_seconds - timestamp(node_time_seconds)
```

would return the difference between when the scrape of the Node Exporter
was started by Prometheus and what time the Node Exporter thought was
the current time. While this isn't 100% accurate (it will vary with
machine load), it will allow you to know if time is out of sync by a few
seconds without needing a 1-second scrape
interval.[]{#ch16.xhtml#idm45207090753104 primary="PromQL"
secondary="functions" startref="ix_PQLfnctmda" tertiary="time and date"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090751584
primary="time and date functions" startref="ix_tmdafnc"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090750640
primary="functions" secondary="time and date" startref="ix_fncttmda"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Labels" data-type="sect1"}
::: {#ch16.xhtml#idm45207090749296 .sect1}
# Labels

In an ideal world the label names []{#ch16.xhtml#idm45207090747808
primary="PromQL" secondary="functions" tertiary="label"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090746560 primary="labels"
data-type="indexterm"}and label values used by different parts of your
system would be consistent; for example, you wouldn't have `customer` in
one place and `cust` in another. While it is best to resolve such
inconsistencies in the source code, or failing that with
`metric_relabel_configs` as discussed in
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"},
this is not always possible. Thus the two label functions allow you to
change labels.[]{#ch16.xhtml#idm45207090743408
primary="metric_relabel_configs" data-type="indexterm"}

::: {.section pdf-bookmark="label_replace" data-type="sect2"}
::: {#ch16.xhtml#idm45207090742736 .sect2}
## label_replace

`label_replace` allows you to do regular expression substitution on
label values. []{#ch16.xhtml#idm45207090740784 primary="functions"
secondary="label_replace"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090739728
primary="label_replace function" data-type="indexterm"}For example, if
you needed the `device` label on `node_disk_read_bytes_total` to be
`dev` instead for vector matching to work as you needed, you could
do:^[6](#ch16.xhtml#idm45207090737680){#ch16.xhtml#idm45207090737680-marker
data-type="noteref"}^

``` {data-type="programlisting"}
label_replace(node_disk_read_bytes_total, "dev", "${1}", "device", "(.*)")
```

which would return a result like:

``` {data-type="programlisting"}
node_disk_read_bytes_total{dev="sda",device="sda",instance="localhost:9100",
    job="node"} 4766305792
```

Unlike most functions, `label_replace` does not remove the metric name,
as it is presumed that you are doing something unusual if you have to
resort to `label_replace`, and removing the metric name could make that
harder for you.

The arguments to `label_replace` are the instant vector input, the name
of the output label, the replacement, the name of the source label, and
the regular expression. `label_replace` is similar to the `replace`
relabeling action, but you can only use one label as a source label. If
the regular expression does not match for a given sample, then that
sample is returned unchanged.
:::
:::

::: {.section pdf-bookmark="label_join" data-type="sect2"}
::: {#ch16.xhtml#idm45207090730096 .sect2}
## label_join

`label_join` allows you to join label []{#ch16.xhtml#idm45207090728080
primary="functions" secondary="label_join"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090727104
primary="label_join function" data-type="indexterm"}values, similarly to
how `source_labels` is handled in relabeling. For example, if you wanted
to join the `job` and `instance` labels into a new label, you could do:

``` {data-type="programlisting"}
label_join(node_disk_read_bytes_total, "combined", "-", "instance", "job")
```

which would return a result such as:

``` {data-type="programlisting"}
node_disk_read_bytes_total{combined="localhost:9100-node",device="sda",
    instance="localhost:9100",job="node"} 4766359040
```

As with `label_replace`, `label_join` does not remove the metric name.
The arguments are the instant vector input, the name of the output
label, the separator, and then zero or more label names.

You could combine `label_join` with `label_replace` to provide the full
functionality of the `replace` relabel action, but at that point you
should seriously consider [`metric_relabel_configs`]{.keep-together} or
fixing the source metrics instead.
:::
:::
:::
:::

::: {.section pdf-bookmark="Missing Series, absent, and absent_over_time" data-type="sect1"}
::: {#ch16.xhtml#absent .sect1}
# Missing Series, absent, and absent_over_time

As mentioned in ["Many-to-Many and Logical
Operators"](#ch15.xhtml#logical_operators){data-type="xref"}, the
`absent` function plays the role of a *not* operator. If you pass a
nonempty instant vector as the `absent` argument, it returns an empty
instant vector. []{#ch16.xhtml#idm45207090714096
primary="absent function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090713392
primary="missing series"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090712720
primary="functions"
secondary="missing series, absent and absent_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090711680
primary="absent_over_time function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090710944 primary="PromQL"
secondary="functions"
tertiary="missing series, absent and absent_over_time"
data-type="indexterm"}If you pass an empty instant vector, it returns an
instant vector with one sample and a value of `1`.

You might expect that this sample has no labels, since there are no
labels to work with. However, `absent` is a little smarter than that,
and if the argument is an instant vector selector, it uses the labels
from any equality matchers present.

  Expression                                        Result
  ------------------------------------------------- ---------------------------
  `absent(up)`                                      empty instant vector
  `absent(up{job="prometheus"})`                    empty instant vector
  `absent(up{job="missing"})`                       `{job="missing"} 1`
  `absent(up{job=~"missing"})`                      `{} 1`
  `absent(non_existent)`                            `{} 1`
  `absent(non_existent{job="foo",env="dev"})`       `{job="foo",env="dev"} 1`
  `absent(non_existent{job="foo",env="dev"} * 0)`   `{} 1`

`absent` is useful for detecting if an entire job has gone missing from
service discovery.[]{#ch16.xhtml#idm45207090691648 primary="alerting"
secondary="missing job and" data-type="indexterm"} Alerting on `up == 0`
doesn't work too well when you have no targets to produce `up` metrics!
Even when using `static_configs` it can be wise to have such an alert in
case generation of your *prometheus.yml* goes
awry.[]{#ch16.xhtml#idm45207090688720 primary="static_configs"
data-type="indexterm"}

If you want instead to alert on specific metrics that are missing from a
target, you can use `unless`, which was covered in ["unless
operator"](#ch15.xhtml#unless){data-type="xref"}.

`absent` has a range variant, `absent_over_time`. It returns an empty
vector if the range vector passed to it has any elements, and a
one-element vector with the value `1` if the range vector passed has no
elements.

This is useful for alerting on when no time series exist for a given
metric name and label combination for a certain amount of time.

The following query:

``` {data-type="programlisting"}
absent_over_time(up{job="myjob"}[1h])
```

would mean that the job `myjob` hasn't got any target for at least one
hour.
:::
:::

::: {.section pdf-bookmark="Sorting with sort and sort_desc" data-type="sect1"}
::: {#ch16.xhtml#idm45207090717920 .sect1}
# Sorting with sort and sort_desc

PromQL generally does not specify the order of elements within an
instant vector, so it can change from evaluation to
evaluation.[]{#ch16.xhtml#idm45207090680048 primary="functions"
secondary="sorting with sort and sort_desc"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090679008
primary="sort function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090678336
primary="sort_desc function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090677664 primary="PromQL"
secondary="functions" tertiary="sorting with sort and sort_desc"
data-type="indexterm"} But if you use `sort` or `sort_desc` as the last
thing that is evaluated in a PromQL expression, then the instant vector
will be sorted by value. For example:

``` {data-type="programlisting"}
sort(node_filesystem_size_bytes)
```

might return:

``` {data-type="programlisting"}
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/lock"} 5238784
node_filesystem_free_bytes{device="/dev/sda1",fstype="vfat",
    instance="localhost:9100",job="node",mountpoint="/boot/efi"} 70300672
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run"} 817094656
node_filesystem_free_bytes{device="tmpfs",fstype="tmpfs",
    instance="localhost:9100",job="node",mountpoint="/run/user/1000"} 826912768
node_filesystem_free_bytes{device="/dev/sda5",fstype="ext4",
    instance="localhost:9100",job="node",mountpoint="/"} 30791843840
```

The effect of these functions is cosmetic, but may save you some effort
in reporting scripts. `NaN`s are always sorted to the end, so `sort` and
`sort_desc` are not quite the reverse of each
other.[]{#ch16.xhtml#idm45207090671552 primary="NaN (not a number)"
secondary="sorting and" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

The instant vectors returned from the `topk` and `bottomk` aggregators
already come with their samples sorted within the aggregation groups.
:::
:::
:::

::: {.section pdf-bookmark="Histograms with histogram_quantile" data-type="sect1"}
::: {#ch16.xhtml#idm45207090668432 .sect1}
# Histograms with histogram_quantile

The `histogram_quantile` function was already touched on in
["Histogram"](#ch13.xhtml#histogram_intro){data-type="xref"}. It is
internally a bit like an aggregator, since it groups samples together
like a `without(le)` clause would and then calculates a quantile from
their values.[]{#ch16.xhtml#idm45207090665072
primary="histogram_quantile function" secondary="about"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090663952
primary="functions" secondary="histogram_quantile"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090663008 primary="PromQL"
secondary="functions" tertiary="histogram_quantile"
data-type="indexterm"} For example:

``` {data-type="programlisting"}
histogram_quantile(
    0.90,
    rate(prometheus_tsdb_compaction_duration_seconds_bucket[1d])
)
```

would calculate the 0.9 quantile (also known as the 90th percentile)
latency of Prometheus's compaction latency over the past day. Values
outside of the range from zero to one do not make sense for quantiles,
and will result in infinities.

As discussed in ["Cumulative
Histograms"](#ch03.xhtml#cumulative_histograms){data-type="xref"}, the
values in the buckets must be cumulative and there must be a `+Inf`
bucket.

You must always use `rate` first for buckets exposed by Prometheus's
histogram metric type, as shown in ["The
Histogram"](#ch03.xhtml#histogram){data-type="xref"}, as
`histogram_quantile` needs gauges to work on.
[]{#ch16.xhtml#idm45207090656464 primary="rate function"
secondary="using first for buckets exposed by histogram metric type"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090655440 primary="gauges"
secondary="needed by histogram_quantile" data-type="indexterm"}But there
are a very small number of exporters that expose histogram-like time
series where the buckets are gauges rather than counters. If you come
across one of these, it is OK to use `histogram_quantile` on them
directly.
:::
:::

::: {.section pdf-bookmark="Counters" data-type="sect1"}
::: {#ch16.xhtml#idm45207090653808 .sect1}
# Counters

Counters include not just the counter metric, but also the `_sum`,
`_count`, and `_bucket` time series from summary and histogram metrics.
[]{#ch16.xhtml#ix_PQLfncctr primary="PromQL" secondary="functions"
tertiary="counters" data-type="indexterm"}[]{#ch16.xhtml#ix_fnctcount
primary="functions" secondary="counter"
data-type="indexterm"}[]{#ch16.xhtml#ix_counts primary="counters"
data-type="indexterm"}Counters can only go up. When an application
starts or restarts, counters will initialize to `0`, and the counter
functions take this into account automatically.

The values of counters are not particularly useful on their own; you
will almost always want to convert them to gauges using one of the
counter-related functions.

Functions working on counters all take a range vector as an argument and
return an instant vector. Each of the time series in the range vector is
processed individually, and returns at most one sample. If there is only
one sample for one of your time series within the range you provide, you
will get no output for it when using these [functions]{.keep-together}.

::: {.section pdf-bookmark="rate" data-type="sect2"}
::: {#ch16.xhtml#rate .sect2}
## rate

The `rate` function is the primary function you will use with counters,
and indeed likely the main function you will use from
PromQL.[]{#ch16.xhtml#idm45207090641712 primary="counters"
secondary="rate function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090640736
primary="rate function" secondary="about"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090639792
primary="functions" secondary="counter" tertiary="rate function"
data-type="indexterm"} `rate` returns how fast a counter is increasing
per second for each time series in the range vector passed to it. You
have already seen many examples of `rate`, such as:

``` {data-type="programlisting"}
rate(process_cpu_seconds_total[1m])
```

which returns a result like:

``` {data-type="programlisting"}
{instance="localhost:9090",job="prometheus"} 0.0018000000000000683
{instance="localhost:9100",job="node"} 0.005
```

`rate` automatically handles counter resets, and any decrease in a
counter is considered to be a counter reset. So, for example, if you had
a time series that had values `[5,10,4,6]`, it would be treated as
though it was `[5,10,14,16]`. `rate` presumes that the targets it is
monitoring are relatively long-lived compared to a scrape interval, as
it cannot detect multiple resets in a short period of time. If you have
targets that are expected to regularly live for less than a handful of
scrape intervals, you may wish to consider a log-based monitoring
solution instead.

`rate` has to handle scenarios like time series appearing and
disappearing, such as if one of your instances started up and then later
crashed. For example, if one of your instances had a counter that was
incrementing at a rate of around 10 per second, but was only running for
half an hour, then a `rate(x_total[1h])` would return a result of around
5 per second.

Values are rarely exact. Since scrapes for different targets happen at
different times, there can be jitter over time, the steps of a
`query_range` call will rarely align perfectly with scrapes, and scrapes
are expected to fail every now and then. In the face of such challenges,
`rate` is designed to be robust, and the result of `rate` is intended to
be correct when looked at on average over time.

`rate` is not intended to catch every single increment, as it is
expected that increments will be lost, such as if an instance dies
between scrapes. This may cause artifacts if you have very slow-moving
counters, such as if they're only incremented a few times an hour.
`rate` can also only deal with changes in counters, because if a counter
time series appears with a value of 100, `rate` has no idea if those
increments were just now or if the target has been running for years and
has only just started being returned by service discovery to be scraped.

It is recommended to use a range for your range vector that is at least
four times your scrape interval. This will ensure that you always have
two samples to work with even if scrapes are slow, ingestion is slow,
and there has been a single scrape failure. Such issues are a fact of
life in real-world systems, so it is important to be resilient. For
example, for a 1-minute scrape interval you
[]{#ch16.xhtml#idm45207090628288 primary="five-minute rate"
data-type="indexterm"}might use a 4-minute rate, but usually that is
rounded up to a 5-minute
rate.^[7](#ch16.xhtml#idm45207090627456){#ch16.xhtml#idm45207090627456-marker
data-type="noteref"}^

Generally you should aim to have the same range used on all your rate
functions within a Prometheus for the sake of sanity, since outputs from
rates over different ranges are not comparable and tend to be hard to
keep track of.

You may wonder with all these implementation details and caveats if
`rate` could be changed to be simpler. There are several ways you can
approach this problem, but at the end of the day they all have both
advantages and disadvantages. If you fix one apparent problem, you will
cause a different problem to pop up. The `rate` function is a good
balance across all of these concerns, and provides a robust solution
suitable for operational monitoring. If you run into a situation where
any rate-like function isn't giving you quite what you need, we would
suggest continuing your debugging based on logs data, which does not
have these particular concerns and can produce exact answers with the
trade-off of a bigger price.
:::
:::

::: {.section pdf-bookmark="increase" data-type="sect2"}
::: {#ch16.xhtml#idm45207090643968 .sect2}
## increase

`increase` is merely syntactic sugar on top of `rate`.
`increase(x_total[5m])` is exactly equivalent to
`rate(x_total[5m]) * 300`, which is to say the result of `rate`
multiplied by the range of the range vector. The logic is otherwise
identical. []{#ch16.xhtml#idm45207090619248 primary="rate function"
secondary="increase function and"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090618272
primary="counters" secondary="increase function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090617328
primary="increase function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090616656
primary="functions" secondary="counter" tertiary="increase function"
data-type="indexterm"}

Seconds are the base unit for Prometheus, so you should use `increase`
only when displaying values to humans. Within your recording rules and
alerts it is best to stick to `rate` for consistency.

One of the outcomes of the robustness of `rate` and `increase` is that
they can return noninteger results when given integer inputs. Consider
that you had the following data points for a time series:

``` {data-type="programlisting"}
21 at 2s
22 at 7s
24 at 12s
```

and you were to calculate `increase(x_total[15s])` with a query time of
15 seconds. The increase here is 3 over a period of 10 seconds, so you
might expect a result of 3. However, the rate was taken over a 15-second
period, so to avoid underestimating the correct answer, the 10 seconds
of data you have is extrapolated out to 15 seconds, producing a result
of 4.5 for the increase.

`rate` and `increase` presume that a time series continues beyond the
bound of the range if the first/last samples is within 110% of the
average interval of the data. If this is not the case, it is presumed
the time series exists for 50% of an interval beyond the samples you
have, but not with the value going below zero.
:::
:::

::: {.section pdf-bookmark="irate" data-type="sect2"}
::: {#ch16.xhtml#idm45207090609776 .sect2}
## irate

`irate` is like `rate` in that it returns the per-second rate at which a
counter is increasing. []{#ch16.xhtml#idm45207090607488
primary="irate function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090606784
primary="functions" secondary="counter" tertiary="irate function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090605568
primary="counters" secondary="irate function" data-type="indexterm"}The
algorithm it uses is much simpler though; it only looks at the last two
samples of the range vector it is passed. This has the advantage that it
is much more responsive to changes and you don't have to care so much
about the relationship between the vector's range and the scrape
interval, but comes with the corresponding disadvantage that as it is
only looking at two samples, it can only be safely used in graphs that
are fully zoomed
in.^[8](#ch16.xhtml#idm45207090604496){#ch16.xhtml#idm45207090604496-marker
data-type="noteref"}^
[Figure 16-1](#ch16.xhtml#irate_vs_rate){data-type="xref"} shows a
comparison of a 5-minute `rate` against an `irate`.

Due to the lack of averaging that `irate` brings, the graphs can be more
volatile^[9](#ch16.xhtml#idm45207090599568){#ch16.xhtml#idm45207090599568-marker
data-type="noteref"}^ and harder to read. It is not advisable to use
`irate` in alerts due to it being sensitive to brief spikes and dips;
use `rate` instead.[]{#ch16.xhtml#idm45207090596736
primary="rate function" secondary="irate function and"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090595728
primary="instant rate" see="irate function" data-type="indexterm"}

<figure>
<div id="ch16.xhtml#irate_vs_rate" class="figure">
<img src="assets/pur2_1601.png" width="600" height="270"
alt="Two plots on a graph, one spiky the other reasonably smooth" />
<h6><span class="label">Figure 16-1. </span>CPU usage of a Node Exporter
viewed with <code>rate</code> and <code>irate</code></h6>
</div>
</figure>
:::
:::

::: {.section pdf-bookmark="resets" data-type="sect2"}
::: {#ch16.xhtml#idm45207090591744 .sect2}
## resets

You may sometimes suspect that a counter is resetting more often than it
should be. []{#ch16.xhtml#idm45207090590240 primary="resets function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090589536
primary="counters" secondary="resets function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090588592
primary="functions" secondary="counter" tertiary="resets function"
data-type="indexterm"}The `resets` function returns how many times each
time series in a range vector has reset. For example, the expression:

``` {data-type="programlisting"}
resets(process_cpu_seconds_total[1h])
```

will indicate how many times the CPU time of the process has reset in
the past hour.[]{#ch16.xhtml#idm45207090585600 primary="restarts"
data-type="indexterm"} This should be the number of times the process
has
restarted,^[10](#ch16.xhtml#idm45207090584768){#ch16.xhtml#idm45207090584768-marker
data-type="noteref"}^ but if you had a bug that was causing it to go
backward, the value would be higher.

`resets` is intended as a debugging tool for counters, since counters
might reset too often and nonmonotonic counters will cause artifacts in
the form of large spikes in your graphs. However, some users have found
occasional uses for it when they want to know how many times a gauge has
been seen to decrease.[]{#ch16.xhtml#idm45207090582240 primary="PromQL"
secondary="functions" startref="ix_PQLfncctr" tertiary="ounters"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090580720
primary="functions" secondary="counter" startref="ix_fnctcount"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090579504
primary="counters" startref="ix_counts" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Changing Gauges" data-type="sect1"}
::: {#ch16.xhtml#idm45207090578432 .sect1}
# Changing Gauges

Unlike counters, the values of gauges are useful on their own and you
can use binary operators and aggregators directly on
them.[]{#ch16.xhtml#ix_fnctchgauge primary="functions"
secondary="changing gauges"
data-type="indexterm"}[]{#ch16.xhtml#ix_gaugechg primary="gauges"
secondary="changing, functions for" data-type="indexterm"} But sometimes
you will want to analyze the history of a gauge, and there are several
functions for this purpose.

As with the []{#ch16.xhtml#idm45207090573680 primary="range vectors"
secondary="gauge-changing functions taking"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090572704
primary="instant vectors" secondary="gauge-changing functions returning"
data-type="indexterm"}counter functions, these functions also take a
range vector and return an instant vector with at most one sample for
each time series in your input.

::: {.section pdf-bookmark="changes" data-type="sect2"}
::: {#ch16.xhtml#idm45207090571376 .sect2}
## changes

Some gauges are expected to change very
rarely.[]{#ch16.xhtml#idm45207090569616 primary="changes function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090568912 primary="gauges"
secondary="changing, functions for" tertiary="changes function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090567696
primary="functions" secondary="changing gauges"
tertiary="changes function" data-type="indexterm"} For example, the
start time of a process does not change in the lifetime of a
process.^[11](#ch16.xhtml#idm45207090566352){#ch16.xhtml#idm45207090566352-marker
data-type="noteref"}^ []{#ch16.xhtml#idm45207090564848
primary="process_start_time_seconds" data-type="indexterm"}The `changes`
function allows you to count how many times a gauge has changed value,
so:

``` {data-type="programlisting"}
changes(process_start_time_seconds[1h])
```

will tell you how many times your process has restarted in the past
hour. If you aggregated this across entire applications, it would allow
you to spot if your applications were in a slow crash loop.

Due to the fundamental nature of metrics sampling, Prometheus may not
scrape often enough to see every possible
change.[]{#ch16.xhtml#idm45207090561808 primary="sampling"
secondary="not catching every possible change" data-type="indexterm"}
However, if a process is restarting that frequently, you will still
detect it either via this method or by `up` being `0`.

You can use `changes` beyond `process_start_time_seconds` for other
situations where the fact that a gauge has changed is interesting to
you.
:::
:::

::: {.section pdf-bookmark="deriv" data-type="sect2"}
::: {#ch16.xhtml#deriv .sect2}
## deriv

Often you will want to know how quickly a gauge is changing; for
example, how quickly a backlog is increasing if it is increasing at
all.[]{#ch16.xhtml#idm45207090556288 primary="functions"
secondary="changing gauges" tertiary="deriv function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090555040
primary="deriv function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090554368 primary="gauges"
secondary="changing, functions for" tertiary="deriv function"
data-type="indexterm"} This would allow you to alert on not only the
backlog being higher than you'd like but also that it has not already
started to go down.

You could do `x - x offset 1h`, but this only
[]{#ch16.xhtml#idm45207090551808 primary="least-squares regression"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090551056
primary="simple linear regression"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090550368
primary="regression" data-type="indexterm"}uses two samples, and thus
lacks robustness because it is susceptible to individual outlier values.
The `deriv` function uses *least-squares
regression*^[12](#ch16.xhtml#idm45207090548736){#ch16.xhtml#idm45207090548736-marker
data-type="noteref"}^ to estimate the slope of each of the time series
in a range vector. For example:

``` {data-type="programlisting"}
deriv(process_resident_memory_bytes[1h])
```

would calculate how fast resident memory is changing per second based on
samples from the past hour.
:::
:::

::: {.section pdf-bookmark="predict_linear" data-type="sect2"}
::: {#ch16.xhtml#idm45207090546272 .sect2}
## predict_linear

`predict_linear` goes a step further than `deriv` and predicts what the
value of a gauge will[]{#ch16.xhtml#idm45207090543904
primary="predict_linear function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090543200 primary="gauges"
secondary="changing, functions for" tertiary="predict_linear function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090541984
primary="functions" secondary="changing gauges"
tertiary="predict_linear function" data-type="indexterm"} be in the
future based on data in the provided range. For example:

``` {data-type="programlisting"}
predict_linear(node_filesystem_free_bytes{job="node"}[1h], 4 * 3600)
```

would predict how much free space would be left on each filesystem in
four hours based on the past hour of samples. This expression is roughly
equivalent to:

``` {data-type="programlisting"}
  deriv(node_filesystem_free_bytes{job="node"}[1h]) * 4 * 3600
+
  node_filesystem_free_bytes{job="node"}
```

but `predict_linear` is slightly more accurate because it uses the
intercept from the [regression]{.keep-together}.

`predict_linear` is useful for resource limit alerts, where static
thresholds such as [1 GB]{.keep-together} free or percentage thresholds
such as 10% free tend to have false positives and false negatives
depending on whether you are working with relatively large or small
filesystems.[]{#ch16.xhtml#idm45207090535264 primary="alerting"
secondary="predict_linear function for resource limit alerts"
data-type="indexterm"} A 1 GB threshold on a 1 TB filesystem would alert
you too late, but would also alert you too early on a 2 GB filesystem.
`predict_linear` works better across all sizes.

It can take some tweaking to choose good values for the range and to
determine how far to predict forward. If there was a regular sawtooth
pattern in the data, you would want to ensure that the range was long
enough not to extrapolate the upward part of the cycle out indefinitely.
:::
:::

::: {.section pdf-bookmark="delta" data-type="sect2"}
::: {#ch16.xhtml#idm45207090532816 .sect2}
## delta

`delta` is similar to `increase`, but without the counter reset
handling.[]{#ch16.xhtml#idm45207090530528 primary="delta function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090529824
primary="functions" secondary="changing gauges"
tertiary="delta function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090528608 primary="gauges"
secondary="changing, functions for" tertiary="delta function"
data-type="indexterm"} This function should be avoided as it can be
overly affected by single outlier values.
[]{#ch16.xhtml#idm45207090527152 primary="deriv function"
secondary="using instead of delta" data-type="indexterm"}You should use
`deriv` instead, or `x - x offset 1h` if you really want to compare with
the value a given time ago.
:::
:::

::: {.section pdf-bookmark="idelta" data-type="sect2"}
::: {#ch16.xhtml#idm45207090525088 .sect2}
## idelta

`idelta` takes the last two samples in a range and returns their
difference. `idelta` is intended for advanced use cases. For example,
the way `rate` and `irate` work is not to everyone's personal tastes, so
using `idelta` and recording rules allows users to implement what they'd
like without polluting PromQL with various subtle variations of the
`rate` function.
:::
:::

::: {.section pdf-bookmark="holt_winters" data-type="sect2"}
::: {#ch16.xhtml#idm45207090520560 .sect2}
## holt_winters

The `holt_winters`
function^[13](#ch16.xhtml#idm45207090518384){#ch16.xhtml#idm45207090518384-marker
data-type="noteref"}^ implements *Holt-Winters double exponential
smoothing*. []{#ch16.xhtml#idm45207090516400
primary="holt_winters function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090515696
primary="functions" secondary="changing gauges"
tertiary="holt_winters function"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090514480 primary="gauges"
secondary="changing, functions for" tertiary="holt_winters function"
data-type="indexterm"}Gauges can at times be very spiky and hard to read
so some smoothing is often good. At the simplest you could use
`avg_over_time`, but you might want something more
sophisticated.[]{#ch16.xhtml#idm45207090512560 primary="avg_over_time"
data-type="indexterm"}

This function works through the samples for a time series, tracks the
smoothed value []{#ch16.xhtml#idm45207090511600
primary="smoothing factor"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090510896
primary="trend factor" data-type="indexterm"}so far, and provides an
estimate of the trend in the data. Each new sample is taken into account
based on the *smoothing factor*, which indicates how much old data is
important relative to new data, and the *trend factor*, which controls
how important the trend is. For example:

``` {data-type="programlisting"}
holt_winters(process_resident_memory_bytes[1h], 0.1, 0.5)
```

would smooth memory usage with a smoothing factor of `0.1` and a trend
factor of `0.5`. Both factors must be between `0` and
`1`.[]{#ch16.xhtml#idm45207090505696 primary="functions"
secondary="changing gauges" startref="ix_fnctchgauge"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090504416 primary="gauges"
secondary="changing, functions for" startref="ix_gaugechg"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Aggregation Over Time" data-type="sect1"}
::: {#ch16.xhtml#_over_time .sect1}
# Aggregation Over Time

Aggregators such as `avg` work across the samples in an instant
vector.[]{#ch16.xhtml#ix_aggovrtm primary="aggregation"
secondary="functions for aggregation over time"
data-type="indexterm"}[]{#ch16.xhtml#ix_fnctAOT primary="functions"
secondary="aggregation over time"
data-type="indexterm"}[]{#ch16.xhtml#ix_PQLfncAOT primary="PromQL"
secondary="functions" tertiary="aggregation over time"
data-type="indexterm"} There is also a set of functions such as
`avg_over_time` that apply the same logic, but across the values of a
time series in a range vector. []{#ch16.xhtml#idm45207090496032
primary="sum_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090495328
primary="count_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090494656
primary="avg_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090493984
primary="stddev_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090493312
primary="stdvar_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090492640
primary="min_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090491968
primary="max_over_time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090491296
primary="quantile_over_time" data-type="indexterm"}These functions are:

-   `sum_over_time`

-   `count_over_time`

-   `avg_over_time`

-   `stddev_over_time`

-   `stdvar_over_time`

-   `min_over_time`

-   `max_over_time`

-   `quantile_over_time`

Two extra functions are not directly linked to aggregators:

-   `present_over_time` acts like the `group` aggregator, returns the
    value 1 for any series matched by the range
    selector.[]{#ch16.xhtml#idm45207090479072
    primary="present_over_time" data-type="indexterm"}

-   `last_over_time` returns the last value for any series matched by
    the range
    [selector]{.keep-together}.[]{#ch16.xhtml#idm45207090476384
    primary="last_over_time" data-type="indexterm"}

For example, to see the peak memory usage that Prometheus saw for a
process, you could use:

``` {data-type="programlisting"}
max_over_time(process_resident_memory_bytes[1h])
```

and even go a step further and calculate that across the application:

``` {data-type="programlisting"}
max without(instance)(max_over_time(process_resident_memory_bytes[1h]))
```

These functions only work from the values of the samples; there is no
weighting based on the length of time between samples or any other logic
relating to timestamps. This means that if you change the scrape
interval, for example, there will be a bias toward the time period with
the more frequent scrapes for functions such as `avg_over_time` and
`quantile_over_time`. Similarly, if there are failed scrapes for a
period of time, that period will be less represented in your result.

These functions are used with
gauges.^[14](#ch16.xhtml#idm45207090470832){#ch16.xhtml#idm45207090470832-marker
data-type="noteref"}^ If you want to take an `avg_over_time` of a
`rate`, this isn't possible as that function returns instant rather than
range vectors. However, `rate` already calculates an average over time,
so you can increase the range on the `rate`. For example, instead of
trying to do:

``` {data-type="programlisting"}
avg_over_time(rate(x_total[5m])[1h])
```

which will produce a parse error, you can instead do:

``` {data-type="programlisting"}
rate(x_total[1h])
```

How to use the instant vector output of functions as the input of
functions that require range vectors is covered in the next chapter on
recording rules.[]{#ch16.xhtml#idm45207090465216 primary="functions"
secondary="aggregation over time" startref="ix_fnctAOT"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090463968
primary="aggregation" secondary="functions for aggregation over time"
startref="ix_aggovrtm"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090462784 primary="PromQL"
secondary="functions" startref="ix_PQLfncAOT"
tertiary="aggregation over time"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090461296 primary="PromQL"
secondary="functions" startref="ix_PQLfnc"
data-type="indexterm"}[]{#ch16.xhtml#idm45207090460080
primary="functions" startref="ix_fnct" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch16.xhtml#idm45207090935792-marker)^ A 99% success rate is two
9s.

^[2](#ch16.xhtml#idm45207090828272-marker)^ Midnight January 1st, 1970
UTC.

^[3](#ch16.xhtml#idm45207090801632-marker)^ Minutes are
`floor(vector(time() / 60 % 60))`, for example.

^[4](#ch16.xhtml#idm45207090774544-marker)^ The default value of this
argument is `vector(time())`.

^[5](#ch16.xhtml#idm45207090759760-marker)^ As will the timestamps of
samples if you provide a range vector selector to the `query` HTTP API.

^[6](#ch16.xhtml#idm45207090737680-marker)^ In reality, as
`node_disk_read_bytes_total` is a counter, you would use `rate` first
and then `label_replace`.

^[7](#ch16.xhtml#idm45207090627456-marker)^ *Five-minute rate* is a
colloquial way to say a `rate` function on a range vector with a
5-minute range, such as `rate(x_total[5m])`.

^[8](#ch16.xhtml#idm45207090604496-marker)^ If the `step` for a
`query_range` is greater than the scrape interval, you would skip data
when using `irate`.

^[9](#ch16.xhtml#idm45207090599568-marker)^ `irate` is short for
*instant rate*, though that the function is called *irate* still brings
Brian minor amusement.

^[10](#ch16.xhtml#idm45207090584768-marker)^
`changes(process_start_time_seconds[1h])` is a better way to count
restarts, as [timestamps are gauges](https://oreil.ly/EHfI0).

^[11](#ch16.xhtml#idm45207090566352-marker)^ Although there have been
cases, such as in [Prometheus bug report 289](https://oreil.ly/HW1Kl),
where a cloud provider's kernel was providing bad metrics.

^[12](#ch16.xhtml#idm45207090548736-marker)^ Also known as *simple
linear regression*.

^[13](#ch16.xhtml#idm45207090518384-marker)^ It is possible this
function is misnamed; see [Prometheus issue
#2458](https://oreil.ly/WTR0v).

^[14](#ch16.xhtml#idm45207090470832-marker)^ Though as `count_over_time`
and `present_over_time` ignore values, they can be useful for debugging
any type of metric.
:::
:::
:::

[]{#ch17.xhtml}

::: {#ch17.xhtml#sbo-rt-content}
::: {#ch17.xhtml#promql_rules_chapter .chapter}
# [Chapter 17. ]{.label}Recording Rules

The HTTP API is not the only way in which you can access PromQL. You can
also use *recording rules* to have Prometheus evaluate PromQL
expressions regularly and ingest their results.[]{#ch17.xhtml#ix_PQLrec
primary="PromQL" secondary="recording rules"
data-type="indexterm"}[]{#ch17.xhtml#ix_recru primary="recording rules"
data-type="indexterm"} This is useful to speed up your dashboards,
provide aggregated results for use elsewhere, and compose range vector
functions. Other monitoring systems might call their equivalent feature
standing queries or continuous queries. Alerting rules (covered in
[Chapter 18](#ch18.xhtml#alerting_rules_chapter){data-type="xref"}) are
also a variant of recording rules. This chapter will show you how and
when to use recording rules.

::: {.section pdf-bookmark="Using Recording Rules" data-type="sect1"}
::: {#ch17.xhtml#idm45207090453664 .sect1}
# Using Recording Rules

Recording rules go in separate files from your *prometheus.yml*, which
are known as *rule files*. []{#ch17.xhtml#ix_recruuse
primary="recording rules" secondary="using" data-type="indexterm"}As
with *prometheus.yml*, rule files also use the YAML format.
[]{#ch17.xhtml#idm45207090449056 primary="rule files"
data-type="indexterm"}[]{#ch17.xhtml#ix_PQLrecuse primary="PromQL"
secondary="recording rules" tertiary="using" data-type="indexterm"}You
can specify where your rule files are located using the `rule_files`
top-level field in your *prometheus.yml*. For example,
[Example 17-1](#ch17.xhtml#rules_prometheus_yml){data-type="xref"} loads
a rule file called *rules.yml*, in addition to scraping two targets.

::: {#ch17.xhtml#rules_prometheus_yml data-type="example"}
##### [Example 17-1. ]{.label}*prometheus.yml* scraping two targets and loading a rule file

``` {code-language="yaml" data-type="programlisting"}
global:
  scrape_interval: 10s
  evaluation_interval: 10s
rule_files:
 - rules.yml
scrape_configs:
 - job_name: prometheus
   static_configs:
    - targets:
      - localhost:9090
 - job_name: node
   static_configs:
    - targets:
      - localhost:9100
```
:::

Similar []{#ch17.xhtml#idm45207090438416 primary="files field"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090437808
primary="file_sd_configs" data-type="indexterm"}to the `files` field of
`file_sd_configs`, as covered in
["File"](#ch08.xhtml#file_sd){data-type="xref"}, `rule_files` takes a
list of paths, and you can use globs in the filename. Unlike file
service discovery, `rule_files` does not use inotify nor does it
automatically pick up changes you make to rule
files.[]{#ch17.xhtml#idm45207090392912 primary="rule_files field"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090392208
primary="configuration" secondary="asking Prometheus to reload"
data-type="indexterm"} Instead, you must either restart Prometheus or
reload its configuration.

To ask Prometheus []{#ch17.xhtml#idm45207090390912
primary="kill -HUP command"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090390176
primary="SIGHUP signal" data-type="indexterm"}to reload its
configuration, you can send it the `SIGHUP` signal using a command like:

``` {data-type="programlisting"}
kill -HUP <pid>
```

where `pid` is the process ID of Prometheus.
[]{#ch17.xhtml#idm45207090387184 primary="pid (process ID)"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090386480
primary="process ID (pid)"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090385808
primary="POST method (HTTP)" data-type="indexterm"}You can also send an
HTTP `POST` to the *[/-/reload]{.keep-together}* endpoint of Prometheus,
but for security reasons this requires that the `--web.enable-lifecycle`
flag is specified.[]{#ch17.xhtml#idm45207090383360
primary="--web.enable-lifecycle flag" primary-sortas="web.enable"
data-type="indexterm"} If the reload fails, Prometheus will log this,
and you will see the `prometheus_config_last_reload_successful` metric
change to `0`.

To detect bad configuration files or rules in advance,
you[]{#ch17.xhtml#idm45207090313776 primary="configuration"
secondary="checking with promtool check rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090312928
primary="recording rules"
secondary="detecting bad rules with promtool check rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090312016
primary="promtool" secondary="check rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090311104
primary="check rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090310432
primary="check config (promtool)" data-type="indexterm"} can use the
`promtool check config` command to check your *prometheus.yml*. This
will also check all the rule files referenced by the *prometheus.yml*.
You might have this as a pre-submit check or unit test that is applied
before the configuration file is rolled out. If you want to check the
syntax of individual rule files, you can use `promtool check rules`.

Rule files themselves consist []{#ch17.xhtml#idm45207090307616
primary="rule files" secondary="example" data-type="indexterm"}of
zero^[1](#ch17.xhtml#idm45207090306512){#ch17.xhtml#idm45207090306512-marker
data-type="noteref"}^ or more groups of rules.
[Example 17-2](#ch17.xhtml#rules_yml_simple){data-type="xref"} shows a
rule file.

::: {#ch17.xhtml#rules_yml_simple data-type="example"}
##### [Example 17-2. ]{.label}*rules.yml* with one group containing two rules

``` {code-language="yaml" data-type="programlisting"}
groups:
 - name: example
   rules:
    - record: job:process_cpu_seconds:rate5m
      expr: sum without(instance)(rate(process_cpu_seconds_total[5m]))
    - record: job:process_open_fds:max
      expr: max without(instance)(process_open_fds)
```
:::

You will notice that the group has a `name`.
[]{#ch17.xhtml#idm45207090296800 primary="groups"
secondary="in rule files" secondary-sortas="rule"
data-type="indexterm"}This must be unique within a rule file, and is
used in the Prometheus UI and metrics. `expr` is the PromQL expression
to be evaluated and output into the metric name specified by `record`.

It is possible to[]{#ch17.xhtml#idm45207090277328
primary="evaluation_interval"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090276592
primary="scrape_interval" data-type="indexterm"} specify an
`evaluation_interval` for a group, but as with `scrape_interval` you
should aim for only one interval in a Prometheus for sanity.
[]{#ch17.xhtml#idm45207090274960 primary="labels"
secondary="specifying in labels field of rule file"
data-type="indexterm"}You can also specify a set of labels in the
`labels` field to be added to the output, but this is rarely appropriate
for recording
rules.^[2](#ch17.xhtml#idm45207090273504){#ch17.xhtml#idm45207090273504-marker
data-type="noteref"}^

Each rule in a group is evaluated in turn, and the output of your first
rule is ingested into the time series database before your second rule
is run. While rules within a group are executed sequentially, different
groups will be run at different times just as different targets are
scraped at different times. This is to spread out the load on your
Prometheus.[]{#ch17.xhtml#idm45207090272240
primary="Rules status page (Prometheus)" data-type="indexterm"}

Once your rules are loaded and running, you can view them on the Rules
status page at *http://localhost:9090/rules*, as shown in
[Figure 17-1](#ch17.xhtml#rules_status){data-type="xref"}.

<figure>
<div id="ch17.xhtml#rules_status" class="figure">
<img src="assets/pur2_1701.png" width="600" height="327"
alt="Two rules in a group on the rules status page" />
<h6><span class="label">Figure 17-1. </span>Rules status page of
Prometheus</h6>
</div>
</figure>

In addition to listing your rules, how long each group as a whole took
to last evaluate and how long each rule took to execute are also
displayed. You can use this to [find expensive]{.keep-together} rules
that may need adjustment or reconsideration. The
[`prometheus_rule_group_last_duration_seconds`]{.keep-together} metric
will also tell you how long the last evaluation of each group took,
which you can use to determine if there have been recent changes in the
cost of your rules. There is no metric with the duration of individual
rules as that could cause cardinality issues. In this case, the rules
are taking less than a millisecond, which is well under the evaluation
interval, so there is nothing to worry about.

::: {.note data-type="note"}
###### Note

There is no API to upload or change rules. As with Prometheus
configuration generally, files are intended to be a base upon which you
could build such a system on top of if you so
wish.[]{#ch17.xhtml#idm45207090213632 primary="PromQL"
secondary="recording rules" startref="ix_PQLrecuse" tertiary="using"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090212112
primary="recording rules" secondary="using" startref="ix_recruuse"
data-type="indexterm"}
:::
:::
:::

::: {.section pdf-bookmark="When to Use Recording Rules" data-type="sect1"}
::: {#ch17.xhtml#idm45207090453072 .sect1}
# When to Use Recording Rules

There are several cases when you might want to use recording rules.
Recording rules are mainly used to aggregate metrics in order to make
your queries more efficient. []{#ch17.xhtml#ix_PQLrecwhen
primary="PromQL" secondary="recording rules" tertiary="when to use"
data-type="indexterm"}[]{#ch17.xhtml#ix_recruwhen
primary="recording rules" secondary="when to use"
data-type="indexterm"}This is common for dashboards, federation, and
before storing the metrics in long-term storage. You might also use
recording rules to compose range vector functions, and on occasion offer
APIs of metrics to other teams.

::: {.section pdf-bookmark="Reducing Cardinality" data-type="sect2"}
::: {#ch17.xhtml#reducing_cardinality .sect2}
## Reducing Cardinality

If you have an expression such as:

``` {data-type="programlisting"}
sum without(instance)(rate(process_cpu_seconds_total{job="node"}[5m]))
```

in a dashboard, you will find you get a prompt response from Prometheus
if you have a few targets.[]{#ch17.xhtml#idm45207090203024
primary="dashboards" secondary="making faster"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090202048
primary="cardinality" secondary="reducing with recording rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090201040
primary="recording rules" secondary="when to use"
tertiary="reducing cardinality" data-type="indexterm"} As the number of
targets grows to the hundreds and thousands, you will find that the
response time for a `query_range` is not as snappy.
[]{#ch17.xhtml#idm45207090199184 primary="query_range"
data-type="indexterm"}

Rather than[]{#ch17.xhtml#idm45207090198064 primary="YAML"
secondary="multiline strings in"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090197056 primary="groups"
secondary="in rule files" secondary-sortas="rule" data-type="indexterm"}
asking PromQL to access and process thousands of time series for the
entire range of each graph on your dashboard, you can precompute this
value using a rule group using something
like:^[3](#ch17.xhtml#idm45207090195712){#ch17.xhtml#idm45207090195712-marker
data-type="noteref"}^

``` {data-type="programlisting"}
groups:
 - name: node
   rules:
    - record: job:process_cpu_seconds:rate5m
      expr: >
        sum without(instance)(
          rate(process_cpu_seconds_total{job="node"}[5m])
        )
```

which will output to a metric called `job:process_cpu_seconds:rate5m`.

Now you only need to fetch that one time series when your dashboard is
being rendered.[]{#ch17.xhtml#idm45207090192784
primary="job:process_cpu_seconds:rate5m" data-type="indexterm"} The same
applies even if you have instrumentation labels in play, as you are
reducing the number of time series to process by a factor of how many
instances you have. Effectively, you are trading an ongoing resource
cost against much lower latency and resource cost for your queries. Due
to this trade-off it is not generally wise to have rules that use long
vector ranges, as such queries tend to be expensive, and running them
regularly can cause performance
problems.[]{#ch17.xhtml#idm45207090191888 primary="range vectors"
secondary="in recording rules" secondary-sortas="recording"
data-type="indexterm"}

You should try to put all rules for one job in one
group.[]{#ch17.xhtml#idm45207090190288 primary="aggregation"
secondary="using in recording rules" data-type="indexterm"} That way
they will have the same timestamp and avoid artifacts when you do
further math on them. All recording rules in a group have the same query
evaluation time for an execution, and all output samples will also have
that timestamp.

You will find aggregation rules like these are useful beyond making your
dashboards faster.[]{#ch17.xhtml#idm45207090188672 primary="federation"
data-type="indexterm"} When using federation, as discussed in ["Going
Global with Federation"](#ch21.xhtml#federation){data-type="xref"}, you
will always want to pull aggregated metrics, as otherwise you would be
pulling in large swathes of instance-level metrics. At that point, the
Prometheus using federation would be better off scraping the targets
directly itself from a performance
standpoint.^[4](#ch17.xhtml#idm45207090187040){#ch17.xhtml#idm45207090187040-marker
data-type="noteref"}^

Similar logic applies if you want to save some metrics on a long-term
basis. When doing capacity planning over months or years of data,
details of individual instances are not relevant. By keeping primarily
aggregated metrics long term, you can save a lot of resources with
little loss in useful
information.^[5](#ch17.xhtml#idm45207090186128){#ch17.xhtml#idm45207090186128-marker
data-type="noteref"}^

You will often have aggregation rules based off the same metric but with
different sets of labels. Rather than calculating each aggregation
individually, you can be efficient by having one rule use the output of
another. For example:

``` {data-type="programlisting"}
groups:
 - name: node
   rules:
    - record: job_device:node_disk_read_bytes:rate5m
      expr: >
        sum without(instance)(
          rate(node_disk_read_bytes_total{job="node"}[5m])
        )
    - record: job:node_disk_read_bytes:rate5m
      expr: >
        sum without(device)(
          job_device:node_disk_read_bytes:rate5m{job="node"}
        )
```

For this to work properly, the rules in a given hierarchy must be in
order within a single rule
group.^[6](#ch17.xhtml#idm45207090183968){#ch17.xhtml#idm45207090183968-marker
data-type="noteref"}^ It is generally best to explicitly specify the job
that your rules apply to in your selectors, so that your groups don't
step on each others' toes.
:::
:::

::: {.section pdf-bookmark="Composing Range Vector Functions" data-type="sect2"}
::: {#ch17.xhtml#idm45207090205552 .sect2}
## Composing Range Vector Functions

As mentioned in ["Aggregation Over
Time"](#ch16.xhtml#_over_time){data-type="xref"}, you cannot use range
vector functions on the output of functions that produce instant
vectors.[]{#ch17.xhtml#idm45207090180368 primary="recording rules"
secondary="when to use" tertiary="composing range vector functions"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090179136
primary="functions" secondary="composing range vector functions"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090178176
primary="range vectors" secondary="composing range vector functions"
data-type="indexterm"} For example,
`max_over_time(sum without(instance)(rate(x_total[5m]))[1h])` is not
possible, and will produce a parse error.
[]{#ch17.xhtml#idm45207090176608 primary="max_over_time"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090175936
primary="subqueries" data-type="indexterm"}While PromQL features
subqueries, you can use recording rules to the same effect:

``` {data-type="programlisting"}
groups:
 - name: j_job_rules
   rules:
    - record: job:x:rate5m
      expr: >
        sum without(instance)(
          rate(x_total{job="j"}[5m])
        )
    - record: job:x:max_over_time1h_rate5m
      expr: max_over_time(job:x:rate5m{job="j"}[1h])
```

This approach can be used with any range vector function, including not
only the `_over_time` functions but also `predict_linear`, `deriv`, and
`holt_winters`.[]{#ch17.xhtml#idm45207090172144
primary="over_time functions"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090171408
primary="aggregation" secondary="functions for aggregation over time"
data-type="indexterm"}

However, this[]{#ch17.xhtml#idm45207090170016 primary="rate function"
secondary="using with sum"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090169040 primary="sum"
secondary="using rate before" data-type="indexterm"} technique should
not be used with `rate`, `irate`, or `increase`, as an effective
expression of `rate(sum(x_total)[5m])` would have massive spikes every
time one of its constituent counters reset or disappeared.

::: {.warning data-type="warning"}
###### Warning

Always `rate` and then `sum`, never `sum` and then `rate`.
:::

You are not required to have the outer function in a recording rule.
With the preceding example it might make more sense to have the
`max_over_time` performed as you need it. For example, the primary use
for this particular example would be capacity planning, as you need to
plan for peak rather than average traffic. Since capacity planning is
often performed once a month or once a quarter, there is not much point
in you evaluating the `max_over_time` at least once a minute rather than
running the query just when you need it. Functions over longer time
ranges can also get expensive due to the amount of data they have to
process. Be careful with ranges over an hour and particularly across
many time series.
:::
:::

::: {.section pdf-bookmark="Rules for APIs" data-type="sect2"}
::: {#ch17.xhtml#rules_for_apis .sect2}
## Rules for APIs

Usually the Prometheus servers you run are going to be used entirely by
you and your team. []{#ch17.xhtml#idm45207090159840
primary="recording rules" secondary="when to use"
tertiary="rules for APIs"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090158592 primary="APIs"
secondary="rules for" data-type="indexterm"}But you may run into
situations where other teams wish to pull metrics from your Prometheus.
If their usage is just informational or depends on metrics that are
unlikely to change, that's generally OK, because if you break things on
them it's not the end of the world. But if the metrics are being used as
part of automated systems or processes outside of your control, it may
be a good idea to create metrics just for other teams to consume as a
form of public API. Then if you need to change the labels or rules
inside your Prometheus you can do so, while still ensuring that the
metrics the other team depends on keep the same semantics.

The naming of such metrics doesn't tend to follow the normal naming
conventions, and you will typically put the name of the consuming team
either in the metric name or a label.

Such uses of rules are quite rare. If another team's use of your
Prometheus is getting to the stage where it is placing a nontrivial
maintenance burden on you, you might want to ask them to run their own
Prometheus for the metrics they need.
:::
:::

::: {.section pdf-bookmark="How Not to Use Rules" data-type="sect2"}
::: {#ch17.xhtml#idm45207090156400 .sect2}
## How Not to Use Rules

We have noticed a few common antipatterns with recording rules that we
would like to help you avoid.[]{#ch17.xhtml#idm45207090154624
primary="recording rules" secondary="when to use"
tertiary="how not to use recording rules" data-type="indexterm"}

The first of these is rules that undo the benefits of
labels.[]{#ch17.xhtml#idm45207090153024 primary="labels"
secondary="recording rules undoing benefits of" data-type="indexterm"}
For example:

``` {data-type="programlisting"}
    - record: job_device:node_disk_read_bytes_sda:rate5m
      expr: >
        sum without(instance)(
          rate(node_disk_read_bytes_total{job="node",device="sda"}[5m])
        )
    - record: job_device:node_disk_read_bytes_sdb:rate5m
      expr: >
        sum without(instance)(
          rate(node_disk_read_bytes_total{job="node",device="sdb"}[5m])
        )
```

This would require you to have a rule per potential `device` label, and
you cannot easily aggregate across these metrics.
[]{#ch17.xhtml#idm45207090150368 primary="device labels"
secondary="recording rules and" data-type="indexterm"}This basically
defeats the entire purpose of labels, one of the most powerful features
of Prometheus. You should avoid moving label values into metric names,
and if you want to limit what time series are returned based on a label
value, use a matcher at query time. Similarly do not move the `job`
label into the metric name.

Another antipattern is preaggregating every metric an application
exposes. While it is true that aggregation is a good idea to reduce
cardinality for performance, it is counterproductive to overdo
it.[]{#ch17.xhtml#idm45207090148384 primary="aggregation"
secondary="preaggregating every application metric"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090147392 primary="metrics"
secondary="use of in monitoring systems" data-type="indexterm"} In a
metrics-based monitoring system it is not uncommon to never use over 90%
of your
metrics,^[7](#ch17.xhtml#idm45207090146192){#ch17.xhtml#idm45207090146192-marker
data-type="noteref"}^ so aggregating everything by default is a waste of
resources and would require unnecessary maintenance as metrics are added
and removed over time. Instead, you should add aggregation as you need
it. Those other 90% of metrics are still accessible for when you end up
debugging some weird issue in the bowels of your system, and the only
cost of not aggregating them is that your queries on them will take
slightly longer.

The primary purpose of recording rules is to reduce cardinality, so
there is often not much point in having recording rules that still have
an `instance` label in their output.[]{#ch17.xhtml#idm45207090144576
primary="cardinality" secondary="reducing with recording rules"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090143552
primary="instance labels" secondary="in recording rule output"
secondary-sortas="recording" data-type="indexterm"} Querying ten time
series at query time isn't notably more expensive than querying one. If
you have metrics with high cardinality within a target, recording rules
with `instance` labels can make sense, though you should also consider
if those instrumentation labels should be removed on cardinality
grounds.

With rules such as:

``` {data-type="programlisting"}
    - record: job:x:max_over_time1h_rate5m
      expr: max_over_time(job:x:rate5m{job="j"}[1h])
```

from the preceding section, you might be tempted to change their
`evaluation_interval` to an hour in order to save
resources.[]{#ch17.xhtml#idm45207090139456 primary="evaluation_interval"
secondary="changing to an hour, reasons not to" data-type="indexterm"}
This is not a good idea for three reasons. First, as the input metric
came from a recording rule that already reduced cardinality, any
resource savings will likely be tiny in the grand scheme of things.
Second, Prometheus only guarantees that the rule will be executed once
an hour, not when in the hour it will be executed. As you likely want
results around the start of the hour, this, combined with staleness
handling, will not work out. Third, for the sake of your sanity, you
should aim for one interval inside your Prometheus servers.

The final pattern we would advise you to avoid is using recording rules
to fix poor metric names and labels. []{#ch17.xhtml#idm45207090137776
primary="labels" secondary="not using recording rules to fix bad labels"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090136704 primary="metrics"
secondary="not using recording rules to fix metric names"
data-type="indexterm"}This pattern loses the original timestamps of the
data, and makes it harder to figure out where a metric came from and
what it means. First, you should try improving the metrics at their
source, and if that is not possible for technical or political reasons,
consider whether using `metric_relabel_configs`, as described in
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"},
to improve them is worth the downsides of them differing from what
everyone else expects them to be named.

Unfortunately, there will always be cases where systems expose metrics
that are too far outside the Prometheus way of doing things, and you
have no choice but to fix them up however you
can.[]{#ch17.xhtml#idm45207090133536 primary="PromQL"
secondary="recording rules" startref="ix_PQLrecwhen"
tertiary="when to use"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090132016
primary="recording rules" secondary="when to use"
startref="ix_recruwhen" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Naming of Recording Rules" data-type="sect1"}
::: {#ch17.xhtml#naming_rules .sect1}
# Naming of Recording Rules

By using a good convention for naming recording rules, you can not only
tell at a glance what a given recording rule metric name means, but it
will also be easier to share your rules with others due to a shared
vocabulary.[]{#ch17.xhtml#ix_namerecru primary="naming recording rules"
data-type="indexterm"}[]{#ch17.xhtml#ix_PQLrecnm primary="PromQL"
secondary="recording rules" tertiary="naming"
data-type="indexterm"}[]{#ch17.xhtml#ix_recrunm
primary="recording rules" secondary="naming" data-type="indexterm"}

As mentioned in ["What Should I Name My
Metrics?"](#ch03.xhtml#metric_naming){data-type="xref"}, colons are
valid characters to have in metric names but are to be avoided in
instrumentation. The reason for this is so the user can take advantage
of them to add your own structure in recording rules. The convention we
use here balances precision and succinctness and comes from years of
experience.

The way this convention works is to have your metric names contain the
labels that are in play, followed by the metric name, followed by the
operations that have been performed on the metric. These three sections
are separated by colons, so you will always have either zero or two
colons in a metric name. For example, given the metric name:

``` {data-type="programlisting"}
job_device:node_disk_read_bytes:rate5m
```

We can tell that it has `job` and `device` labels, the metric it is
based off is `node_disk_read_bytes`, and it is a counter that
`rate(node_disk_read_bytes_​total[5m])` was applied to.
[]{#ch17.xhtml#idm45207090119536 primary="recording rules"
secondary="naming" tertiary="level, metric, and operations"
data-type="indexterm"}These parts are the *level*, *metric*, and
*operations*:

level

:   The level indicates the aggregation level of the metric by the
    labels it has. This will always[]{#ch17.xhtml#idm45207090114848
    primary="level (recording rule names)"
    data-type="indexterm"}[]{#ch17.xhtml#idm45207090114176
    primary="aggregation" secondary="level of in recording rule names"
    data-type="indexterm"} include the instrumentation labels (if they
    have not been aggregated away yet), the `job` label that should be
    present, and any other target labels that are relevant. Which target
    labels to include depends on context. If you have an `env` label
    across all your targets that doesn't affect your rules, then there's
    no need to bloat your metric names with it. But if a job was broken
    up by a `shard` label, you should probably include
    it.[]{#ch17.xhtml#idm45207090111664
    primary="metric (recording rule names)" data-type="indexterm"}

metric

:   The metric is just that---the metric or time series name. It's
    normal to remove the `_total` on counters to make things more
    succinct, but otherwise this should be the exact metric name. The
    benefits of keeping the metric name is that it is then easy to
    search your code base for that metric name, and vice versa if you
    are looking at code to find if the metric has been aggregated. For
    ratios you would use `foo_per_bar`, but there's a special rule for
    dealing with `_sum` and `_count` ratios.

operations

:   The operations are a list of functions and aggregators that have
    been applied to the metric, the most recent
    first.[]{#ch17.xhtml#idm45207090106016
    primary="operations (recording rule names)" data-type="indexterm"}
    If you have two `sum` or `max` operations, you only need to list
    one, as a sum of a sum is still a sum. Since sum is the default
    aggregation, you generally don't need to list it. But if you have no
    other operation to use, or haven't applied any operations yet, `sum`
    is a good default. Depending on what operations you plan on applying
    at other levels, `min` and `max` can make sense for a base metric
    name. The operation you should use for division is `ratio`.

To take some examples, if you had a `foo_total` counter with a `bar`
instrumentation label, then aggregating away the `instance` label would
look like:

``` {data-type="programlisting"}
- record: job_bar:foo:rate5m
  expr: sum without(instance)(rate(foo_total{job="j"}[5m]))
```

Going from there, to aggregate away the `bar` label would look like:

``` {data-type="programlisting"}
- record: job:foo:rate5m
  expr: sum without(bar)(job_bar:foo:rate5m{job="j"})
```

You can start to see some of the advantages of this approach. It is
clear from inspection that the label handling is as expected here, as
the input time series had `job_bar` as the level, `bar` was removed
using a `without` clause, and the output had `job` as the level. In more
complex rules and hierarchies this can be helpful to spot mistakes. For
example, the rule:

``` {data-type="programlisting"}
- record: job:foo_per_bar:ratio_rate5m
  expr: >
    (
        job:foo:rate5m{job="j"}
      /
        job:bar:rate10m{job="j"}
    )
```

seems to be following the naming scheme for ratios, but there is a
mismatch between the `rate5m` and the `rate10m`, which you should notice
and realize that this expression and the resulting recording rule don't
make sense. A correct ratio might look like:

``` {data-type="programlisting"}
- record: job_mountpoint:node_filesystem_avail_bytes_per_
              node_filesystem_size_bytes:ratio
  expr: >
    (
        job_mountpoint:node_filesystem_avail_bytes:sum{job="node"}
      /
        job_mountpoint:node_filesystem_size_bytes:sum{job="node"}
    )
```

Here you can see that the numerator and denominator have the same level
and operations, which are propagated to the output metric
name.^[8](#ch17.xhtml#idm45207090092320){#ch17.xhtml#idm45207090092320-marker
data-type="noteref"}^ Here the `sum` is removed, as it doesn't tell you
anything. This would not be the case if there was a `rate5m` operation
in the input metrics.

Using the preceding notation for average event sizes would be a bit
wordy, so instead the metric name is preserved and `mean5m` is used as
the output operation as it is based on a `rate5m` and is thus a mean
over 5 minutes:

``` {.pagebreak-before data-type="programlisting"}
- record: job_instance:go_gc_duration_seconds:mean5m
  expr: >
    (
        job_instance:go_gc_duration_seconds_sum:rate5m{job="prometheus"}
      /
        job_instance:go_gc_duration_seconds_count:rate5m{job="prometheus"}
    )
```

If you later saw the rule:

``` {data-type="programlisting"}
- record: job:go_gc_duration_seconds:mean5m
  expr:
    avg without(instance)(
      job_instance:go_gc_duration_seconds:mean5m{job="prometheus"}
    )
```

it would be immediately obvious that this is attempting to take an
average of an average, which doesn't make
sense.[]{#ch17.xhtml#idm45207090085712 primary="averages"
secondary="attempt to average averages" data-type="indexterm"} The
correct aggregation would be:

``` {data-type="programlisting"}
- record: job:go_gc_duration_seconds:mean5m
  expr:
    (
        sum without(instance)(
          job_instance:go_gc_duration_seconds_sum:rate5m{job="prometheus"})
        )
      /
        sum without(instance)(
          job_instance:go_gc_duration_seconds_count:rate5m{job="prometheus"})
        )
    )
```

You should sum to aggregate, and only perform division for averaging at
the last step of your calculation.

While the preceding cases are straightforward, like metric naming in
general, once you get off the beaten track, recording rule naming can be
more of an art than a science. []{#ch17.xhtml#idm45207090082736
primary="metrics" secondary="naming" data-type="indexterm"}You should
endeavor to ensure that your recording rule names are clear in what
their semantics and labels are, while also attempting to make it easy to
tie back recording rule names to the code that produced the original
metrics.

Aside from the very rare exception (see ["Rules for
APIs"](#ch17.xhtml#rules_for_apis){data-type="xref"}), metric names
should indicate the identity of a metric name so that you can know what
it is. Metric names should not be used as a way to store annotations for
policy.

For example, you should not feel tempted to add `:federate` or
`:longterm` or similar to metric names to indicate that you want such
and such a metric transferred to another system. This bloats metric
names, and will cause problems when your policy changes. Instead, define
and implement your policy via matchers when extracting the data, such
as, say, pulling all metric names matching `job:.*`, rather than trying
to micro-optimize which exact metrics will and won't be fetched. By the
time a metric has been through a recording rule, it has likely been
aggregated sufficiently that its cardinality is negligible, and thus it
is probably not worth your time to worry about the resource costs
downstream.[]{#ch17.xhtml#idm45207090077984
primary="naming recording rules" startref="ix_namerecru"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090077008 primary="PromQL"
secondary="recording rules" startref="ix_PQLrecnm" tertiary="naming"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090075520
primary="recording rules" secondary="naming" startref="ix_recrunm"
data-type="indexterm"}

Now that you know how to use recording rules, the next chapter will look
at alerting rules. Alerting rules also live in rule groups, and have a
similar syntax.[]{#ch17.xhtml#idm45207090073920 primary="PromQL"
secondary="recording rules" startref="ix_PQLrec"
data-type="indexterm"}[]{#ch17.xhtml#idm45207090072672
primary="recording rules" startref="ix_recru" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch17.xhtml#idm45207090306512-marker)^ Zero groups or zero rules in
group is technically possible, but serves no purpose.

^[2](#ch17.xhtml#idm45207090273504-marker)^ However, `labels` is used in
virtually all alerting rules.

^[3](#ch17.xhtml#idm45207090195712-marker)^ The `>` here is one of the
ways to have multiline strings in YAML.

^[4](#ch17.xhtml#idm45207090187040-marker)^ Performance-wise, many small
scrapes staggered over time are better than the samples from all those
scrapes being combined into one massive scrape.

^[5](#ch17.xhtml#idm45207090186128-marker)^ This can be done via
federation, remote write relabeling, or you could delete time series you
are no longer interested in via the API. As always, be careful when
deleting metrics.

^[6](#ch17.xhtml#idm45207090183968-marker)^ Prior to Prometheus 2.0 this
approach was not practical. There was no notion of rule groups, so you
couldn't guarantee that one rule would only run after another rule had
completed.

^[7](#ch17.xhtml#idm45207090146192-marker)^ Brian has heard numbers
around this mark from multiple monitoring systems.

^[8](#ch17.xhtml#idm45207090092320-marker)^ Arguably, you could remove
the `_bytes` here as it cancels out, but that might make it harder to
find the original metrics in the source code.
:::
:::
:::

[]{#part05.xhtml}

::: {#part05.xhtml#sbo-rt-content}
::: {#part05.xhtml#part5 .part pdf-bookmark="Part V. Alerting" data-type="part"}
# [Part V. ]{.label}Alerting

If you want to be woken up at 3 a.m. by your monitoring
system,^[1](#part05.xhtml#idm45207090070256){#part05.xhtml#idm45207090070256-marker
data-type="noteref"}^ these are the chapters for you.

Building on the previous chapter,
[Chapter 18](#ch18.xhtml#alerting_rules_chapter){data-type="xref"}
covers alerting rules in Prometheus, which offer you the ability to
alert on far more than simple thresholds.

Once you have alerts firing in Prometheus, the Alertmanager converts
those into notifications while attempting to group and throttle
notifications to increase the value of each notification, as explained
in [Chapter 19](#ch19.xhtml#alertmanager_chapter){data-type="xref"}.

::: {data-type="footnotes"}
^[1](#part05.xhtml#idm45207090070256-marker)^ Hopefully when there's a
true emergency.
:::
:::
:::

[]{#ch18.xhtml}

::: {#ch18.xhtml#sbo-rt-content}
::: {#ch18.xhtml#alerting_rules_chapter .chapter}
# [Chapter 18. ]{.label}Alerting

Back in ["What Is
Monitoring?"](#ch01.xhtml#what_is_monitoring){data-type="xref"} we
stated that alerting was one of the components of monitoring, allowing
you to notify a human when there is a
problem.[]{#ch18.xhtml#idm45207090063856 primary="alerting"
data-type="indexterm"} Prometheus allows you to define conditions in the
form of PromQL expressions that are continuously evaluated, and any
resulting time series become alerts. This chapter will show you how to
configure *alerts* in Prometheus.

As you saw from the example in
["Alerting"](#ch02.xhtml#intro_alerting){data-type="xref"}, Prometheus
is not responsible for sending out *notifications* such as emails, chat
messages, or pages. []{#ch18.xhtml#idm45207090060880
primary="notifications"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090060176
primary="Alertmanager"
secondary="Prometheus and Alertmanager architecture"
data-type="indexterm"}That role is handled by the *Alertmanager*.

Prometheus is where your logic to determine what is or isn't alerting is
defined. Once an alert is *firing* in Prometheus, it is sent to an
Alertmanager, which can take in alerts from many Prometheus servers. The
Alertmanager then groups alerts together and sends you throttled
notifications
([Figure 18-1](#ch18.xhtml#am_architecture_diagram){data-type="xref"}).

<figure>
<div id="ch18.xhtml#am_architecture_diagram" class="figure">
<img src="assets/pur2_1801.png" width="600" height="206"
alt="Prometheus and Alertmanager architecture." />
<h6><span class="label">Figure 18-1. </span>Prometheus and Alertmanager
architecture</h6>
</div>
</figure>

This architecture shown in
[Figure 18-1](#ch18.xhtml#am_architecture_diagram){data-type="xref"}
allows you not only flexibility, but also the ability to have a single
notification based on alerts from multiple different Prometheus servers.
For example, if you had an issue propagating serving data to all of your
datacenters, you could configure your alert grouping so that you got
only a single notification rather than being spammed by a notification
for each datacenter you have.

::: {.section pdf-bookmark="Alerting Rules" data-type="sect1"}
::: {#ch18.xhtml#alerting_rules .sect1}
# Alerting Rules

Alerting rules are similar to recording rules, which were covered in
[Chapter 17](#ch17.xhtml#promql_rules_chapter){data-type="xref"}.
[]{#ch18.xhtml#ix_alrtru primary="alerting rules"
data-type="indexterm"}You place alerting rules in the same rule groups
as recording rules, and can mix and match as you see
fit.[]{#ch18.xhtml#idm45207090048880 primary="groups"
secondary="alerting rules in"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090047936
primary="recording rules" data-type="indexterm"} For example, it is
normal to have all the rules and alerts for a job in one
group:^[1](#ch18.xhtml#idm45207090047136){#ch18.xhtml#idm45207090047136-marker
data-type="noteref"}^

``` {data-type="programlisting"}
groups:
 - name: node_rules
   rules:
    - record: job:up:avg
      expr: avg without(instance)(up{job="node"})
    - alert: ManyInstancesDown
      expr: job:up:avg{job="node"} < 0.5
```

This defines an alert with the name `ManyInstancesDown` that will fire
if more than half of your Node Exporters are down. You can tell that it
is an alerting rule because it has an `alert` field rather than a
`record` field.[]{#ch18.xhtml#idm45207090043936 primary="record field"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090043200
primary="ManyInstancesDown alert"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090042528
primary="alert field" data-type="indexterm"}

In this example we are careful to use `without` rather than `by` so that
any other labels the time series have are preserved and will be passed
on to the Alertmanager.[]{#ch18.xhtml#idm45207090040496
primary="without clause" secondary="avg without" data-type="indexterm"}
Knowing details such as the job, environment, and cluster of your alert
is rather useful when you get the eventual notification.

For recording rules, you should avoid
filtering[]{#ch18.xhtml#idm45207090038800 primary="filtering"
secondary="avoiding in recording rule expressions"
data-type="indexterm"} in your expressions, as time series appearing and
disappearing are challenging to deal with. For alerting rules, filtering
is essential. If evaluating your alert expression results in an empty
instant vector, then no alerts will fire, but if there are any samples
returned, each of them will become an alert.

Due to this, a single alerting rule like:

``` {data-type="programlisting"}
- alert: InstanceDown
  expr: up{job="node"} == 0
```

automatically applies to every instance in the `node` job that service
discovery returns, and if you had a hundred down instances you would get
a hundred firing alerts. If on the next evaluation cycle some of those
instances are back up, those alerts are considered *resolved*.

An alert is identified across evaluation cycles by its labels and does
not include the metric name label `__name__`, but does include an
`alertname` label with the name of the
alert.[]{#ch18.xhtml#idm45207090033680 primary="alertname labels"
data-type="indexterm"}

In addition []{#ch18.xhtml#idm45207090031920 primary="ALERTS metric"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090031184
primary="ALERTS_FOR_STATE metric" data-type="indexterm"}to sending
alerts to the Alertmanager, your alerting rules will also populate two
metrics: `ALERTS` and `ALERTS_FOR_STATE`. In addition to all the labels
of your alert, an `alertstate` label is also added to `ALERTS`. The
`alertstate` label will have a value of `firing` for firing alerts and
`pending` for pending alerts, as discussed in
["for"](#ch18.xhtml#for){data-type="xref"}. Resolved alerts do not have
samples added to `ALERTS`. []{#ch18.xhtml#idm45207090025808
primary="firing alerts"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090025072
primary="pending alerts" data-type="indexterm"}While you can use
`ALERTS` in your alerting rules as you would any other metric, we would
advise caution as it may indicate that you are overcomplicating your
setup.

The value of `ALERT_FOR_STATE` is the Unix timestamp when the alert
started. That metric is used internally by Prometheus to restore the
state of alerts after a restart.

::: {.note data-type="note"}
###### Note

Correct staleness handling for resolved alerts in `ALERTS` depends on
alerts always firing from the same alerting rule.
[]{#ch18.xhtml#idm45207090021088 primary="staleness"
secondary="for resolved alerts" secondary-sortas="resolved"
data-type="indexterm"}If you have multiple alerts with the same name in
a rule group, and a given alert can come from more than one of those
alerting rules, then you may see odd behavior from
`ALERTS`.^[2](#ch18.xhtml#idm45207090019056){#ch18.xhtml#idm45207090019056-marker
data-type="noteref"}^
:::

If you []{#ch18.xhtml#idm45207090017808 primary="notifications"
secondary="sending only at certain times for an alert"
data-type="indexterm"}want notifications for an alert to be sent only at
certain times of the day, the Alertmanager does not support routing
based on time. But you can use the date functions described in ["minute,
hour, day_of_week, day_of_month, day_of_year, days_in_month, month, and
year"](#ch16.xhtml#date_functions){data-type="xref"}. For example:

``` {data-type="programlisting"}
- alert: ManyInstancesDown
  expr: >
    (
        avg without(instance)(up{job="node"}) < 0.5
      and on()
        hour() >= 9 < 17
    )
```

This alert will only fire from 9 a.m. to 5 p.m. UTC.
[]{#ch18.xhtml#idm45207090014096 primary="and operator"
data-type="indexterm"}It is common to use `and` as discussed in ["and
operator"](#ch15.xhtml#and_operator){data-type="xref"} to combine
alerting conditions together. []{#ch18.xhtml#idm45207090011920
primary="on clause" data-type="indexterm"}Here we used `on()` as there
were no shared labels between the two sides of the `and`, which is not
usually the case.

For batch jobs, you will want to []{#ch18.xhtml#idm45207090009648
primary="batch jobs" secondary="alerting on not succeeding recently"
data-type="indexterm"}alert on the job not having succeeded recently:

``` {data-type="programlisting"}
- alert: BatchJobNoRecentSuccess
  expr: >
    time() - my_batch_job_last_success_time_seconds{job="batch"} > 86400*2
```

As discussed in ["Idempotency for Batch
Jobs"](#ch03.xhtml#batch_idempotency){data-type="xref"}, with idempotent
batch jobs you can avoid having to care about or be notified by a single
failure of a batch job.

::: {.section pdf-bookmark="for" data-type="sect2"}
::: {#ch18.xhtml#for .sect2}
## for

Metrics-based monitoring involves many race conditions---a scrape may
timeout due to a lost network packet, a rule evaluation could be a
little delayed due to process scheduling, and the systems you are
monitoring could have a brief blip.[]{#ch18.xhtml#idm45207090003856
primary="alerting rules" secondary="for field"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090002880
primary="race conditions"
data-type="indexterm"}[]{#ch18.xhtml#idm45207090002208
primary="for field" data-type="indexterm"}

You don't want to be woken up in the middle of the night for every
artifact or oddity in your systems; you want to save your energy for
real problems that affect users. Accordingly, firing alerts based on the
result of a single rule evaluation is rarely a good idea. This is where
the `for` field of alerting rules comes in:

``` {data-type="programlisting"}
groups:
- name: node_rules
  rules:
  - record: job:up:avg
    expr: avg without(instance)(up{job="node"})
  - alert: ManyInstancesDown
    expr: avg without(instance)(up{job="node"}) < 0.5
    for: 5m
```

The `for` field says that a given alert must be returned for at least
this long before it starts firing. Until the `for` condition is met, an
alert is considered to be `pending`. []{#ch18.xhtml#idm45207089997632
primary="pending alerts" data-type="indexterm"}An alert in the pending
state but that has not yet fired is not sent to the
Alertmanager.[]{#ch18.xhtml#idm45207089996656 primary="firing alerts"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089995984
primary="Alert status page" data-type="indexterm"} You can view the
current pending and firing alerts at *http://localhost:9090/alerts*,
which will look like
[Figure 18-2](#ch18.xhtml#alert_status){data-type="xref"} after you
click on an alert name.

Prometheus has no notion of hysteresis or flapping detection for
alerting.[]{#ch18.xhtml#idm45207089993600 primary="alerting"
secondary="choosing alert thresholds" data-type="indexterm"} You should
choose your alert thresholds so that the problem is sufficiently bad
that it is worth calling in a human, even if the problem subsequently
subsides.

We generally recommend using a `for` of at least 5 minutes for all of
your alerts. This will eliminate false positives from the majority of
artifacts, including from brief flaps. You may worry that this will
prevent you from jumping immediately on an issue, but keep in mind that
it will likely take you the guts of 5 minutes to wake up, boot up your
laptop, log in, connect to the corporate network, and start debugging.
Even if you are sitting in front of your computer all ready to go, it is
our experience that once your system is well developed, the alerts you
will handle will be nontrivial and it will take you at least 20--30
minutes just to get an idea of what is going on.

<figure>
<div id="ch18.xhtml#alert_status" class="figure">
<img src="assets/pur2_1802.png" width="600" height="282"
alt="Alert status page showing one firing and one pending alert" />
<h6><span class="label">Figure 18-2. </span>The Alert status page
displays firing and pending alerts</h6>
</div>
</figure>

While wanting to immediately jump on every problem is commendable, a
high rate of alerts will burn you and your team out and greatly reduce
your effectiveness. If you have an alert that requires a human to take
an action in less than 5 minutes, then you should work toward automating
that action as such a response time comes at a high human cost if you
can even reliably react in less than 5 minutes.

You may have some alerts that are less critical or a bit more noisy,
with which you would use a longer duration in the `for` field. As with
other durations and intervals, try to keep things simple. For example,
across all of your alerts a `5m`, `10m`, `30m`, and `1h` `for` are
probably sufficient in practice and there's not much point in
micro-optimizing by adding a `12m` or `20m` on top of that.

Because `for` requires that your alerting rule return the same time
series for a period of time, your `for` state can be reset if a single
rule evaluation does not contain a given time series. For example, if
you are using a gauge metric that comes directly from a target, if one
of the scrapes fails, then the `for` state will be reset if you had an
alerting rule such as:

``` {data-type="programlisting"}
- alert: FDsNearLimit
  expr: >
    process_open_fds > process_max_fds * .8
  for: 5m
```

To protect against []{#ch18.xhtml#idm45207089981200
primary="over_time functions" data-type="indexterm"}this gotcha you can
use the `_over_time` functions, discussed in ["Aggregation Over
Time"](#ch16.xhtml#_over_time){data-type="xref"}. Usually, you will want
to use `avg_over_time`, `last_over_time`, or `max_over_time`:

``` {data-type="programlisting"}
- alert: FDsNearLimit
  expr:
    (
        max_over_time(process_open_fds[5m])
      >
        max_over_time(process_max_fds[5m]) * 0.9
    )
  for: 5m
```

The `up` metric is special in that it is always present even if a scrape
fails, so you do not need to use an `_over_time` function.
[]{#ch18.xhtml#idm45207089975456 primary="scraping"
secondary="catching failed scrapes in Blackbox exporter"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089974352
primary="ProbeFailing alert"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089973680 primary="up"
secondary="failed scrapes and"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089972736
primary="Blackbox exporters"
secondary="catching filed scrapes or failed probes"
data-type="indexterm"}So if you were running the Blackbox Exporter, as
covered in ["Blackbox"](#ch10.xhtml#blackbox){data-type="xref"}, and
wanted to catch both failed scrapes or failed
probes,^[3](#ch18.xhtml#idm45207089970768){#ch18.xhtml#idm45207089970768-marker
data-type="noteref"}^ you could use:

``` {data-type="programlisting"}
- alert: ProbeFailing
  expr: up{job="blackbox"} == 0 or probe_success{job="blackbox"} == 0
  for: 5m
```
:::
:::

::: {.section pdf-bookmark="Alert Labels" data-type="sect2"}
::: {#ch18.xhtml#idm45207090005504 .sect2}
## Alert Labels

Just like with recording rules, you can specify `labels` for an alerting
rule. Using `labels` with recording rules is quite rare, but it is
standard practice with alerting rules.[]{#ch18.xhtml#ix_alrtrulbl
primary="alerting rules" secondary="labels for"
data-type="indexterm"}[]{#ch18.xhtml#ix_lblalrt primary="labels"
secondary="alert" data-type="indexterm"}

When routing alerts in the Alertmanager, as covered in ["Routing
Tree"](#ch19.xhtml#routing){data-type="xref"}, you do not want to have
to mention the name of every single alert you have individually in the
Alertmanager's configuration file. Instead, you should take advantage of
labels to indicate intent.

It is usual for you to have a `severity` label indicating whether an
alert is intended to page someone, and potentially wake them up, or that
it is a ticket that can be handled less
urgently.[]{#ch18.xhtml#idm45207089961808 primary="severity labels"
data-type="indexterm"}

For example, a single machine being down should not be an emergency, but
half your machines going down requires urgent investigation:

``` {data-type="programlisting"}
- alert: InstanceDown
  expr: up{job="node"} == 0
  for: 1h
  labels:
    severity: ticket
- alert: ManyInstancesDown
  expr: job:up:avg{job="node"} < 0.5
  for: 5m
  labels:
    severity: page
```

The `severity` label here does not have any special semantic meaning;
it's merely a label added to the alert that will be available for your
use when you configure the Alertmanager. As you add alerts in
Prometheus, you should set things up so you only need to add a
`severity` label to get the alert routed appropriately, and rarely have
to adjust your Alertmanager configuration.

In addition to the `severity` label, if a Prometheus can send alerts to
different teams, it's not unusual to have a `team` or `service` label.
[]{#ch18.xhtml#idm45207089956048 primary="team labels"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089955312
primary="service labels" data-type="indexterm"}If an entire Prometheus
was only sending alerts to one team, you would use external labels (as
discussed in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}).
[]{#ch18.xhtml#idm45207089953584 primary="env labels"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089952880
primary="region labels" data-type="indexterm"}There should be no need to
mention labels like `env` or `region` in alerting rules; they should
already either be on the alert due to being target labels that end up in
the output of the alerting expression, or will be added subsequently by
`external_labels`.[]{#ch18.xhtml#idm45207089950576
primary="external_labels" data-type="indexterm"}

Because all the labels of an alert, from both the expression and the
`labels`, define the identity of an alert, it is important that they do
not vary from evaluation cycle to evaluation cycle. Aside from such
alerts never satisfying the `for` field, they will spam the time series
database within Prometheus, the Alertmanager, and
you.[]{#ch18.xhtml#idm45207089948720 primary="for field"
data-type="indexterm"}

Prometheus []{#ch18.xhtml#idm45207089947632 primary="alerts"
secondary="defining multiple alerts with different thresholds and labels"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089946576
primary="FDsNearLimit alert" data-type="indexterm"}does not permit an
alert to have multiple thresholds, but you can define multiple alerts
with different thresholds and labels:

``` {data-type="programlisting"}
- alert: FDsNearLimit
  expr: >
    process_open_fds > process_max_fds * .95
  for: 5m
  labels:
    severity: page
- alert: FDsNearLimit
  expr: >
    process_open_fds > process_max_fds * .8
  for: 5m
  labels:
    severity: ticket
```

Note that if you are over 95% of the file descriptor limit, both of
these alerts will fire. Attempting to make only one of them fire would
be dangerous, because if the value was oscillating around 95%, then
neither alert would ever fire. []{#ch18.xhtml#idm45207089944560
primary="firing alerts" data-type="indexterm"}In addition, an alert
firing should be a situation where you have already decided it is worth
demanding a human take a look at an issue.
[]{#ch18.xhtml#idm45207089943568 primary="alerts" secondary="owners for"
data-type="indexterm"}If you feel this may be spammy, then you should
try to adjust the alerts themselves and consider if they are worth
having in the first place, rather than trying to put the genie back in
the bottle when the alert is already firing.

```{=html}
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar">
```
::: {#ch18.xhtml#alerts_need_owners .sidebar}
# Alerts Need Owners

Brian purposefully did not include a severity of `email` or `chat` in
the examples. To explain why, let him tell you a story:

I was once on a team that had to create a team mailing list every few
months. There was a mailing list for email alerts, but alerts sent there
didn't always get the attention that was desired as there were just too
many of them and responsibility was diffuse, which is to say it wasn't
actually anyone's job to take care of them. There were some alerts
considered important, but not important enough to page the on call
engineer. So these alerts were sent to the main team mailing list, in
the hope that someone would take a look. Fast forward a bit and the
exact same thing happened to the team mailing list, which now had
regular automated alerts coming in. At some point it got bad enough that
a new team mailing list was created, and this story repeated itself, at
which point this team had three email alert lists.

Based on this experience and that of others, I strongly discourage email
alerts and alerts that are assigned to a
team.^[4](#ch18.xhtml#idm45207089938240){#ch18.xhtml#idm45207089938240-marker
data-type="noteref"}^ Instead, I advocate having alert notifications
going to a ticketing system of some form, where they will be assigned to
a specific person whose job it is to handle them. I have also seen it
work out to have a daily email to the on call team members that lists
all currently firing alerts.

After an outage it is everyone's fault for not looking at the email
alerts,^[5](#ch18.xhtml#idm45207089936672){#ch18.xhtml#idm45207089936672-marker
data-type="noteref"}^ but still not anyone's responsibility. The key
point is that there needs to be ownership and not merely using email as
logging.

The same applies to chat messages for alerts, with messaging systems
such as IRC, Slack, and Telegram. Having your pages duplicated to your
messaging system is handy, and pages are rare. Having nonpages
duplicated has the same issues as email alerts, and is worse as it tends
to be more distracting. You can't filter chat messages away to a folder
you ignore like you do with emails.[]{#ch18.xhtml#idm45207089935104
primary="labels" secondary="alert" startref="ix_lblalrt"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089933856
primary="alerting rules" secondary="labels for" startref="ix_alrtrulbl"
data-type="indexterm"}
:::

```{=html}
</aside>
```
:::
:::

::: {.section pdf-bookmark="Annotations and Templates" data-type="sect2"}
::: {#ch18.xhtml#alert_templates .sect2}
## Annotations and Templates

Alert labels define the identity of the alert, so you can't use them to
provide additional information about the alert such as its current value
as that can vary from evaluation cycle to evaluation
cycle.[]{#ch18.xhtml#ix_tmplalrt primary="templating" secondary="alerts"
data-type="indexterm"}[]{#ch18.xhtml#ix_alrtruanntmpl
primary="alerting rules" secondary="annotations and templates"
data-type="indexterm"}[]{#ch18.xhtml#ix_annalrt
primary="annotations (alert)" data-type="indexterm"} Instead, you can
use *alert annotations*, which are similar to labels and can be used in
notifications. However, annotations are not part of an alert's identity,
so they cannot be used for grouping and routing in the Alertmanager.

The `annotations` field allows you to provide additional information
about an alert, such as a brief description of what is going wrong. In
addition, the values of the `annotations` field are templated using
[Go's templating system](https://oreil.ly/x0tjn).
[]{#ch18.xhtml#idm45207089924304 primary="Go"
secondary="templating system" data-type="indexterm"}This allows you to
format the value of the query to be more readable, or even perform
additional PromQL queries to add additional context to alerts.

Prometheus does not send the value of your alerts to the Alertmanager.
Because Prometheus allows you to use the full power of PromQL in
alerting rules, there is no guarantee that the value of an alert is in
any way useful or even meaningful. Labels define an alert rather than a
value, and alerts can be more than a simple threshold on a single time
series.

For example, you may wish to present the number of instances that are up
as a percentage in an annotation. It's not easy to do math in Go's
templating system, but you can prepare the value in the alert
expression:^[6](#ch18.xhtml#idm45207089922240){#ch18.xhtml#idm45207089922240-marker
data-type="noteref"}^

``` {data-type="programlisting"}
groups:
 - name: node_rules
   rules:
    - alert: ManyInstancesDown
      for: 5m
      expr: avg without(instance)(up{job="node"}) * 100 < 50
      labels:
        severity: page
      annotations:
        summary: 'Only {{printf "%.2f" $value}}% of instances are up.'
```

Here `$value` is the value of your
alert.[]{#ch18.xhtml#idm45207089918592 primary="labels"
secondary="alert" data-type="indexterm"} It is being passed to the
`printf`
function,^[7](#ch18.xhtml#idm45207089917056){#ch18.xhtml#idm45207089917056-marker
data-type="noteref"}^ which formats it nicely. Curly braces indicate
template expressions.[]{#ch18.xhtml#idm45207089914880 primary="query"
data-type="indexterm"}

In addition to `$value`, there is `$labels` with the labels of the
alert. For example, `$labels.job` would return the value of the `job`
label.

You can evaluate queries in annotation templates by using the `query`
function. Usually you will want to then `range` over the result
[]{#ch18.xhtml#idm45207089910448 primary="range loop"
data-type="indexterm"}of the query, which is a `for` loop:

``` {data-type="programlisting"}
- alert: ManyInstancesDown
  for: 5m
  expr: avg without(instance)(up{job="node"}) < 0.5
  labels:
    severity: page
  annotations:
    summary: 'More than half of instances are down.'
    description: >
      Down instances: {{ range query "up{job=\"node\"} == 0" }}
        {{ .Labels.instance }}
      {{ end }}
```

The value of the element will be in `.`, which is a single period or
full stop character. So `.Labels` is the labels of the current sample
from the instant vector, and `.Labels.instance` is the instance label of
that sample. `.Value` contains the value of the sample within the
`range` loop.

::: {.note data-type="note"}
###### Note

Every alert that results from an alerting rule has its templates
evaluated independently on every evaluation cycle. If you had an
expensive template for a rule producing hundreds of alerts, it could
cause you performance issues.
:::

You can also use annotations with static values, such as links to useful
dashboards or documentation:

``` {data-type="programlisting"}
- alert: InstanceDown
  for: 5m
  expr: up{job="prometheus"} == 0
  labels:
    severity: page
  annotations:
    summary: 'Instance {{$labels.instance}} of {{$labels.job}} is down.'
    dashboard: http://some.grafana:3000/dashboard/db/prometheus
```

In a mature system, attempting to provide all possible debug information
in an alert would not only be slow and confuse the on call person, but
would likely also be of minimal use for anything but the simplest of
issues. You should consider alert annotations and notifications
primarily as a signpost to point you in the right direction for initial
debugging. You can gain far more detailed and up-to-date information in
a dashboard than you can in a few lines of an alert notification.

*Notification templating* (covered in ["Notification
templates"](#ch19.xhtml#notification_templates){data-type="xref"}) is
another layer of templating performed in the
Alertmanager.[]{#ch18.xhtml#idm45207089900720 primary="notifications"
secondary="templating" data-type="indexterm"} In terms of what to put
where, think of notification templating as being an email with several
blanks that need to be filled in. Alert templates in Prometheus provide
values for those blanks.[]{#ch18.xhtml#idm45207089899360
primary="playbook for alerts" data-type="indexterm"}

For example, you may wish to have a
playbook^[8](#ch18.xhtml#idm45207089898240){#ch18.xhtml#idm45207089898240-marker
data-type="noteref"}^ for each of your alerts linked from the
notification, and you will probably name the wiki pages after the
alerts. You could add a `wiki` annotation to every alert, but any time
you find yourself adding the same annotation to every alerting rule, you
should probably be using notification templating in the Alertmanager
instead. The Alertmanager already knows the alert's name so it can
default to `wiki.mycompany/Alertname`, saving you from having to repeat
yourself in alerting rules. As with many things in configuration
management and monitoring, having consistent conventions across your
team and company makes life easier.

::: {.note data-type="note"}
###### Note

Alerting rule `labels` are also templated in
[]{#ch18.xhtml#idm45207089894592 primary="labels" secondary="alert"
data-type="indexterm"}the same fashion as `annotations`, but this is
only useful in advanced use cases, and you will almost always have
simple static values for `labels`. If you do use templating on `labels`,
it is important that the label values do not vary from evaluation cycle
to evaluation cycle.[]{#ch18.xhtml#idm45207089891968
primary="templating" secondary="alert" startref="ix_tmplalrt"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089890720
primary="annotations (alert)" startref="ix_annalrt"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089889776
primary="alerting rules" secondary="annotations and templates"
startref="ix_alrtruanntmpl" data-type="indexterm"}
:::
:::
:::

::: {.section pdf-bookmark="What Are Good Alerts?" data-type="sect2"}
::: {#ch18.xhtml#good_alerts .sect2}
## What Are Good Alerts?

In Nagios-style monitoring, it would be typical to alert on potential
issues such as high load average, high CPU usage, or a process not
running.[]{#ch18.xhtml#idm45207089886368 primary="alerts"
secondary="good alerts"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089885392
primary="alerting rules" secondary="good alerts" data-type="indexterm"}
These are all potential *causes* of problems, but they do not
necessarily indicate a problem that requires the urgent intervention by
a human that paging the on call person implies.

As systems grow ever more complex and dynamic, having alerts on every
possible thing that can go wrong is not tractable. Even if you could
manage to do so, the volume of false positives would be so high that you
and your team would get burned out and end up missing real problems
buried among the noise.

A better approach is to instead alert on *symptoms*.
[]{#ch18.xhtml#idm45207089882464 primary="symptoms, alerting on"
data-type="indexterm"}Your users do not care whether your load average
is high; they care if their cat videos aren't loading quickly enough.
[]{#ch18.xhtml#idm45207089881456 primary="metrics" secondary="alerts on"
data-type="indexterm"}By having alerts on metrics such as latency and
failures experienced by
users,^[9](#ch18.xhtml#idm45207089880384){#ch18.xhtml#idm45207089880384-marker
data-type="noteref"}^ you will spot problems that really matter, rather
than things that maybe might possibly indicate an issue.

For example, nightly cronjobs may cause CPU usage to spike, but with few
users at that time of day you probably will have no problems serving
them.[]{#ch18.xhtml#idm45207089878928 primary="cronjobs"
secondary="causing CPU usage to spike" data-type="indexterm"}
Conversely, intermittent packet loss can be tricky to alert on directly,
but will be fairly clearly exposed by latency metrics. If you have
Service-Level Agreements (SLAs) with your users, then those provide good
metrics to alert on and good starting points for your thresholds. You
should also have alerts to catch resource utilization issues, such as
running out of quota or disk space, and alerts to ensure that your
monitoring is
working.^[10](#ch18.xhtml#idm45207089877760){#ch18.xhtml#idm45207089877760-marker
data-type="noteref"}^

The ideal to aim for is that every page to the on call person, and every
alert ticket filed, requires intelligent human action. If an alert
doesn't require intelligence to resolve, then it is a prime candidate
for you to automate. As a nontrivial on call incident can take a few
hours to resolve, you should aim for less than two incidents per day.
For nonurgent alerts going to your ticketing system you don't have to be
as strict, but you wouldn't want too many more than you have pages.

If you find yourself responding to pages with "it went away," that is an
indication that the alert should not have fired in the first place. You
should consider bumping the threshold of the alert to make it less
sensitive, or potentially deleting the
alert.[]{#ch18.xhtml#idm45207089875600 primary="alerting"
secondary="how to approach, further information on"
data-type="indexterm"}

For further discussion of how to approach alerting on and managing
systems we would recommend reading ["My Philsophy on
Alerting"](https://oreil.ly/WYPVf) by Rob Ewaschuk. Rob also wrote
Chapter 6 of *Site Reliability Engineering* (Betsy Beyer et al, eds.,
O'Reilly), which also has more general advice on how to manage
systems.[]{#ch18.xhtml#idm45207089872768 primary="alerting rules"
startref="ix_alrtru" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Configuring Alertmanagers in Prometheus" data-type="sect1"}
::: {#ch18.xhtml#configuring_alertmanagers .sect1}
# Configuring Alertmanagers in Prometheus

You configure Prometheus with a list of Alertmanagers to talk to using
the same service discovery configuration covered in
[Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}.
[]{#ch18.xhtml#ix_alrtmgrcfg primary="Alertmanager"
secondary="configuring" data-type="indexterm"}For example, to configure
a single local Alertmanager, you might have a *prometheus.yml* that
looks like:

``` {data-type="programlisting"}
global:
  scrape_interval: 10s
  evaluation_interval: 10s
alerting:
  alertmanagers: 
   - static_configs:
      - targets: ['localhost:9093']
rule_files:
 - rules.yml
scrape_configs:
 - job_name: node
   static_configs:
    - targets:
      - localhost:9100
 - job_name: prometheus
   static_configs:
    - targets:
      - localhost:9090
```

[![1](assets/1.png){height="12" width="12"}](#ch18.xhtml#co_alerting_CO1-1){#ch18.xhtml#callout_alerting_CO1-1 .co}

:   This section of the configuration is focused on setting up the
    discovery of the Alertmanagers.

Here the `alertmanagers` field works similarly to a scrape config, but
there is no `job_name` and labels output from relabeling have no impact
since there is no notion of target labels when discovering the
Alertmanagers to send alerts to. []{#ch18.xhtml#idm45207089859088
primary="relabeling"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089858384
primary="alertmanagers field" data-type="indexterm"}Accordingly, any
relabeling will typically only involve `drop` and `keep` actions.

You can have more than one Alertmanager, which will be further covered
in ["Alertmanager
Clustering"](#ch21.xhtml#alertmanager_clustering){data-type="xref"}.
Prometheus will send all alerts to all the configured Alertmanagers.

The `alerting` field also has `alert_relabel_configs`, which is
relabeling, as covered in
["Relabeling"](#ch08.xhtml#relabeling){data-type="xref"}, but applied to
alert labels. You can adjust alert labels, or even drop alerts.
[]{#ch18.xhtml#idm45207089853136 primary="alerting field"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089852464
primary="alert_relabel_configs"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089851792
primary="relabeling" secondary="alert_relabel_configs"
data-type="indexterm"}For example, you may wish to have informational
alerts that never make it outside your Prometheus:

``` {data-type="programlisting"}
alerting:
  alertmanagers:
   - static_configs:
      - targets: ['localhost:9093']
  alert_relabel_configs:
   - source_labels: [severity]
     regex: info
     action: drop
```

You could use this to add `env` and `region` labels to all your alerts,
saving you hassle elsewhere, but there is a better way to do this using
`external_labels`.

::: {.section pdf-bookmark="External Labels" data-type="sect2"}
::: {#ch18.xhtml#external_labels .sect2}
## External Labels

*External labels* are labels []{#ch18.xhtml#idm45207089845568
primary="Alertmanager" secondary="configuring"
tertiary="external labels"
data-type="indexterm"}[]{#ch18.xhtml#idm45207089844288
primary="external labels" data-type="indexterm"}applied as defaults when
your Prometheus talks to other systems, such as the Alertmanager,
federation, remote read, and remote
write,^[11](#ch18.xhtml#idm45207089843344){#ch18.xhtml#idm45207089843344-marker
data-type="noteref"}^ but not the HTTP query APIs. External labels are
the identity of Prometheus, and every single Prometheus in your
organization should have unique external labels.
[`external_labels`]{.keep-together} is part of
[]{#ch18.xhtml#idm45207089840176
primary="global section (prometheus.yml)" data-type="indexterm"}the
`global` section of *prometheus.yml*:

``` {data-type="programlisting"}
global:
  scrape_interval: 10s
  evaluation_interval: 10s
  external_labels:
    region: eu-west-1
    env: prod
    team: frontend
alerting:
  alertmanagers:
   - static_configs:
      - targets: ['localhost:9093']
```

It is easiest to have []{#ch18.xhtml#idm45207089837152
primary="region labels" data-type="indexterm"}labels such as `region` in
your `external_labels` as you don't have to apply them to every single
target that is scraped, keep them in mind when writing PromQL, or add
them to every single alerting rule within a Prometheus. This saves you
time and effort, and also makes it easier to share recording and
alerting rules across different Prometheus servers as they aren't tied
to one environment or even to one organization. If a potential external
label varies within a Prometheus, then it should probably be a target
label instead.

Since external labels are applied after alerting rules are
evaluated,^[12](#ch18.xhtml#idm45207089835008){#ch18.xhtml#idm45207089835008-marker
data-type="noteref"}^ they are not available in alert templating. Alerts
should not care which of your Prometheus servers they are being
evaluated in, so this is OK. The Alertmanager will have access to the
external labels just like any other label in its notification templates,
and that is the appropriate place to work with them.

External labels are only defaults; if one of your time series already
has a label with the same name, then that external label will not apply.
Accordingly, we advise not having targets whose label names overlap with
your external labels.[]{#ch18.xhtml#idm45207089832736
primary="Alertmanager" secondary="configuring in"
startref="ix_alrtmgrcfg" data-type="indexterm"}

Now that you know how to have Prometheus evaluate and fire useful
alerts, the next step is to configure the Alertmanager to convert them
into notifications, the topic of the next chapter.
:::
:::
:::
:::

::: {data-type="footnotes"}
^[1](#ch18.xhtml#idm45207090047136-marker)^ If a group gets too large to
be calculated in one interval, you may have to split it up if trimming
it down is not an option.

^[2](#ch18.xhtml#idm45207090019056-marker)^ This also applies to
recording rules, but it is quite rare to have multiple recording rules
with the same metric name in a group.

^[3](#ch18.xhtml#idm45207089970768-marker)^ While the Blackbox Exporter
should return a response before it times out, things can always go
wrong, such as the network being slow or the Blackbox Exporter being
down.

^[4](#ch18.xhtml#idm45207089938240-marker)^ I am also strongly against
any form of email that was not written by hand by a human going to team
mailing lists, including from alerts, pull requests, and bug/issue
trackers.

^[5](#ch18.xhtml#idm45207089936672-marker)^ Invariably among the
thousands of spam alerts that everyone ignored there was one alert that
foreshadowed the outage. Hindsight is 20/20, but to spot that email you
would have had to also investigate the thousands of irrelevant
notifications.

^[6](#ch18.xhtml#idm45207089922240-marker)^ For more advanced cases than
this, you []{#ch18.xhtml#idm45207089921632 primary="and operator"
data-type="indexterm"}can consider using the `and` operator with the
value for templating usage on the lefthand side and the alerting
expression on the righthand side.

^[7](#ch18.xhtml#idm45207089917056-marker)^ Despite the name, this is
actually a `sprintf` as it returns the output rather than writing it
out. This allows you to build up a query that is passed to the `query`
function using `printf`.

^[8](#ch18.xhtml#idm45207089898240-marker)^ A playbook is a document or
set of procedures that outlines the steps to be taken in response to a
specific type of incident.

^[9](#ch18.xhtml#idm45207089880384-marker)^ Users don't have to be
customers of your company, such as if you are running an internal
service within a company.

^[10](#ch18.xhtml#idm45207089877760-marker)^ We will demonstrate this in
detail in ["Meta- and
Cross-Monitoring"](#ch21.xhtml#metamonitoring){data-type="xref"}.

^[11](#ch18.xhtml#idm45207089843344-marker)^ Covered in ["Going Global
with Federation"](#ch21.xhtml#federation){data-type="xref"} and
["Long-Term Storage"](#ch21.xhtml#long_term_storage){data-type="xref"}.

^[12](#ch18.xhtml#idm45207089835008-marker)^ `alert_relabel_configs`
happens after `external_labels`.
:::
:::
:::

[]{#ch19.xhtml}

::: {#ch19.xhtml#sbo-rt-content}
::: {#ch19.xhtml#alertmanager_chapter .chapter}
# [Chapter 19. ]{.label}Alertmanager

In [Chapter 18](#ch18.xhtml#alerting_rules_chapter){data-type="xref"}
you saw how to define alerting rules in Prometheus, which result in
alerts being sent to the Alertmanager.[]{#ch19.xhtml#ix_Alrtmgr2
primary="Alertmanager" data-type="indexterm"} It is the responsibility
of your Alertmanager to take in all the alerts from all of your
Prometheus servers and convert them to notifications such as emails,
chat messages, and pages.
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"} gave
you a brief introduction to using the Alertmanager, but in this chapter
you will learn how to configure and use the full power of it.

::: {.section pdf-bookmark="Notification Pipeline" data-type="sect1"}
::: {#ch19.xhtml#idm45207089825888 .sect1}
# Notification Pipeline

The Alertmanager does more for you than blindly convert alerts into
notifications on a one-to-one basis. []{#ch19.xhtml#idm45207089824464
primary="Alertmanager" secondary="notification pipeline"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089823488
primary="notifications" secondary="Alertmanager notification pipeline"
data-type="indexterm"}In an ideal world you would receive exactly one
notification for each production incident. While this is a stretch, the
Alertmanager tries to get you there by providing you with a controllable
pipeline for how your alerts are processed as they become notifications.
[]{#ch19.xhtml#idm45207089822400 primary="labels"
secondary="for Alertmanager" secondary-sortas="Alertmanager"
data-type="indexterm"}Just as labels are at the core of Prometheus
itself, labels are also key to the Alertmanager:

Inhibition

:   On occasion, even when using symptom-based alerting, you will want
    to prevent notifications for some alerts if another more severe
    alert is firing, such as preventing alerts for your service if a
    datacenter it is in is failing but is also receiving no traffic.
    []{#ch19.xhtml#idm45207089818816 primary="inhibitions"
    data-type="indexterm"}This is the role of *inhibition*.

Silencing

:   If you already know about a problem or are taking a service down for
    maintenance, there's no point in paging the on call person about
    it.[]{#ch19.xhtml#idm45207089815968 primary="silencing alerts"
    data-type="indexterm"} *Silences* allow you to ignore certain alerts
    for a while, and are added via the Alertmanager's web
    interface.[]{#ch19.xhtml#idm45207089814608 primary="silences"
    data-type="indexterm"}

Routing

:   It is intended that you would run one Alertmanager per organization,
    but it wouldn't do for all of your notifications to go to one
    place.[]{#ch19.xhtml#idm45207089812464 primary="routing"
    data-type="indexterm"} Different teams will want their notifications
    delivered to different places; and even within a team you might want
    alerts for production and development environments handled
    differently. The Alertmanager allows you to configure this with a
    *routing tree*.[]{#ch19.xhtml#idm45207089810992
    primary="routing tree" data-type="indexterm"}

Grouping

:   You now have the production alerts[]{#ch19.xhtml#idm45207089808976
    primary="grouping" secondary="of alerts in Alertmanager"
    secondary-sortas="alerts" data-type="indexterm"} for your team going
    to a route. Getting an individual notification for each of the
    machines in a
    rack^[1](#ch19.xhtml#idm45207089807472){#ch19.xhtml#idm45207089807472-marker
    data-type="noteref"}^ that failed would be spammy, so you could have
    the Alertmanager group alerts and only get one notification per
    rack, one notification per datacenter, or even one notification
    globally about the unreachable machines.

Throttling and repetition

:   You have your group of alerts that are firing due to the rack of
    machines being down, and the alert for one of the machines on the
    rack comes in after you have already sent out the notification.
    []{#ch19.xhtml#idm45207089804864 primary="throttling notifications"
    data-type="indexterm"}If Alertmanager sent a new notification every
    time a new alert comes in from a group, that would defeat the
    purpose of grouping. Instead, the Alertmanager will throttle
    notifications for a given group so you don't get spammed.

    In an ideal world all notifications would be handled promptly, but
    in reality the on call person or other system might let an issue
    slip through the cracks.[]{#ch19.xhtml#idm45207089803344
    primary="repetition of notifications" data-type="indexterm"} The
    Alertmanager will repeat notifications so that they don't get lost
    for too long.

Notification

:   Now that your alerts have been inhibited, silenced, routed, grouped,
    and throttled, they finally get to the stage of being sent out as
    notifications through a *receiver*. []{#ch19.xhtml#idm45207089800704
    primary="notifications"
    data-type="indexterm"}[]{#ch19.xhtml#idm45207089799968
    primary="receivers" data-type="indexterm"}Notifications are
    templated, allowing you to customize their content and emphasize the
    details that matter to you.
:::
:::

::: {.section pdf-bookmark="Configuration File" data-type="sect1"}
::: {#ch19.xhtml#idm45207089798784 .sect1}
# Configuration File

As with all the other configurations you have seen, the Alertmanager is
configured via a YAML file often called
*alertmanager.yml*.[]{#ch19.xhtml#ix_Alrtmgr2cfg primary="Alertmanager"
secondary="configuration file"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089794992
primary="check-config (amtool)" data-type="indexterm"} As with
Prometheus, the configuration file can be reloaded at runtime by sending
a `SIGHUP` or sending an HTTP `POST` to the */-/reload* endpoint. To
detect bad configuration files in advance, you can use the
`amtool check-config` command to check your
*alertmanager.yml*.^[2](#ch19.xhtml#idm45207089792048){#ch19.xhtml#idm45207089792048-marker
data-type="noteref"}^

For example, a minimal configuration[]{#ch19.xhtml#idm45207089790832
primary="Alertmanager" secondary="configuration file"
tertiary="minimal configuration example"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089789616
primary="email alerts" data-type="indexterm"} that sends everything to
an email address using a local SMTP server would look like:

``` {code-language="yaml" data-type="programlisting"}
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'yourprometheus@example.org' 

route:
  receiver: example-email

receivers:
 - name: example-email
   email_configs:
    - to: 'youraddress@example.org' 
```

[![1](assets/1.png){height="12" width="12"}](#ch19.xhtml#co_alertmanager_CO1-1){#ch19.xhtml#callout_alertmanager_CO1-1 .co}

:   The email address that will be used as the *From* field.

[![2](assets/2.png){height="12" width="12"}](#ch19.xhtml#co_alertmanager_CO1-2){#ch19.xhtml#callout_alertmanager_CO1-2 .co}

:   The email address the emails will be sent to.

You must always have at least one route and one receiver. There are
various global settings, which are almost all defaults for the various
types of receivers. We will now cover the various other parts of the
configuration file. You can find a full *alertmanager.yml* combining the
examples in this chapter [on GitHub](https://oreil.ly/hQduB).

::: {.section pdf-bookmark="Routing Tree" data-type="sect2"}
::: {#ch19.xhtml#routing .sect2}
## Routing Tree

The `route` field specifies the top-level, *fallback*, or *default*
route. Routes form a tree, so you can and usually will have multiple
routes below that.[]{#ch19.xhtml#ix_Alrtmgr2cfgrt primary="Alertmanager"
secondary="configuration file" tertiary="routing tree"
data-type="indexterm"}[]{#ch19.xhtml#ix_rttree primary="routing tree"
secondary="configuring for Alertmanager"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089683088
primary="fallback (or default) route"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089682400
primary="default route" data-type="indexterm"} For example, you could
have:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  routes:
   - matchers:
       - severity = page
     receiver: team-pager
   - matchers:
       - severity = ticket
     receiver: team-ticket
```

When an alert arrives, it starts at the default route and tries to match
against its first *child route*, which is defined in the (possibly
empty) `routes` field. []{#ch19.xhtml#idm45207089612688
primary="child routes"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089612048
primary="routes field" data-type="indexterm"}If your alert has a label
that is exactly `severity="page"`, it matches this route and matching
halts, as this route has no children to
consider.[]{#ch19.xhtml#idm45207089653904 primary="severity labels"
data-type="indexterm"}

If your alert does not have a `severity="page"` label, then the next
child route of the default route is checked; in this case, for a
`severity="ticket"` label. If this matches your alert, then matching
will also halt. Otherwise, since all the child routes have failed to
match, matching goes back up the tree and matches the default route.
This is known as a *post-order tree transversal*, which is to say that
children are checked before their parent, and the first match
wins.[]{#ch19.xhtml#idm45207089651696
primary="post-order tree transversal" data-type="indexterm"}

Next to the `=` operator in matchers, there are other
[]{#ch19.xhtml#idm45207089650000 primary="matchers"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089649296
primary="regular expressions" secondary="matchers"
data-type="indexterm"}operators like `!=`, `=~`, and `!~` . `=~`
requires that the given label match the given regular expression, and
`!~` requires that it does not match the given regular expression. As
with
almost^[3](#ch19.xhtml#idm45207089646048){#ch19.xhtml#idm45207089646048-marker
data-type="noteref"}^ all other places, regular expressions are fully
anchored. For a refresher on regular expressions, see ["Regular
Expressions"](#ch08.xhtml#regex){data-type="xref"}.

You could use `=~` if there were variants in what label values were used
for a given purpose, such as if some teams used `ticket`, others used
`issue`, and others had yet to be convinced that `email` was possibly
not the best place to send notifications:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  routes:
   - matchers:
       - severity = page
     receiver: team-pager
   - matchers:
       - severity =~ "(ticket|issue|email)"
     receiver: team-ticket
```

Multiple matchers can be used in the same route, and alerts must satisfy
all of the match conditions.

::: {.note data-type="note"}
###### Note

All alerts must match some route, and the top-level route is the last
route checked, so it acts as a fallback that all alerts must
match.[]{#ch19.xhtml#idm45207089518688 primary="matchers"
secondary="error to use on default route"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089561600
primary="default route" secondary="error to use matchers on"
data-type="indexterm"} Thus it is an error for you to use `matchers` on
the default route.
:::

Rarely will it just be one team using an Alertmanager, and different
teams will want alerts routed differently. You should have a standard
label such as `team` or `service` across your organization that
distinguishes who owns what alerts. []{#ch19.xhtml#idm45207089558512
primary="alerts" secondary="owners for"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089557536
primary="external_labels"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089556864 primary="teams"
secondary="routing alerts and notifications to"
data-type="indexterm"}This label will usually but not always come from
`external_labels`, as discussed in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}. Using this
`team`-like label you would have a route per team, and then the teams
would have their own routing configuration below that:

``` {.pagebreak-before code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  routes:
   # Frontend team.
   - matchers:
       - team = frontend
     receiver: frontend-pager
     routes:
      - matchers:
          - severity = page
        receiver: frontend-pager
      - matchers:
          - severity = ticket
        receiver: frontend-ticket
   # Backend team.
   - matchers:
       - team = backend
     receiver: backend-pager
     routes:
      - matchers:
          - severity = page
          - env = dev
        receiver: backend-ticket
      - matchers:
          - severity = page
        receiver: backend-pager
      - matchers:
          - severity = ticket
        receiver: backend-ticket
```

The frontend team has a simple setup, with pages going to the pager,
tickets going to the ticketing system, and any pages with unexpected
`severity` labels going to the pager.

The backend team has customized things a little.
[]{#ch19.xhtml#idm45207089280624 primary="backend-ticket receiver"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089280016
primary="receivers" secondary="backend-ticket" data-type="indexterm"}Any
pages from the development environment will be sent to the
`backend-ticket` receiver, which is to say that they will be downgraded
to just tickets rather than
pages.^[4](#ch19.xhtml#idm45207089326528){#ch19.xhtml#idm45207089326528-marker
data-type="noteref"}^ In this way you can have alerts from different
environments routed differently in the Alertmanager, saving you from
having to customize alerting rules per environment. This approach allows
you to only have to vary the `external_labels` in most cases.

::: {data-type="tip"}
###### Tip

It can be a little challenging to come to grips with an existing routing
tree, particularly if it doesn't follow a standard structure. There is a
[visual routing tree editor](https://oreil.ly/KtvK-) on the Prometheus
website that can show you the tree and what routes alerts will follow on
it.[]{#ch19.xhtml#idm45207089323040 primary="routing tree"
secondary="visual editor for" data-type="indexterm"}
:::

Because such a configuration grows as you gain more teams, you may want
to write a utility to combine routing tree fragments together from
smaller files. YAML is a standard format with readily available
unmarshallers and marshallers, so this is not a difficult task.

There is one other setting we should mention in the context of
routing---`continue`. []{#ch19.xhtml#idm45207089320528
primary="continue setting (routing)" data-type="indexterm"}Usually the
first matching route wins, but if `continue: true` is specified, then a
match will not halt the process of finding a matching route. Instead, a
matching [`continue`]{.keep-together} route will be matched *and* the
process of finding a matching route will continue. In this way an alert
can be part of multiple routes. `continue` is primarily used to log all
alerts to another system:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  routes:
   # Log all alerts.
   - receiver: log-alerts
     continue: true
   # Frontend team.
   - matchers:
       - team = frontend
     receiver: frontend-pager
```

Once your alert has a route, the grouping, throttling, repetition, and
receiver for that route will apply to that alert and all the other
alerts that match that route. All settings for child routes are
inherited as defaults from their parent route, with the exception of
`continue`.

::: {.section pdf-bookmark="Grouping" data-type="sect3"}
::: {#ch19.xhtml#idm45207089245536 .sect3}
### Grouping

Your alerts have now arrived at their
route.[]{#ch19.xhtml#idm45207089244064 primary="routing tree"
secondary="configuring for Alertmanager" tertiary="grouping"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089242848
primary="grouping" secondary="of alerts in Alertmanager"
secondary-sortas="alerts" data-type="indexterm"} By default, the
Alertmanager will put all alerts for a route into a single group,
meaning you will get one big notification. While this may be OK in some
cases, usually you will want your notifications a bit more bite-sized
than that.

The `group_by` field allows you to specify a list of
[]{#ch19.xhtml#idm45207089240704 primary="group_by"
data-type="indexterm"}labels to group alerts by; this works in the same
way as the `by` clause that you can use with aggregation operators
(discussed in ["by"](#ch14.xhtml#by){data-type="xref"}). Typically you
will want to split out your alerts by one or more of alertname,
environment, and/or location.

An issue in production is unlikely to be related to an issue in
development, and similarly with issues in different datacenters
depending on the exact alert. []{#ch19.xhtml#idm45207089238144
primary="symptoms, alerting on" data-type="indexterm"}When alerting on
symptoms rather than causes, as encouraged by ["What Are Good
Alerts?"](#ch18.xhtml#good_alerts){data-type="xref"}, it is likely that
different alerts indicate different
incidents.^[5](#ch19.xhtml#idm45207089236512){#ch19.xhtml#idm45207089236512-marker
data-type="noteref"}^

To use this in practice, you might end up with a configuration such as:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  group_by: [team]
  routes:
   # Frontend team.
   - matchers:
       - team = frontend
     group_by: [region, env]
     receiver: frontend-pager
     routes:
      - matchers:
          - severity = page
        receiver: frontend-pager
      - matchers:
          - severity = ticket
        group_by: [region, env, alertname]
        receiver: frontend-ticket
```

Here the default route has its alerts grouped by the `team` label, so
that any team missing a route can be dealt with individually.
[]{#ch19.xhtml#idm45207089166928 primary="teams"
secondary="alerts grouped by"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089078512
primary="region labels"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089077840
primary="env labels" data-type="indexterm"}The frontend team has chosen
to group alerts based on the `region` and `env` labels. This `group_by`
will be inherited by their child routes, so all their tickets and pages
will also be grouped by `region` and `env`.

Generally, it is not a good idea to group by the `instance` label, since
that can get very spammy when there is an issue affecting an entire
application.[]{#ch19.xhtml#idm45207089073936 primary="instance labels"
secondary="grouping alerts and" data-type="indexterm"} However, if you
were alerting on machines being down in order to create tickets to have
a human physically inspect them, grouping by `instance` may make sense
depending on the inspection workflow.

::: {.note data-type="note"}
###### Note

You can disable grouping alerting in the Alertmanager by setting
`group_by` to
`[ ... ]`.^[6](#ch19.xhtml#idm45207089070352){#ch19.xhtml#idm45207089070352-marker
data-type="noteref"}^ However, grouping is a good thing, because it
reduces notification spam and allows you[]{#ch19.xhtml#idm45207089069632
primary="grouping" secondary="of alerts in Alertmanager"
secondary-sortas="alerts" tertiary="disabling" data-type="indexterm"} to
perform more focused incident response. []{#ch19.xhtml#idm45207089068048
primary="pager storm" data-type="indexterm"}It is far harder to miss a
notification about a new incident among a few pages than a hundred
pages.^[7](#ch19.xhtml#idm45207089067136){#ch19.xhtml#idm45207089067136-marker
data-type="noteref"}^

If you want to disable grouping due to your organization already having
something that fills the Alertmanager's role, you may be better off not
using the Alertmanager and working from the alerts sent by Prometheus
instead.
:::
:::
:::

::: {.section pdf-bookmark="Throttling and repetition" data-type="sect3"}
::: {#ch19.xhtml#idm45207089066096 .sect3}
### Throttling and repetition

When sending notifications for a group, you don't want to get a new
notification every time the set of firing alerts changes as that would
be too spammy.[]{#ch19.xhtml#idm45207089040416 primary="routing tree"
secondary="configuring for Alertmanager"
tertiary="throttling and repetition"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089039440
primary="throttling notifications" data-type="indexterm"} On the other
hand, neither do you only want to learn about additional alerts that
started firing many hours after the
fact.[]{#ch19.xhtml#idm45207089038704 primary="groups"
secondary="throttling notifications for" data-type="indexterm"}

There are two settings you can adjust to control how the Alertmanager
throttles notifications for a group: `group_wait` and
`group_interval`.[]{#ch19.xhtml#idm45207089036576 primary="group_wait"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089035968
primary="group_interval" data-type="indexterm"}

If you have a group with no alerts and then a new set of alerts starts
firing, it is likely that all these new alerts will not all start firing
at exactly the same time. For example, as scrapes are spread across the
scrape interval, if a rack of machines fails, you will usually spot some
machines as down one interval before the others. It'd be good if you
could delay the initial notification for the group a little to see if
more alerts are going to come in. This is exactly what `group_wait`
does. By default, the Alertmanager will wait 30 seconds before sending
the first notification. You may worry this will delay response to
incidents, but keep in mind that if 30 seconds matter, you should be
aiming for an automated rather than a human response.

Now that the first notification has been sent for the group, some
additional alerts might start firing for your group.
[]{#ch19.xhtml#idm45207089034080 primary="repetition of notifications"
data-type="indexterm"}[]{#ch19.xhtml#idm45207089033472 primary="groups"
secondary="repeating notifications for" data-type="indexterm"}When
should the Alertmanager send you another notification for the group, now
including these new alerts? This is controlled by `group_interval`,
which defaults to 5 minutes. Every group interval after the first
notification, a new notification will be sent if there are new firing
alerts. If there are no new alerts for a group, you will not receive an
additional notification.

Once all alerts stop firing for your group and an interval has passed,
the state is reset and `group_wait` will apply once again. The
throttling for each group is independent, so if you were grouping by
`region`, then alerts firing for one region wouldn't make new alerts in
another region wait for a `group_interval`, just a `group_wait`.

Let's take an example, where there are four alerts firing at different
times:

``` {data-type="programlisting"}
t=  0  Alert firing {x="foo"}
t= 25  Alert firing {x="bar"}
t= 30  Notification for {x="foo"} and {x="bar"}
t=120  Alert firing {x="baz"}
t=330  Notification for {x="foo"}, {x="bar"} and {x="baz"}
t=400  Alert resolved {x="foo"}
t=700  Alert firing {x="quu"}
t=930  Notification for {x="bar"}, {x="baz"}, {x="quu"}
```

After the first alert the `group_wait` countdown starts, and a second
alert comes in while you are waiting. Both these `foo` and `bar` alerts
will be in a notification sent 30 seconds in. Now the `group_interval`
timer kicks in. In the first interval there is a new `baz` alert, so 300
seconds (one group interval) after the first notification there is a
second notification containing all three alerts that are currently
firing. At the next interval one alert has been resolved, but there are
no new alerts so there is no notification at `t=630`. A fourth alert for
`quu` fires, and at the next interval there is a third notification
containing all three alerts currently firing.

::: {.note data-type="note"}
###### Note

If an alert fires, resolves, and fires again within a group interval,
then it is treated in the same way as if the alert never stopped firing.
Similarly if an alert resolves, fires, and resolves again within a group
interval, it is the same as if the alert never fired in that interval.
This is not something to worry about in practice.
:::

Neither humans nor machines are fully reliable; even if a page got
through to the on call person and they acknowledged it, they might
forget about the alert if more pressing incidents occur. For ticketing
systems, you may have closed off an issue as resolved, but you will want
it reopened if the alert is still firing.

For this you can take advantage of the `repeat_interval`, which defaults
to 4 hours. []{#ch19.xhtml#idm45207089022368 primary="repeat_interval"
data-type="indexterm"}If it has been a repeat interval since a
notification was sent for a group with firing alerts, a new notification
will be sent. That is to say that a notification sent due to the group
interval will reset the timer for the repeat interval. A
`repeat_interval` shorter than the `group_interval` does not make
sense.[]{#ch19.xhtml#idm45207089020704 primary="group_interval"
secondary="tweaking" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

If you are getting notifications too often, you probably want to tweak
`group_interval` rather than `repeat_interval` because the issue is more
likely alerts flapping rather than hitting the (usually rather long)
repeat interval.
:::

The defaults for these settings are all generally sane, although you may
wish to tweak them a little. For example, even a complex outage tends to
be under control within 4 hours, so if an alert is still firing after
that long, it is a good bet that either the on call person forgot to put
in a silence or forgot about the issue and the repeated notification is
unlikely to be spammy. For a ticketing system, once a day is generally
frequent enough to create and poke tickets, so you could set
`group_interval` and `repeat_interval` to a day. The Alertmanager will
retry failed attempts at notification a few times so there's no need to
reduce `repeat_interval` for that reason alone. Depending on your setup
you might increase `group_wait` and `group_interval` to reduce the
number of pages you receive.

All these settings can be provided on a per-route basis, and are
inherited as defaults by child routes. An example configuration using
these might look like:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  group_by: [team]
  routes:
   # Frontend team.
   - matchers:
       - team = frontend
     group_by: [region, env]
     group_interval: 10m
     receiver: frontend-pager
     routes:
      - matchers:
          - severity = page
        receiver: frontend-pager
        group_wait: 1m
      - matchers:
          - severity = ticket
        receiver: frontend-ticket
        group_by: [region, env, alertname]
        group_interval: 1d
        repeat_interval: 1d
```
:::
:::
:::
:::

::: {.section pdf-bookmark="Receivers" data-type="sect2"}
::: {#ch19.xhtml#receivers_chap_nineteen .sect2}
## Receivers

Receivers take your grouped alerts and produce
notifications.[]{#ch19.xhtml#idm45207088948208 primary="Alertmanager"
secondary="configuration file" startref="ix_Alrtmgr2cfgrt"
tertiary="routing tree"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088862640
primary="routing tree" secondary="configuring for Alertmanager"
startref="ix_rttree" data-type="indexterm"}[]{#ch19.xhtml#ix_recv
primary="receivers" secondary="configuring"
data-type="indexterm"}[]{#ch19.xhtml#ix_Alrtmgr2cfgrec
primary="Alertmanager" secondary="configuration file"
tertiary="receivers" data-type="indexterm"}[]{#ch19.xhtml#ix_notify
primary="notifiers" data-type="indexterm"} A receiver contains
*notifiers*, which do the actual notifications. As of Alertmanager
0.24.0, the supported notifiers are email, PagerDuty, Pushover, Slack,
Opsgenie, VictorOps, WeChat, AWS SNS, Telegram, and the webhook. Just as
file SD is a generic mechanism for service discovery, the webhook is the
generic notifier that allows you to hook in systems that are not
supported out of the box.[]{#ch19.xhtml#idm45207088857264
primary="webhook notifier" data-type="indexterm"}

The layout of receivers is similar to service discovery within a scrape
config. All receivers must have a unique name, and then may contain any
number of notifiers. In the simplest cases you will have a single
notifier in a receiver:

``` {.pagebreak-before data-type="programlisting"}
receivers:
 - name: fallback-pager
   pagerduty_configs:
    - service_key: XXXXXXXX
```

PagerDuty is one of the simpler notifiers to get going with, since it
only requires a service key to work.[]{#ch19.xhtml#idm45207088854672
primary="PagerDuty notifier" data-type="indexterm"} All notifiers need
to be told where to send the notification, whether that's the name of a
chat channel, an email address, or whatever other identifiers a system
may use. Most notifiers are for commercial software as a service (SaaS)
offerings, and you will need to use their UI and documentation to obtain
the various keys, identifiers, URLs, and tokens that are specific to
you, and where exactly you want the notification sent to. We are not
going to attempt to give full instructions here, because the notifiers
and SaaS UIs are constantly changing.

You might also have one receiver going to multiple notifiers, such as
having the `frontend-pager` receiver []{#ch19.xhtml#idm45207088853040
primary="frontend-pager receiver" data-type="indexterm"}sending
notifications both to your[]{#ch19.xhtml#idm45207088852176
primary="Slack" secondary="PagerDuty integration with"
data-type="indexterm"} PagerDuty service and your Slack
channel:^[8](#ch19.xhtml#idm45207088851136){#ch19.xhtml#idm45207088851136-marker
data-type="noteref"}^

``` {code-language="yaml" data-type="programlisting"}
receivers:
 - name: frontend-pager
   pagerduty_configs:
    - service_key: XXXXXXXX
   slack_configs:
    - api_url: https://hooks.slack.com/services/XXXXXXXX
      channel: '#pages'
```

Some of the notifiers have settings that you will want to be the same
across all your uses of that notifier, such as the VictorOps API key.
You could specify that in each receiver, but the Alertmanager also has a
globals section for these so you only need to specify in the case of
VictorOps a routing key in the notifier itself:

``` {code-language="yaml" data-type="programlisting"}
global:
  victorops_api_key: XXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX

receivers:
 - name: backend-pager
   victorops_configs:
    - routing_key: a_route_name
```

Since each field like `victorops_configs` is a list, you can send
notifications to multiple different notifiers of one type at once, such
as sending to multiple Telegram
chats:^[9](#ch19.xhtml#idm45207088696208){#ch19.xhtml#idm45207088696208-marker
data-type="noteref"}^

``` {code-language="yaml" data-type="programlisting"}
receivers:
 - name: backend-pager
   opsgenie_configs:
    - teams: backendTeam   # This is a comma separated list.
   telegram_configs:
    - bot_token: XXX
      chat_id: YYY
    - bot_token: XXX
      chat_id: ZZZ
```

It is also possible for you to specify no receivers at all, which will
not result in any notifications:

``` {code-language="yaml" data-type="programlisting"}
receivers:
 - name: null
```

However, it'd be better where possible for you not to send alerts to the
Alertmanager in the first place, rather than spending Alertmanager
resources on processing alerts just to throw them away.

The webhook notifier is unique in that[]{#ch19.xhtml#idm45207088598400
primary="webhook notifier" data-type="indexterm"} it doesn't directly
notify an existing paging or messaging system that you might already
have in place. Instead, it sends all the information the Alertmanager
has about a group of alerts as a JSON HTTP message and allows you to do
what you like with it. You could use this to log your alerts, to perform
an automated action of some form, or to send a notification via some
system that the Alertmanager doesn't support directly. An HTTP endpoint
that accepts an HTTP `POST` from a webhook notification is known as a
*webhook receiver*.[]{#ch19.xhtml#idm45207088597008 primary="notifiers"
startref="ix_notify"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088596000
primary="webhook receiver" data-type="indexterm"}

::: {data-type="tip"}
###### Tip

While it's tempting to use webhooks liberally to execute code, it's wise
to keep your control loops as small as possible. For example, rather
than going from an exporter to Prometheus to the Alertmanager to a
webhook receiver to restart a stuck process, keeping it all on one
machine with a supervisor such as Supervisord or Monit is a better idea.
This will provide a faster response time, and generally be more robust
due to fewer moving parts.
:::

The webhook notifier is similar to the others; it takes a URL to which
notifications are sent. []{#ch19.xhtml#idm45207088593696
primary="continue setting (routing)" data-type="indexterm"}If you were
logging all alerts, you would use `continue` on the first route, which
would go to a webhook:

``` {code-language="yaml" data-type="programlisting"}
route:
  receiver: fallback-pager
  routes:
   - receiver: log-alerts
     continue: true
   # Rest of routing config goes here.

receivers:
 - name: log-alerts
   webhook_configs:
    - url: http://localhost:1234/log
```

You could use a Python 3 script such as in
[Example 19-1](#ch19.xhtml#webhook_receiver){data-type="xref"} to take
in these notifications and process the alerts
within.[]{#ch19.xhtml#idm45207088532880 primary="webhook receiver"
secondary="written in Python 3" data-type="indexterm"}

::: {#ch19.xhtml#webhook_receiver data-type="example"}
##### [Example 19-1. ]{.label}A simple webhook receiver written in Python 3

``` {code-language="python" data-type="programlisting"}
import json
from http.server import BaseHTTPRequestHandler
from http.server import HTTPServer

class LogHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        self.send_response(200)
        self.end_headers()
        length = int(self.headers['Content-Length'])
        data = json.loads(self.rfile.read(length).decode('utf-8'))
        for alert in data["alerts"]:
            print(alert)

if __name__ == '__main__':
   httpd = HTTPServer(('', 1234), LogHandler)
   httpd.serve_forever()
```
:::

All HTTP-based receivers []{#ch19.xhtml#idm45207088472832 primary="HTTP"
secondary="receivers based on"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088471984
primary="http_config field" data-type="indexterm"}have a field called
`http_config` which, similar to the settings in a scrape config as
discussed in ["How to
Scrape"](#ch08.xhtml#how_to_scrape){data-type="xref"}, allows setting a
`proxy_url`, HTTP Basic Authentication, TLS settings, and other
HTTP-related configuration.

::: {.section pdf-bookmark="Notification templates" data-type="sect3"}
::: {#ch19.xhtml#notification_templates .sect3}
### Notification templates

The layouts of messages from the various notifiers are fine to use when
starting out, but you will[]{#ch19.xhtml#ix_reccfgnottmpl
primary="receivers" secondary="configuring"
tertiary="notification templates"
data-type="indexterm"}[]{#ch19.xhtml#ix_tmplnoti primary="templating"
secondary="of notifications by Alertmanager"
secondary-sortas="notifications"
data-type="indexterm"}[]{#ch19.xhtml#ix_notitmpl primary="notifications"
secondary="templating" data-type="indexterm"} probably want to customize
them as your setup matures. All
notifiers[]{#ch19.xhtml#idm45207088344224 primary="webhook notifier"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088343616
primary="notifiers" data-type="indexterm"} except the
webhook^[10](#ch19.xhtml#idm45207088342880){#ch19.xhtml#idm45207088342880-marker
data-type="noteref"}^ permit templating using the same [Go templating
system](https://oreil.ly/pY91X) as you used for alerting rules in
["Annotations and
Templates"](#ch18.xhtml#alert_templates){data-type="xref"}. However, the
data and functions you have access to are slightly different, as you are
dealing with a group of alerts rather than a single
alert.[]{#ch19.xhtml#idm45207088340864 primary="Slack"
secondary="slack_configs, region and env labels in"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088339952
primary="region labels" secondary="in Slack" secondary-sortas="Slack"
data-type="indexterm"}[]{#ch19.xhtml#idm45207088338736
primary="env labels" secondary="in Slack" secondary-sortas="Slack"
data-type="indexterm"}

As an example, you might always want the `region` and `env` labels in
your Slack [notification]{.keep-together}:

``` {code-language="yaml" data-type="programlisting"}
receivers:
 - name: frontend-pager
   slack_configs:
    - api_url: https://hooks.slack.com/services/XXXXXXXX
      channel: '#pages'
      title: 'Alerts in {{ .GroupLabels.region }} {{ .GroupLabels.env }}!'
```

This will produce a notification[]{#ch19.xhtml#idm45207088312480
primary="Slack" secondary="message with region and environment"
data-type="indexterm"} like the one you see in
[Figure 19-1](#ch19.xhtml#slack_simple){data-type="xref"}.

<figure>
<div id="ch19.xhtml#slack_simple" class="figure">
<img src="assets/pur2_1901.png" width="385" height="71"
alt="A message in Slack" />
<h6><span class="label">Figure 19-1. </span>A message in Slack with the
region and environment</h6>
</div>
</figure>

`GroupLabels` is one of the top-level fields you can access in
templating, but there are several others:

`GroupLabels`

:   `GroupLabels` contains the group labels of the notification, so will
    be all the labels listed in the `group_by` for the route that this
    group came from.

`CommonLabels`

:   `CommonLabels` is all the labels that are common across all the
    alerts in your notification. This will always include all the labels
    in `GroupLabels`, and also any other labels that happen to be
    common. This is useful for opportunistically listing similarities in
    alerts. For example, if you were grouping by region and a rack of
    machines failed, the alerts for all the down instances might all
    have a common `rack` label that you could access in `CommonLabels`.
    However, if a single other machine in another rack failed, the
    `rack` label would no longer be in your `CommonLabels`.

`CommonAnnotations`

:   `CommonAnnotations` is like `CommonLabels`, but for annotations.
    This is of very limited use. As your annotations tend to be
    templated, it is unlikely that there will be any common values.
    However, if you had a simple string as an annotation, it might show
    up here.

`ExternalURL`

:   `ExternalURL` will contain the *external URL* of this Alertmanager,
    which can make it easier to get to the Alertmanager to create a
    silence. You can also use it to figure out which of your
    Alertmanagers sent a notification in a clustered setup. There is
    more discussion of external URLs in ["Networks and
    Authentication"](#ch21.xhtml#prometheus_network){data-type="xref"}.

`Status`

:   `Status` will be `firing` if at least one alert in the notification
    is firing; if all alerts are resolved, it will be resolved. Resolved
    notifications are covered in ["Resolved
    notifications"](#ch19.xhtml#resolved_notifications){data-type="xref"}.

`Receiver`

:   The name of the receiver, which is `frontend-pager` in the preceding
    example.

`GroupKey`

:   An opaque string with a unique identifier for the group. This is of
    no use to humans, but it helps ticketing and paging systems tie
    notifications from a group to previous notifications. This could be
    useful to prevent opening a new ticket in your ticketing system if
    there was already one open from the same group.

`Alerts`

:   `Alerts` is the actual meat of the notification, a list of all the
    alerts in your notification.[]{#ch19.xhtml#idm45207088234112
    primary="Alerts list" data-type="indexterm"}

Within each alert in the `Alerts` list there are also several fields:

`Labels`

:   As you would expect, this contains the labels of your alert.

`Annotations`

:   No prizes for guessing that this contains the annotations of your
    alert.

`Status`

:   `firing` if the alert is firing; otherwise, it'll be `resolved`.

`StartsAt`

:   This is the time the alert started firing as a Go `time.Time`
    object. Due to how Prometheus and the alerting protocol work, this
    is not necessarily when the alert condition was first satisfied.
    This is of little use in practice.

`EndsAt`

:   This is when the alert will stop or has stopped firing. This is of
    no use for firing alerts, but will tell you when a resolved alert
    resolved.

`GeneratorURL`

:   For alerts from
    Prometheus,^[11](#ch19.xhtml#idm45207088221824){#ch19.xhtml#idm45207088221824-marker
    data-type="noteref"}^ this is a link to the alerting rule on the
    Prometheus web interface, which can be handy for debugging. To us,
    the real reason this field exists is for a future Alertmanager
    feature that will allow you to drop alerts coming from a particular
    source, such as if there's a broken Prometheus that you can't shut
    down sending bad alerts to the Alertmanager.

You can use these fields as you see fit in your templates. For example,
you may wish to include all the labels, a link to your wiki, and a link
to a dashboard in all of your notifications:

``` {code-language="yaml" data-type="programlisting"}
receivers:
 - name: frontend-pager
   slack_configs:
    - api_url: https://hooks.slack.com/services/XXXXXXXX
      channel: '#pages'
      title: 'Alerts in {{ .GroupLabels.region }} {{ .GroupLabels.env }}!'
      text: >
        {{ .Alerts | len }} alerts:
        {{ range .Alerts }}
        {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
        {{ if eq .Annotations.wiki "" -}}
        Wiki: http://wiki.mycompany/{{ .Labels.alertname }}
        {{- else -}}
        Wiki: http://wiki.mycompany/{{ .Annotations.wiki }}
        {{- end }}
        {{ if ne .Annotations.dashboard "" -}}
        Dashboard: {{ .Annotations.dashboard }}&region={{ .Labels.region }}
        {{- end }}

        {{ end }}
```

Let's break this down:

``` {data-type="programlisting"}
{{ .Alerts | len }} alerts:
```

`.Alerts` is a list, and the built-in `len` function of Go templates
counts how many alerts you have in the list. This is about the most math
you can do in Go templates as there are no math operators, so you should
use alerting templates in Prometheus, as discussed in ["Annotations and
Templates"](#ch18.xhtml#alert_templates){data-type="xref"}, to calculate
any numbers and render them nicely:

``` {data-type="programlisting"}
{{ range .Alerts }}
{{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }}
```

This iterates over the alerts and then the sorted labels of each alert.

`range` in Go templates reuses `.` as the iterator, so the original `.`
is shadowed or hidden while you are inside the
iteration.^[12](#ch19.xhtml#idm45207088136752){#ch19.xhtml#idm45207088136752-marker
data-type="noteref"}^ While you could iterate over the label key value
pairs in the usual Go fashion, they will not be in a consistent order.
The `SortedPairs` method of the various label and annotation fields
sorts the label names and provides a list that you can iterate over:

``` {data-type="programlisting"}
{{ if eq .Annotations.wiki "" -}}
Wiki: http://wiki.mycompany/{{ .Labels.alertname }}
{{- else -}}
Wiki: http://wiki.mycompany/{{ .Annotations.wiki }}
{{- end }}
```

Empty labels are the same as no labels, so this checks if the `wiki`
annotation exists. If it does, it is used as the name of the wiki page
to link; otherwise, the name of the alert is used. In this way you can
have a sensible default that avoids you having to add a `wiki`
annotation to every single alerting rule, while still allowing
customization if you want to override it for one or two alerts. The
`{{-` and `-}}` tell Go templates to ignore whitespace before or after
the curly braces, allowing you to spread templates across multiple lines
for readability without introducing extraneous whitespace in the
[output]{.keep-together}:

``` {data-type="programlisting"}
{{ if ne .Annotations.dashboard "" -}}
Dashboard: {{ .Annotations.dashboard }}&region={{ .Labels.region }}
{{- end }}
```

If a `dashboard` annotation is present, it will be added to your
notification, and in addition, the region will be added as a URL
parameter.[]{#ch19.xhtml#idm45207088129168
primary="dashboard annotation" data-type="indexterm"} If you have a
Grafana template variable with this name, you will have it set to point
to the right value. As discussed in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}, alerting rules
do not have access to the external labels that usually contain things
such as `region`, so this is how you can add architectural details to
your notifications without your alerting rules having to be aware of how
your applications are deployed.

The end result of this is a notification like the one shown in
[Figure 19-2](#ch19.xhtml#slack_complex){data-type="xref"}. When using
chat-like notifiers and paging systems, it is wise for you to keep
notifications brief. This reduces the chances of your computer or mobile
phone screen being overcome with lengthy alert details, making it hard
to get a basic idea of what is going on. Notifications such as these
should get you going on debugging by pointing to a potentially useful
dashboard and playbook that have further information, not try to info
dump everything that might be useful in the notification
itself.[]{#ch19.xhtml#idm45207088075440 primary="Slack"
secondary="customized message" data-type="indexterm"}

<figure>
<div id="ch19.xhtml#slack_complex" class="figure">
<img src="assets/pur2_1902.png" width="600" height="288"
alt="A message in Slack" />
<h6><span class="label">Figure 19-2. </span>A customized Slack
message</h6>
</div>
</figure>

In addition to templating text fields, the destination of notifications
can also be templated. Usually each of your teams has their own part of
the routing tree and associated receivers. If another team wanted to
send your team alerts, they would set labels accordingly to use your
team's routing tree. For cases where you are offering a service,
particularly to external customers, having to define a receiver for
every potential destination could be a little
tedious.^[13](#ch19.xhtml#idm45207088072464){#ch19.xhtml#idm45207088072464-marker
data-type="noteref"}^

Combining the power of PromQL, labels, and notification templating for
alert destinations, you can go so far as to define a per-customer
threshold and notification destination[]{#ch19.xhtml#idm45207088071552
primary="destination (notifications)" data-type="indexterm"} in a metric
and have the Alertmanager deliver to that destination. The first step is
to have alerts that include their destination as a label:

``` {data-type="programlisting"}
groups:
 - name: example
   rules:
    - record: latency_too_high_threshold
      expr: 0.5
      labels:
        email_to: foo@example.com
        owner: foo
    - record: latency_too_high_threshold
      expr: 0.7
      labels:
        email_to: bar@example.com
        owner: bar
    - alert: LatencyTooHigh
      expr: |
        # Alert based on per-owner thresholds.
          owner:latency:mean5m
        > on (owner) group_left(email_to)
          latency_too_high_threshold
```

Here the different owners have different thresholds coming from a
metric, which also provides an `email_to`
label.[]{#ch19.xhtml#idm45207088068896 primary="email_to labels"
data-type="indexterm"} This is fine for internal customers who can add
their own `latency_too_high_threshold` to your rule file; for external
customers you may have an exporter exposing these thresholds and
destinations from a database.[]{#ch19.xhtml#idm45207088067648
primary="latency_too_high_threshold" data-type="indexterm"}

Then in the Alertmanager[]{#ch19.xhtml#idm45207088066528
primary="destination (notifications)"
secondary="setting based on email_to label" data-type="indexterm"} you
can set the destination of the notifications based on this `email_to`
label:

``` {code-language="yaml" data-type="programlisting"}
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'youraddress@example.org'

route:
  group_by: [email_to, alertname] 
  receiver: customer_email

receivers:
- name: customer_email
  email_configs:
   - to: '{{ .GroupLabels.email_to }}'
     headers:
      subject: 'Alert: {{ .GroupLabels.alertname }}'
```

[![1](assets/1.png){height="12" width="12"}](#ch19.xhtml#co_alertmanager_CO2-1){#ch19.xhtml#callout_alertmanager_CO2-1 .co}

:   The `group_by` must include the `email_to` label that
    []{#ch19.xhtml#idm45207087990272 primary="group_by"
    data-type="indexterm"}you are using to specify the destination,
    because each destination needs its own alert group. The same
    approach can be used with other notifiers. Note that anyone with
    access to Prometheus or the Alertmanager will be able to see the
    destinations since labels are visible to everyone. This may be a
    concern if some destination fields are potentially
    sensitive.[]{#ch19.xhtml#idm45207087989408 primary="notifications"
    secondary="templating" startref="ix_notitmpl"
    data-type="indexterm"}[]{#ch19.xhtml#idm45207087988192
    primary="templating" secondary="of notifications by Alertmanager"
    secondary-sortas="notifications" startref="ix_tmplnoti"
    data-type="indexterm"}[]{#ch19.xhtml#idm45207087986736
    primary="receivers" secondary="configuring"
    startref="ix_reccfgnottmpl" tertiary="notification templates"
    data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Resolved notifications" data-type="sect3"}
::: {#ch19.xhtml#resolved_notifications .sect3}
### Resolved notifications

All notifiers have a `send_resolved` field, with varying
defaults.[]{#ch19.xhtml#idm45207087939776
primary="resolved notifications"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087939136
primary="send_resolved field"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087938464
primary="receivers" secondary="configuring"
tertiary="resolved notifications"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087937248
primary="notifications" secondary="resolved" data-type="indexterm"} If
it is set to `true` then in addition to receiving notifications about
when alerts fire, your notifications will also include alerts that are
no longer firing and are now resolved. The practical effect of this is
that when Prometheus informs the Alertmanager that an alert is now
resolved,^[14](#ch19.xhtml#idm45207087935760){#ch19.xhtml#idm45207087935760-marker
data-type="noteref"}^ a notifier with `send_resolved` enabled will
include this alert in the next notification, and will even send a
notification with only resolved alerts if no other alerts are firing.

While it may seem handy to know that an alert is now resolved, we advise
quite a bit of caution with this feature as an alert no longer firing
does not mean that the original issue is handled. In ["What Are Good
Alerts?"](#ch18.xhtml#good_alerts){data-type="xref"} we mentioned that
responding to alerts with "it went away" was a sign that the alert
should probably not have fired in the first place. Getting a resolved
notification may be an indication that a situation is improving, but you
as the on call still need to dig into the issue and verify that it is
fixed and not likely to come back. Halting your handling of an incident
because the alert stopped firing is essentially the same as saying "it
went away." Because the Alertmanager works with alerts rather than
incidents, it is inappropriate to consider an incident resolved just
because the alerts stopped firing.

For example, machine down alerts being resolved might only mean that the
machine running Prometheus has now also gone down. So while your outage
is getting worse, you are no longer getting alerts about
it.^[15](#ch19.xhtml#idm45207087932880){#ch19.xhtml#idm45207087932880-marker
data-type="noteref"}^

Another issue with resolved notifications is that they can be a bit
spammy. If they were enabled for a notifier such as email or Slack, you
could be looking at doubling the message volume, thus halving your
signal-to-noise ratio. As discussed in ["Alerts Need
Owners"](#ch18.xhtml#alerts_need_owners){data-type="xref"}, using email
for notifications is often problematic, and more noise will not help
with that.

If you have a notifier with `send_resolved` enabled, then in
notification templating, `.Alerts` can contain a mix of firing and
resolved alerts. While you could filter the alerts yourself using the
`Status` field of an alert, `.Alert.Firing` will give you a list of just
the firing alerts, and `.Alert.Resolved` the resolved
alerts.[]{#ch19.xhtml#idm45207087927360 primary="Alertmanager"
secondary="configuration file" startref="ix_Alrtmgr2cfgrec"
tertiary="receivers"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087925808
primary="receivers" secondary="configuring" startref="ix_recv"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Inhibitions" data-type="sect2"}
::: {#ch19.xhtml#idm45207088864496 .sect2}
## Inhibitions

Inhibitions are a feature that allows you to treat some alerts as not
firing if other alerts are firing. []{#ch19.xhtml#idm45207087923504
primary="inhibitions"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087922800
primary="Alertmanager" secondary="configuration file"
tertiary="inhibitions" data-type="indexterm"}For example, if an entire
datacenter was having issues but user traffic had been diverted
elsewhere, there's not much point in sending alerts for that datacenter.

Inhibitions
currently^[16](#ch19.xhtml#idm45207087921072){#ch19.xhtml#idm45207087921072-marker
data-type="noteref"}^ live at the top level of *alertmanager.yml*. You
must specify what alerts to look for, what alerts they will suppress,
and which labels must match between the two:

``` {.pagebreak-before data-type="programlisting"}
inhibit_rules:
 - source_matchers:
     - severity = page-regionfail
   target_matchers:
     - severity = page
   equal: ['region']
```

Here, if an alert []{#ch19.xhtml#idm45207087918096
primary="severity labels" data-type="indexterm"}with a `severity` label
of `page-regionfail` is firing, it will suppress all your alerts with
the same `region` label that have a `severity` label of
`page`.^[17](#ch19.xhtml#idm45207087915056){#ch19.xhtml#idm45207087915056-marker
data-type="noteref"}^

Overlap between the `source_match` and `target_match` should be avoided
since it can be tricky to understand and maintain
otherwise.[]{#ch19.xhtml#idm45207087910464 primary="source_match"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087909760
primary="target_match" data-type="indexterm"} Having different
`severity` labels is one way to avoid an overlap. If there is overlap,
any alerts matching the `source_match` will not be suppressed.

We recommend using this feature
sparingly.[]{#ch19.xhtml#idm45207087907808
primary="symptoms, alerting on" data-type="indexterm"} With
symptom-based alerting (as discussed in ["What Are Good
Alerts?"](#ch18.xhtml#good_alerts){data-type="xref"}) there should be
little need for dependency chains between your alerts. Reserve
inhibition rules for large-scale issues such as datacenter
outages.[]{#ch19.xhtml#idm45207087905920 primary="Alertmanager"
secondary="configuration file" startref="ix_Alrtmgr2cfg"
data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Alertmanager Web Interface" data-type="sect1"}
::: {#ch19.xhtml#idm45207087904576 .sect1}
# Alertmanager Web Interface

As you saw in
["Alerting"](#ch02.xhtml#intro_alerting){data-type="xref"}, the
Alertmanager[]{#ch19.xhtml#ix_Alrtmgr2web primary="Alertmanager"
secondary="web interface"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087900864
primary="web interface (Alertmanager)" data-type="indexterm"} allows you
to view what alerts are currently firing and to group and filter them.
[Figure 19-3](#ch19.xhtml#am_status){data-type="xref"} shows several
alerts in an Alertmanager grouped by `alertname`; you can also see all
of the alerts' other labels.

<figure>
<div id="ch19.xhtml#am_status" class="figure">
<img src="assets/pur2_1903.png" width="600" height="501"
alt="Alertmanager status page" />
<h6><span class="label">Figure 19-3. </span>Several alerts showing on
the Alertmanager status page</h6>
</div>
</figure>

From the status page you can click New Silence to create a silence from
scratch, or click the Silence link to prepopulate the silence form with
the labels of that alert. From there you can tweak the labels you want
your silence to have. When working with an existing alert you will
usually want to remove some labels to cover more than just that one
alert. To help track silences you must also enter your name and a
comment for the silence. Finally, you should preview the silence to
ensure it is not too broad, as you can see in
[Figure 19-4](#ch19.xhtml#am_silence){data-type="xref"}, before creating
the silence.

<figure>
<div id="ch19.xhtml#am_silence" class="figure">
<img src="assets/pur2_1904.png" width="600" height="692"
alt="Alertmanager New Silence page" />
<h6><span class="label">Figure 19-4. </span>Previewing a silence before
creating it</h6>
</div>
</figure>

If you visit the Silences page, you can see all silences that are
currently active, the ones that have yet to apply, and the silences that
have expired and no longer apply (as shown in
[Figure 19-5](#ch19.xhtml#am_silence_list){data-type="xref"}). From here
you can also expire silences that no longer apply and re-create silences
that have expired.

<figure>
<div id="ch19.xhtml#am_silence_list" class="figure">
<img src="assets/pur2_1905.png" width="600" height="246"
alt="Alertmanager Silences page" />
<h6><span class="label">Figure 19-5. </span>The Alertmanager Silences
page showing the active silences</h6>
</div>
</figure>

Silences stop alerts with the given labels from being considered as
alerting for the purposes of notification. Silences can be created in
advance, if you know that maintenance is going to happen and don't want
to pointlessly page the on call person, for example. As the on call, you
will also use silences to suppress alerts that you've already known
about for a while, so you are not disturbed while investigating. You can
think of a silence like the snooze button on an alarm clock. When
creating a silence, you have to enter a comment, which you can use to
state the reason of the silence, so it is not forgotten or
misunderstood.

If you want to stop alerts at a set time every day, you should not do so
with silences, rather add a condition to your alerts that the `hour`
function returns the desired value, as shown in ["Alerting
Rules"](#ch18.xhtml#alerting_rules){data-type="xref"}.

Now that you have seen all the key components of Prometheus, it is time
to consider how they all fit together at a higher level. In the next
chapter you will learn how to plan a deployment of
Prometheus.[]{#ch19.xhtml#idm45207087887456 primary="Alertmanager"
secondary="web interface" startref="ix_Alrtmgr2web"
data-type="indexterm"}[]{#ch19.xhtml#idm45207087886208
primary="Alertmanager" startref="ix_Alrtmgr2" data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch19.xhtml#idm45207089807472-marker)^ In datacenters, machines are
typically organized in vertical racks, with each rack usually having its
own power setup and a network switch. It is thus not uncommon for an
entire rack to disappear at once due to a power or switch issue.

^[2](#ch19.xhtml#idm45207089792048-marker)^ `amtool` can also be used to
query alerts and work with silences.

^[3](#ch19.xhtml#idm45207089646048-marker)^ The `reReplaceAll` function
in [alert and notification templates](https://oreil.ly/heUJc) is not
anchored, as that would defeat its [purpose]{.keep-together}.

^[4](#ch19.xhtml#idm45207089326528-marker)^ Receiver naming is just a
convention, but if your configuration does not result in the
`backend-ticket` receiver creating a ticket, it would be quite
misleading.

^[5](#ch19.xhtml#idm45207089236512-marker)^ On the other hand, if you
are following the RED method, a high failure ratio and high latency can
occur together. In practice, one usually happens a good bit before the
other, leaving you plenty of time to mitigate the issue or put in a
silence.

^[6](#ch19.xhtml#idm45207089070352-marker)^ This is a YAML list composed
of a string made of three dots.

^[7](#ch19.xhtml#idm45207089067136-marker)^ A hundred pages would be a
good-sized pager storm.

^[8](#ch19.xhtml#idm45207088851136-marker)^ PagerDuty also has a Slack
integration, which permits acknowledging alerts directly from Slack.
This sort of integration is quite handy, and can also cover pages coming
from sources other than the Alertmanager that are going to PagerDuty.

^[9](#ch19.xhtml#idm45207088696208-marker)^ This is preferable to using
`continue` as it is less fragile, and you don't have to keep multiple
routes in sync.

^[10](#ch19.xhtml#idm45207088342880-marker)^ For the webhook it is
expected that the webhook receiver was specifically designed to work
with the JSON message that is sent, so no templating of the webhook
message sent is required. In fact, the JSON message is the exact same
data structure that notification templates use under the covers.

^[11](#ch19.xhtml#idm45207088221824-marker)^ For other systems it should
be a link to whatever is generating the alert.

^[12](#ch19.xhtml#idm45207088136752-marker)^ To work around this, you
can set a variable such as `{{ $dot := . }}` and then access `$dot`.

^[13](#ch19.xhtml#idm45207088072464-marker)^ Alertmanager configuration
is expected to change relatively rarely, as your label structure
shouldn't change that often. Alerting rules, on the other hand, tend to
have ongoing churn and tweaks.

^[14](#ch19.xhtml#idm45207087935760-marker)^ Resolved alerts will have
the annotations from the last firing evaluation of that alert.

^[15](#ch19.xhtml#idm45207087932880-marker)^ Alerting approaches to
detect this are covered in ["Meta- and
Cross-Monitoring"](#ch21.xhtml#metamonitoring){data-type="xref"}, but
the salient point here is that you should be in a place where once an
alert starts firing, it will get investigated.

^[16](#ch19.xhtml#idm45207087921072-marker)^ They may move to per-route
at some point (having them as a global setting increases the chances for
an inhibition to accidentally suppress more than was intended).

^[17](#ch19.xhtml#idm45207087915056-marker)^ Using `match_re` in your
routes []{#ch19.xhtml#idm45207087914016 primary="match_re"
data-type="indexterm"}makes it easier to have more specific `severity`
labels like these, while still handling all pages in one route. If the
source alerts are not meant to result in notifications, that would be a
good use of a null receiver, as shown in
["Receivers"](#ch19.xhtml#receivers_chap_nineteen){data-type="xref"}.
:::
:::
:::

[]{#part06.xhtml}

::: {#part06.xhtml#sbo-rt-content}
::: {#part06.xhtml#part6 .part pdf-bookmark="Part VI. Deployment" data-type="part"}
# [Part VI. ]{.label}Deployment

Playing around with Prometheus on your own machine is one thing,
deploying it on a real production system is a different kettle of fish.

[Chapter 20](#ch20.xhtml#security_chapter){data-type="xref"} covers the
built-in server-side security features available to secure your
Prometheus server.
[Chapter 21](#ch21.xhtml#operating_chapter){data-type="xref"} looks at
the practicalities of running Prometheus in production and how to
approach rolling it out.
:::
:::

[]{#ch20.xhtml}

::: {#ch20.xhtml#sbo-rt-content}
::: {#ch20.xhtml#security_chapter .chapter}
# [Chapter 20. ]{.label}Server-Side Security

In this chapter, you will learn about security features provided by
Prometheus, such as TLS and Basic
Authentication.[]{#ch20.xhtml#ix_sersec primary="server-side security"
data-type="indexterm"}

::: {.section pdf-bookmark="Security Features Provided by Prometheus" data-type="sect1"}
::: {#ch20.xhtml#idm45207087878224 .sect1}
# Security Features Provided by Prometheus

When operating Prometheus, many operators choose to use a reverse proxy
to secure its endpoints.[]{#ch20.xhtml#idm45207087876608
primary="server-side security"
secondary="security features from Prometheus" data-type="indexterm"}
Indeed, the Prometheus server APIs are exposed over HTTP, which makes
them easy to integrate into any HTTP-capable reverse proxy.

Prometheus itself supports server-side security, making it possible to
either directly expose a secured version of Prometheus to the users or
secure the traffic between Prometheus and these reverse proxies.

Server-side security as described in this chapter applies to the
Prometheus server and most of the official
exporters.[]{#ch20.xhtml#idm45207087874464 primary="exporters"
secondary="server-side security" data-type="indexterm"} The same
command-line flags and options can be shared between these, so what
follows applies to more than just Prometheus.

The options described in this chapter require a dedicated file, whose
path can be passed as `--web.config.file`. On each request, the file is
read, which means that it is not needed to reload Prometheus or the
exporter to apply changes.
:::
:::

::: {.section pdf-bookmark="Enabling TLS" data-type="sect1"}
::: {#ch20.xhtml#idm45207087872032 .sect1}
# Enabling TLS

TLS is widely used in the network area to
secure[]{#ch20.xhtml#ix_sersecenTLS primary="server-side security"
secondary="enabling TLS" data-type="indexterm"}[]{#ch20.xhtml#ix_TLSen
primary="TLS" secondary="enabling" data-type="indexterm"} communications
between clients and servers. Without going into too much detail, TLS
enables the client to validate that the server they connect to is
recognized by a known certificate authority (CA), and then encrypt the
subsequent traffic. It is also possible to use TLS to authenticate
clients by forcing them to also present a valid TLS certificate when
connecting to the server.

To enable TLS on a Prometheus instance, you need to start by getting
some certificates. []{#ch20.xhtml#idm45207087867216
primary="certificates, TLS" data-type="indexterm"}In this example, we
are using self-signed certificates. However, in real-world deployments,
you should use your company's internal CA or public CA like Let's
Encrypt, which will be directly recognized by your
users.[]{#ch20.xhtml#idm45207087866384 primary="certificate authority"
secondary="self-signed, creating using OpenSSL" data-type="indexterm"}

First, create a self-signed CA with OpenSSL:

    $ openssl req -new -newkey rsa:2048 -days 365 -nodes -x509 \
        -keyout prometheus.key -out prometheus.crt \
        -subj "/CN=localhost" -addext "subjectAltName = DNS:localhost"

This command created two files: *prometheus.key* and *prometheus.crt*.

To enable TLS with the certificate and private key you have just
generated, create a *web.yml* file with []{#ch20.xhtml#idm45207087862368
primary="web configuration files" data-type="indexterm"}the content
shown in [Example 20-1](#ch20.xhtml#tls_config){data-type="xref"}.

::: {#ch20.xhtml#tls_config data-type="example"}
##### [Example 20-1. ]{.label}*web.yml*

``` {code-language="yaml" data-type="programlisting"}
tls_server_config:
  cert_file: prometheus.crt
  key_file: prometheus.key
```
:::

::: {.note data-type="note"}
###### Note

You can check the validity of web configuration files with `promtool`:

    $ ./promtool check web-config web.yml
    web.yml SUCCESS
:::

Then, you can launch Prometheus with this file,
using[]{#ch20.xhtml#idm45207087855152 primary="web configuration files"
secondary="launching Prometheus with" data-type="indexterm"} the
following command:

    $ ./prometheus --web.config.file web.yml

You can now access Prometheus with TLS using the following command:

    $ curl --cacert prometheus.crt https://127.0.0.1:9090/metrics

As Prometheus usually scrapes itself, the scrape configuration will also
need to be adapted[]{#ch20.xhtml#idm45207087852416
primary="scrape_configs" secondary="adapting for TLS"
data-type="indexterm"} in the main *prometheus.yml*, as in
[Example 20-2](#ch20.xhtml#config_with_tls){data-type="xref"}.

::: {#ch20.xhtml#config_with_tls data-type="example"}
##### [Example 20-2. ]{.label}*prometheus.yml*

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
  - job_name: 'prometheus'
    scheme: https
    tls_config:
      ca_file: prometheus.crt
    static_configs:
    - targets: ['localhost:9090']
```
:::

Do not forget to reload the Prometheus configuration after adapting the
scrape configuration, if needed:

    $ killall -HUP prometheus
:::
:::

::: {.section pdf-bookmark="Advanced TLS Options" data-type="sect1"}
::: {#ch20.xhtml#idm45207087871440 .sect1}
# Advanced TLS Options

The TLS configuration of Prometheus offers other
settings.[]{#ch20.xhtml#idm45207087782976 primary="TLS"
secondary="enabling" startref="ix_TLSen"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087781728
primary="server-side security" secondary="enabling TLS"
startref="ix_sersecenTLS"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087780512 primary="TLS"
secondary="advanced options"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087779568
primary="server-side security" secondary="advanced TLS options"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087778624
primary="tls_server_config" data-type="indexterm"} In particular, you
set client authentication with the settings shown in
[Example 20-3](#ch20.xhtml#advanced_tls_settings){data-type="xref"}.

::: {#ch20.xhtml#advanced_tls_settings data-type="example"}
##### [Example 20-3. ]{.label}*web.yml*

``` {code-language="yaml" data-type="programlisting"}
tls_server_config:
  client_auth_type: RequireAndVerifyClientCert
  client_ca_file: client_ca.crt
```
:::

Other available settings include:

-   `min_version` and `max_version`, which describe the minimum and
    maximum TLS version negotiated by the server. The versions are named
    TLS10, TLS11, TLS12, and TLS13, respectively, for TLS 1.0, 1.1, 1.2,
    and 1.3.

-   `cipher_suites`, which describes the cipher suite used by the
    server. This option does not affect TLS 1.3.

-   `prefer_cipher_suites`, which controls whether the server selects
    the client's most preferred cipher suite, or the server's most
    preferred cipher suite.

-   `curve_preferences`, which lists the elliptic curves that will be
    used in an
    ECDHE^[1](#ch20.xhtml#idm45207087702304){#ch20.xhtml#idm45207087702304-marker
    data-type="noteref"}^ handshake, in preference order.

These configuration settings allow you to have complete control of the
underlying TLS library.

::: {.warning data-type="warning"}
###### Warning

Prometheus comes with secure default for those settings. It is unwise to
change them if you don't know what you are doing as you could
inadvertently compromise your security.
:::
:::
:::

::: {.section .less_space .pagebreak-before pdf-bookmark="Enabling Basic Authentication" data-type="sect1"}
::: {#ch20.xhtml#idm45207087699712 .sect1}
# Enabling Basic Authentication

Basic Authentication mandates that every request made to a Prometheus
server needs to be authenticated by a username and a
password.[]{#ch20.xhtml#ix_sersecAuth primary="server-side security"
secondary="enabling Basic Authentication"
data-type="indexterm"}[]{#ch20.xhtml#ix_authBasic
primary="authentication" secondary="enabling Basic Authentication"
data-type="indexterm"}[]{#ch20.xhtml#ix_BscAuth
primary="Basic Authentication, enabling" data-type="indexterm"} It works
by providing a list of users and hashed passwords to Prometheus, then
validates every incoming request against that list.

Advanced authorization mechanisms such as restricting which pages a user
can see or using other sources of users, such as OAuth or LDAP, are not
supported by Prometheus. []{#ch20.xhtml#idm45207087693952
primary="authorization"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087693248
primary="reverse proxy"
data-type="indexterm"}[]{#ch20.xhtml#idm45207087692576 primary="TLS"
data-type="indexterm"}If you need to use such fine-grained settings, you
have to put a reverse proxy in front of Prometheus. Thanks to TLS and
Basic Authentication, you could make the reverse proxy authenticate
itself on the backend, therefore still securing your Prometheus server
while doing appropriate user management on the proxy.

Passwords are not provided in the Prometheus configuration as clear
text.[]{#ch20.xhtml#idm45207087691392 primary="passwords"
data-type="indexterm"} They are hashed, which means that if someone gets
access to the configuration file, they will not be able to easily find
out the password. Bcrypt is the password hash mechanism used by
Prometheus.

::: {.warning data-type="warning"}
###### Warning

Basic Authentication sends the password in clear text in the HTTP
headers. To prevent password interception, we highly recommend you use
TLS to encrypt the traffic between the client and the server.
:::

To add a user to our Prometheus server and enable Basic Authentication,
the first step is to generate the hash for their password. Let's use
`htpasswd` for this, but other tools are available as well:

    $ htpasswd -nBC 10 "" | tr -d ':\n'
    New password:
    Re-type new password:
    $2y$10$LbwE6OVsPc4PqDFaYwvw/uOkMMficVQrQjtY5KT/BGnAKPa0vK45C

In this example, the password I have chosen is *demo*.

::: {.note data-type="note"}
###### Note

`10` is the bcrypt cost. []{#ch20.xhtml#idm45207087686048
primary="bcrypt cost" data-type="indexterm"}Usually, the cost used
should be between 10 and 12, with current computing power. Increasing
this number will likely increase the security of the password at the
expense of compute resources.
:::

You can now use this password to update your *web.yml* file, as shown in
[[Example 20-4](#ch20.xhtml#web_with_password){data-type="xref"}]{.keep-together}.

::: {#ch20.xhtml#web_with_password data-type="example"}
##### [Example 20-4. ]{.label}*web.yml*

``` {code-language="yaml" data-type="programlisting"}
tls_server_config:
  cert_file: prometheus.crt
  key_file: prometheus.key
basic_auth_users:
  julien: $2y$10$LbwE6OVsPc4PqDFaYwvw/uOkMMficVQrQjtY5KT/BGnAKPa0vK45C
```
:::

You have configured a user *julien* with a password *demo*, in a
Prometheus server protected with a TLS certificate. To use the
credentials, open
[*http://127.0.0.1:9090*](http://127.0.0.1:9090){.bare} in your web
browser. A prompt should ask for the username and password---enter
**`julien`** and **`demo`**. You should get access to the web interface
of your Prometheus server.

Prometheus []{#ch20.xhtml#idm45207087635584 primary="scraping"
secondary="configuring Prometheus to scrape itself using Basic Authentication"
data-type="indexterm"}itself will need to be configured to scrape itself
using Basic Authentication, as in
[Example 20-5](#ch20.xhtml#scrape_with_basic_auth){data-type="xref"}.

::: {#ch20.xhtml#scrape_with_basic_auth data-type="example"}
##### [Example 20-5. ]{.label}*prometheus.yml*

``` {code-language="yaml" data-type="programlisting"}
scrape_configs:
  - job_name: 'prometheus'
    scheme: https
    tls_config:
      ca_file: prometheus.crt
    basic_auth:
      username: julien
      password: demo
    - targets: ['localhost:9090']
```
:::

You can also pass a username using cURL:

    $ curl --cacert prometheus.crt -u julien:demo https://127.0.0.1:9090/metrics

Now that you've learned how to secure your Prometheus server, we'll
explore deploying it in a production environment in the upcoming
chapter.
:::
:::

::: {data-type="footnotes"}
^[1](#ch20.xhtml#idm45207087702304-marker)^ Elliptic Curve
Diffie-Hellman Ephemeral
:::
:::
:::

[]{#ch21.xhtml}

::: {#ch21.xhtml#sbo-rt-content}
::: {#ch21.xhtml#operating_chapter .chapter}
# [Chapter 21. ]{.label}Putting It All Together

In the preceding chapters you learned about all the components
[]{#ch21.xhtml#ix_depPrm primary="deploying Prometheus"
data-type="indexterm"}in a Prometheus setup: instrumentation,
dashboards, service discovery, exporters, PromQL, alerts, and the
Alertmanager. In this final chapter you will learn how to bring all of
these together and plan a Prometheus deployment and maintain it in the
future.

::: {.section pdf-bookmark="Planning a Rollout" data-type="sect1"}
::: {#ch21.xhtml#idm45207087564240 .sect1}
# Planning a Rollout

When you are considering a new technology, it's
best[]{#ch21.xhtml#ix_plan primary="planning a rollout"
data-type="indexterm"}[]{#ch21.xhtml#ix_depPrmpln
primary="deploying Prometheus" secondary="planning a rollout"
data-type="indexterm"} to start the
rollout^[1](#ch21.xhtml#idm45207087560160){#ch21.xhtml#idm45207087560160-marker
data-type="noteref"}^ with something small that doesn't take too much
effort, nor prematurely commit you to doing a complete
rollout.[]{#ch21.xhtml#idm45207087559232 primary="Node Exporter"
data-type="indexterm"} When starting with Prometheus in an existing
system, we recommend you start by running the Node
Exporter^[2](#ch21.xhtml#idm45207087558304){#ch21.xhtml#idm45207087558304-marker
data-type="noteref"}^ and Prometheus. You already ran both of these in
[Chapter 2](#ch02.xhtml#chapter_getting_started){data-type="xref"}.

The Node Exporter covers all the machine-level metrics that might be
used from other monitoring systems, and then quite a few more, as was
covered in
[Chapter 7](#ch07.xhtml#node_exporter_chapter){data-type="xref"}.
[]{#ch21.xhtml#idm45207087555600 primary="metrics"
secondary="machine-level from other monitoring systems"
data-type="indexterm"}At this stage you will have a wide variety of
metrics for little effort, and you should get comfortable with
Prometheus, set up some dashboards, and maybe even do some alerting.

Next, we'd suggest looking at what third-party systems you are using and
which exporters exist for them and start deploying
those.[]{#ch21.xhtml#idm45207087514016 primary="exporters"
secondary="considering in Prometheus rollout" data-type="indexterm"} For
example, if you have network devices, you can run the SNMP Exporter; if
you have JVM-based applications such as Kafka or Cassandra, you would
use the JMX Exporter; and if you want blackbox monitoring, you might use
the Blackbox Exporter, as covered in
[Chapter 10](#ch10.xhtml#common_exporters_chapter){data-type="xref"}.
The goal at this stage is to gain metrics about as many different parts
of your system as you can with as little effort as possible.

By now you will be comfortable with Prometheus, and will have figured
out your approach to aspects such as service discovery, as discussed in
[Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}.
You could have done all the previous steps of the rollout
alone.[]{#ch21.xhtml#idm45207087510800 primary="instrumentation"
data-type="indexterm"} The next step is to start instrumenting your
organization's own applications, as covered in
[Chapter 3](#ch03.xhtml#instrumentation_chapter){data-type="xref"},
which will likely involve asking other people to also get involved and
commit time to monitoring. Being able to demonstrate all of
[]{#ch21.xhtml#idm45207087509072 primary="dashboards"
data-type="indexterm"}the monitoring and
dashboards^[3](#ch21.xhtml#idm45207087508336){#ch21.xhtml#idm45207087508336-marker
data-type="noteref"}^ you have set up so far (which are backed by
exporters) will make it quite a bit easier to sell others on using
Prometheus; extensively instrumenting all your code as step one would be
unlikely to get buy-in.

As before, when adding instrumentation you want to start with metrics
that give you the biggest gains.[]{#ch21.xhtml#idm45207087507344
primary="chokepoints in your applications" data-type="indexterm"} Look
for chokepoints in your applications that significant proportions of
traffic go through. For example, if you have common HTTP libraries that
all of your applications use to communicate with each other and you
instrument them with the basic RED metrics, as covered in ["Service
instrumentation"](#ch03.xhtml#service_instrumentation){data-type="xref"},
you will get the key performance metrics for large swaths of your online
serving systems from just one instrumentation change.

If you have existing instrumentation from another
[]{#ch21.xhtml#idm45207087505312 primary="monitoring systems  (other)"
secondary="existing instrumentation from"
data-type="indexterm"}monitoring system, you can deploy integrations
such as the StatsD and Graphite Exporters, discussed in
[Chapter 11](#ch11.xhtml#other_monitoring_systems_chapter){data-type="xref"},
to take advantage of what you already have.
[]{#ch21.xhtml#idm45207087503280 primary="StatsD Exporter"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087502608
primary="Graphite Exporter" data-type="indexterm"}Over time you should
look to not only transition entirely to Prometheus instrumentation, as
covered in
[Chapter 3](#ch03.xhtml#instrumentation_chapter){data-type="xref"}, but
also to further instrument your applications.

As your usage of Prometheus grows to cover more and more of your
monitoring and metrics-monitoring needs, you should start turning down
other monitoring systems that are no longer needed. It's not unusual for
a company to end up with 10+ different monitoring systems over time, so
consolidating where practical is always [beneficial]{.keep-together}.

This plan is a general guideline, which you can and should adapt to your
circumstances. For example, if you are a developer you might jump
straight to instrumenting your applications. You might even add a client
library to your application with no instrumentation yet in order to take
advantage of the out-of-the-box metrics such as CPU usage and garbage
collection.[]{#ch21.xhtml#idm45207087499296 primary="planning a rollout"
startref="ix_plan"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087498320
primary="deploying Prometheus" secondary="planning a rollout"
startref="ix_depPrmpln" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Growing Prometheus" data-type="sect1"}
::: {#ch21.xhtml#growing_prometheus .sect1}
# Growing Prometheus

Usually you start out with one Prometheus server per
datacenter.[]{#ch21.xhtml#idm45207087495264
primary="deploying Prometheus" secondary="growing Prometheus"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087494288
primary="growing Prometheus" data-type="indexterm"} Prometheus is
intended to be run on the same network as what it is monitoring, because
this reduces the ways in which things can fail, aligns failure domains,
and provides low-latency, high-bandwidth network access to the targets
that Prometheus is
scraping.^[4](#ch21.xhtml#idm45207087493488){#ch21.xhtml#idm45207087493488-marker
data-type="noteref"}^

A single Prometheus is quite efficient, so you can likely get away with
one Prometheus for an entire datacenter's monitoring needs for longer
than you'd think.[]{#ch21.xhtml#idm45207087492576
primary="scaling Prometheus" data-type="indexterm"} At some point
though, operational overhead, performance, or just social considerations
will lead you to start splitting out parts of the Prometheus to separate
Prometheus servers. For example, it is common to have separate
Prometheus servers for network, infrastructure, and application
monitoring. []{#ch21.xhtml#idm45207087491744 primary="vertical sharding"
data-type="indexterm"}This is known as *vertical sharding* and it is the
best way to scale Prometheus.

Longer term, you may have every team run their own Prometheus servers,
empowering them to choose what target labels and scrape intervals make
sense for them (as discussed in
[Chapter 8](#ch08.xhtml#service_discovery_chapter){data-type="xref"}).
[]{#ch21.xhtml#idm45207087489408 primary="labels"
data-type="indexterm"}You could also run the servers for teams as a
shared service, but you must be prepared for teams getting
overenthusiastic with labels.

A pattern Brian has seen play out many times is that when starting out
it is a struggle to convince teams that they should instrument their own
code or deploy exporters. At some point though, it'll click, and they
will understand the power of labels. []{#ch21.xhtml#idm45207087488144
primary="cardinality" secondary="performance problems with"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087487296 primary="metrics"
secondary="problems from too much cardinality"
data-type="indexterm"}Within a short period of time you will likely find
that your Prometheus server has performance issues due to metrics with a
cardinality that is far beyond what it is reasonable to use in a
metrics-based monitoring system (as discussed in
["Cardinality"](#ch05.xhtml#cardinality_section){data-type="xref"}). If
you are running Prometheus as a shared service and the team consuming
these metrics is not the one getting paged, it can be difficult to
convince them that they need to cut back on cardinality. But if they run
their own Prometheus and are the ones getting woken up at 3 a.m., they
are likely going to be more realistic about what belongs in a
metrics-based monitoring system and what belongs in a logs-based system.

If your team has particularly large systems, they may end up with
multiple Prometheus servers per datacenter. An infrastructure team may
end up with one Prometheus for Node Exporters, one for reverse proxies,
and one for everything else. For ease of management, it is normal to run
Prometheus servers inside each of your Kubernetes clusters rather than
trying to monitor them from outside.

Where you start and end up on this spectrum of setups will depend on
your scale and the culture within your organization. It is our
experience that social
factors^[5](#ch21.xhtml#idm45207087484784){#ch21.xhtml#idm45207087484784-marker
data-type="noteref"}^ usually result in Prometheus servers being split
out before any performance concerns arise.
:::
:::

::: {.section pdf-bookmark="Going Global with Federation" data-type="sect1"}
::: {#ch21.xhtml#federation .sect1}
# Going Global with Federation

With a Prometheus per datacenter, how do you perform global
aggregations?[]{#ch21.xhtml#ix_depPrmfed primary="deploying Prometheus"
secondary="federation" data-type="indexterm"}[]{#ch21.xhtml#ix_fed
primary="federation" secondary="going global with"
data-type="indexterm"}

Reliability is a key property of a good monitoring system, and a core
value of Prometheus. When it comes to graphing and alerting, you want as
few moving parts as possible because a simple system is a reliable
system. When you want a graph of application latency in a datacenter,
you have Grafana talk to the Prometheus in that datacenter that is
scraping that application, and similarly for alerting on per-datacenter
application latency.

This doesn't quite work for global latency, since each of your
datacenter Prometheus servers only has a part of the data. This is where
*federation* comes in. Federation allows you to have a global Prometheus
that pulls aggregated metrics from your datacenter Prometheus servers,
as shown in
[Figure 21-1](#ch21.xhtml#federation_arch_diagram){data-type="xref"}.

<figure>
<div id="ch21.xhtml#federation_arch_diagram" class="figure">
<img src="assets/pur2_2101.png" width="600" height="367"
alt="Global federation architecture." />
<h6><span class="label">Figure 21-1. </span>Global federation
architecture</h6>
</div>
</figure>

For example, to pull in all metrics aggregated to the job level, you
could have a *prometheus.yml* like:

``` {.pagebreak-before code-language="yaml" data-type="programlisting"}
scrape_configs:
 - job_name: 'federate'
   honor_labels: true
   metrics_path: '/federate'
   params:
     'match[]':
       - '{__name__=~"job:.*"}'
   static_configs:
     - targets:
       - 'prometheus-dublin:9090'
       - 'prometheus-berlin:9090'
       - 'prometheus-new-york:9090'
```

The */federate* HTTP endpoint on Prometheus takes a list of selectors
(covered in ["Selectors"](#ch13.xhtml#selectors){data-type="xref"}) in
`match[]` URL parameters. It will return all matching time series
following instant vector selector semantics, including staleness, as
discussed in ["Instant
Vector"](#ch13.xhtml#instant_vector){data-type="xref"}. If you supply
multiple `match[]` parameters, a sample will be returned if it matches
any of them. To avoid the aggregated metrics having the instance label
of the Prometheus target, `honor_labels` (which was discussed in ["Label
Clashes and honor_labels"](#ch08.xhtml#honor_labels){data-type="xref"})
is used
here.^[6](#ch21.xhtml#idm45207087402816){#ch21.xhtml#idm45207087402816-marker
data-type="noteref"}^ The external labels of the Prometheus (as
discussed in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}) are also added
to the federated metrics, so you can tell where each time series came
from.

::: {.warning data-type="warning"}
###### Warning

Unfortunately, some users use federation for purposes other than pulling
in aggregated metrics. To avoid falling into this trap, you should
understand the following:

-   Federation is not for copying the content of entire Prometheus
    servers.

-   Federation is not a way to have one Prometheus proxy another
    Prometheus.

-   You should not use federation to pull metrics with an `instance`
    label.
:::

Let us explain why you should not use federation beyond its intended use
case. First, for reliability you want to have as few moving parts as is
practical. Pulling all your metrics over the internet to a global
Prometheus from where you can then graph and alert on them means that
internet connectivity to another datacenter is now a hard dependency on
your per-datacenter monitoring working. In general, you want to align
your failure domains, so that graphing and alerting for a datacenter do
not depend on another datacenter being operational. That is, as far as
is practical you want the Prometheus that is scraping a set of targets
to also be the one sending alerts for that target. This is particularly
important if there is a network outage or partition.

The second issue is scaling.[]{#ch21.xhtml#idm45207087394112
primary="scaling Prometheus" data-type="indexterm"} For reliability,
each Prometheus is standalone and running on one machine and thus
limited by machine size in terms of how much it can handle. Prometheus
is quite efficient, so even limited to a single machine, it is quite
plausible for you to have a single Prometheus server monitor an entire
datacenter. As you add datacenters you just need to turn up a Prometheus
in each of them. A global Prometheus pulling in only aggregated metrics
will have greatly reduced cardinality data to deal with compared with
the datacenter Prometheus
servers,^[7](#ch21.xhtml#idm45207087393280){#ch21.xhtml#idm45207087393280-marker
data-type="noteref"}^ and thus will prevent bottlenecks. Conversely, if
the global Prometheus was pulling in all metrics from each datacenter
Prometheus, the global Prometheus would become the bottleneck and
greatly limit your ability to scale. Put another way, for federation to
scale you need to use the same approach discussed in ["Reducing
Cardinality"](#ch17.xhtml#reducing_cardinality){data-type="xref"} for
dashboards.

Thirdly, Prometheus is[]{#ch21.xhtml#idm45207087361136
primary="scraping" data-type="indexterm"} designed to scrape many
thousands of small to medium size
targets.^[8](#ch21.xhtml#idm45207087360400){#ch21.xhtml#idm45207087360400-marker
data-type="noteref"}^ By spreading the scrapes over the scrape interval,
Prometheus can keep up with the data volumes with even load. If you
instead have it scrape a handful of targets with massive numbers of time
series, such as massive federation endpoints, this can cause load spikes
and it may not even be possible for Prometheus to complete processing of
one massive scrape worth of data in time to start the next scrape.

The fourth issue is semantics. By passing all the data through an extra
Prometheus, additional race conditions will be
introduced.[]{#ch21.xhtml#idm45207087359280 primary="race conditions"
data-type="indexterm"} You would see increased artifacts [in
your]{.keep-together} graphs, and you would not get the benefit of the
staleness handling the [semantics]{.keep-together}.

One objection to this architecture is if all your metrics don't end up
in one Prometheus, how will you know which Prometheus contains a given
metric? This turns out not to be an issue in practice. As your
Prometheus servers will tend to follow your general architecture, it is
usually quite obvious which Prometheus monitors which targets and thus
which has a specific metric. For example, Node Exporter metrics for
Dublin are going to be in the Dublin infrastructure Prometheus. Grafana
supports both data source templating and having graphs with metrics from
different data sources on them, so this is not an issue for dashboards
either.

Usually you will only have a two-level federation hierarchy with
datacenter Prometheus servers and globals. The global Prometheus will
perform calculations with PromQL that you cannot do in a lower-level
Prometheus, such as how much traffic you are receiving globally.

It is also possible that you will end up with an additional level. For
example, it's normal to run a Prometheus inside each Kubernetes cluster
you have. If you had multiple Kubernetes clusters in a datacenter, you
might federate their aggregated metrics to a per-datacenter Prometheus
before then federating them from there to your global Prometheus.

Another use for federation is to pull limited aggregated metrics from
another team's Prometheus. It is polite to ask first, and if this
becomes a common or more formal thing, the considerations in ["Rules for
APIs"](#ch17.xhtml#rules_for_apis){data-type="xref"} may apply. There is
no need to do this just for dashboards though, as Grafana supports using
multiple data sources in a dashboard and in a
panel.[]{#ch21.xhtml#idm45207087354416 primary="Grafana"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087353744
primary="federation" secondary="going global with" startref="ix_fed"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087352528
primary="deploying Prometheus" secondary="federation"
startref="ix_depPrmfed" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Long-Term Storage" data-type="sect1"}
::: {#ch21.xhtml#long_term_storage .sect1}
# Long-Term Storage

In ["What Is
Monitoring?"](#ch01.xhtml#what_is_monitoring){data-type="xref"} we
mentioned that monitoring was alerting, debugging, trending, and
plumbing.[]{#ch21.xhtml#ix_LTstore primary="long-term storage"
data-type="indexterm"}[]{#ch21.xhtml#ix_storeLT primary="storage"
secondary="long-term" data-type="indexterm"}[]{#ch21.xhtml#ix_depPrmstor
primary="deploying Prometheus" secondary="long-term storage"
data-type="indexterm"} For most alerting, debugging, and plumbing, days
to weeks of data is usually more than
enough.^[9](#ch21.xhtml#idm45207087344672){#ch21.xhtml#idm45207087344672-marker
data-type="noteref"}^ But when it comes to trending, such as capacity
planning, it's usual for you to want years of data.

One approach to long-term storage is to treat Prometheus like a
traditional database and take regular backups that you can restore from
in the event of failure. A Prometheus ingesting 10,000 samples per
second with a conservative 2 bytes per sample would use a bit under 600
GB of disk space per year, which would fit on a modern machine.

Backups can be taken by sending an HTTP `POST` to the
*/api/v1/admin/tsdb/snapshot* endpoint, which will return the name of
the snapshot created under Prometheus's storage directory. This uses
hard links, so it doesn't consume much additional disk space as the data
is stored only once between the snapshot and Prometheus's own database.
After you are done with a snapshot, it is best to delete it to avoid
using more disk space than is needed. To restore from a snapshot,
replace the Prometheus storage directory with the snapshot.

Only a tiny proportion of your metrics will be interesting to you for
long-term trending, usually the aggregated metrics. It's usually not
worth keeping everything forever, so you can save a lot of storage space
by only keeping metrics from a global [Prometheus]{.keep-together} long
term^[10](#ch21.xhtml#idm45207087340768){#ch21.xhtml#idm45207087340768-marker
data-type="noteref"}^ or deleting nonaggregated metrics before a certain
time. The */api/v1/admin/tsdb/delete* HTTP endpoint takes selectors in
its `match[]` URL
parameter^[11](#ch21.xhtml#idm45207087338320){#ch21.xhtml#idm45207087338320-marker
data-type="noteref"}^ and has `start` and `end` parameters to restrict
the time range. Data will be deleted from disk at the next compaction.
It would be reasonable to delete old data, say, once a month.

For security reasons, both the snapshot and delete APIs require the
`--web.enable-admin-api` flag to be passed to Prometheus for them to be
enabled.

Another approach is to send your samples from Prometheus to some form of
clustered storage system that can use []{#ch21.xhtml#idm45207087334576
primary="clustered storage system"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087333856
primary="remote write" data-type="indexterm"}the resources of many
machines. *Remote write* sends samples as they are ingested to another
system. *Remote read* allows PromQL to transparently use samples from
another system, as if they were stored locally within the
Prometheus.[]{#ch21.xhtml#idm45207087332128 primary="remote read"
data-type="indexterm"} These are both configured at the top level of
*prometheus.yml*:

``` {data-type="programlisting"}
remote_write:
   - url: http://localhost:1234/write
remote_read:
   - url: http://localhost:1234/read
```

Remote write supports[]{#ch21.xhtml#idm45207087329568
primary="relabeling" secondary="support by remote write"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087328592
primary="write_relabel_configs" data-type="indexterm"} relabeling
through `write_relabel_configs`, which works similarly to what you saw
in
["metric_relabel_configs"](#ch08.xhtml#metric_relabel_configs){data-type="xref"}.
Your main use of this would be to restrict what metrics are sent to the
*remote write endpoint*, as you may find yourself limited by
cost.[]{#ch21.xhtml#idm45207087326064 primary="remote write endpoint"
data-type="indexterm"} From a bandwidth and memory standpoint, you
should take care when pulling in large numbers of time series from long
time periods via remote read. When using remote write it is important
that each Prometheus has unique external labels so that metrics from
different Prometheus servers don't
clash.[]{#ch21.xhtml#idm45207087325232 primary="external labels"
data-type="indexterm"}

One way to use remote read and write would be to consider Prometheus as
a largely ephemeral cache, and the remote storage as the main
storage.^[12](#ch21.xhtml#idm45207087324144){#ch21.xhtml#idm45207087324144-marker
data-type="noteref"}^ If Prometheus is restarted with an empty data
store, you would rely on remote read for historical graphs. You would
also design your alerts to be resilient under such a restart, which is a
good idea in any case.

There are many projects that integrate with Prometheus Remote Write. On
the open source side, it is worth mentioning CNCF's Thanos, CNCF's
Cortex, and Grafana Mimir, which use part of the Prometheus code as a
library and use S3-compatible storage as a backend. Those systems are
distributed, multitenant, and popular in the Prometheus
community.[]{#ch21.xhtml#idm45207087323072
primary="Cloud Native Computing Foundation (CNCF)"
data-type="indexterm"}

::: {data-type="tip"}
###### Tip

When evaluating your options, keep in mind that a load that would be
considered light for a single Prometheus server may exceed what another
system running across many machines can handle. It is always wise to
load test systems based on your own use case rather than relying on
headline numbers, as different systems are designed with different data
models and access patterns in mind. Simpler solutions can turn out to be
both more efficient and easier to operate. Clustered does not
automatically mean better.
:::

You should expect clustered storage systems to cost at least five times
what the equivalent Prometheus would cost for the same load. This is
because most systems will replicate the data three times, plus have to
take it in and process all the data. Thus you should be judicious about
what metrics you keep only locally and which ones are sent to clustered
storage.[]{#ch21.xhtml#idm45207087320752 primary="long-term storage"
startref="ix_LTstore"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087319776 primary="storage"
secondary="long-term" startref="ix_storeLT"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087318560
primary="deploying Prometheus" secondary="long-term storage"
startref="ix_depPrmstor" data-type="indexterm"}

::: {.note data-type="note"}
###### Note

Prometheus features an experimental agent mode that can be enabled using
the `--enable-feature=agent` flag. []{#ch21.xhtml#idm45207087315856
primary="agent mode" data-type="indexterm"}When using this mode,
Prometheus is optimized for Remote Write scenarios, and data is only
stored locally until it is transmitted to the Remote Write. Please be
aware that while using agent mode, you will not be able to perform local
recording rules or querying the data in Prometheus. This agent mode uses
less resources compared to the server mode, making it a more lightweight
solution for certain use cases.
:::
:::
:::

::: {.section pdf-bookmark="Running Prometheus" data-type="sect1"}
::: {#ch21.xhtml#idm45207087350464 .sect1}
# Running Prometheus

When it comes to actually running the Prometheus server, you will have
to consider hardware, configuration management, and how your network is
set up.[]{#ch21.xhtml#ix_depPrmrun primary="deploying Prometheus"
secondary="running Prometheus"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087312048
primary="running Prometheus" seealso="deploying Prometheus"
data-type="indexterm"}

::: {.section pdf-bookmark="Hardware" data-type="sect2"}
::: {#ch21.xhtml#idm45207087310976 .sect2}
## Hardware

The first question you will probably ask when it comes to running
Prometheus is what hardware Prometheus needs.[]{#ch21.xhtml#ix_hardw
primary="hardware" data-type="indexterm"}[]{#ch21.xhtml#ix_depPrmrunhdw
primary="deploying Prometheus" secondary="running Prometheus"
tertiary="hardware" data-type="indexterm"} Prometheus is best run on
SSDs, though they are not strictly necessary on smaller setups. Storage
space is one of the main resources you need to care about. To estimate
how much you'll need, you have to know how much data you will be
ingesting. For an existing
Prometheus^[13](#ch21.xhtml#idm45207087306592){#ch21.xhtml#idm45207087306592-marker
data-type="noteref"}^ you can run a PromQL query to report the samples
ingested per second:

``` {data-type="programlisting"}
rate(prometheus_tsdb_head_samples_appended_total[5m])
```

While Prometheus can achieve compression of 1.3 bytes per sample in
production, when estimating we tend to use 2 bytes per sample to be
conservative. The default retention for Prometheus is 15 days, so
100,000 samples per second would be [around 240 GB]{.keep-together} over
15 days. You can increase the retention with the
[`--storage.tsdb.retention.time`]{.keep-together} flag, and control
where Prometheus stores data with the
[`--storage.tsdb.path`]{.keep-together} flag. You can also decide to
limit the size of the TSDB with
[`--storage.tsdb.retention.size`]{.keep-together}.^[14](#ch21.xhtml#idm45207087300800){#ch21.xhtml#idm45207087300800-marker
data-type="noteref"}^ There is no particular filesystem recommended or
required for Prometheus, and many users have had success using network
block devices such as Amazon EBS. []{#ch21.xhtml#idm45207087299680
primary="filesystems" data-type="indexterm"}However NFS, including
Amazon EFS, is explicitly not supported by Prometheus because Prometheus
expects a POSIX filesystem, and NFS implementations have never really
had a reputation for offering exact POSIX [semantics]{.keep-together}.
Each Prometheus needs its own storage directory; you cannot share one
storage directory across the network.

The next question is how much RAM you will
need.[]{#ch21.xhtml#idm45207087297648 primary="RAM"
data-type="indexterm"} The storage in Prometheus 2.x works in blocks
that are written out every two hours and subsequently compacted into
larger time ranges. The storage engine does no internal caching, rather
it uses your kernel's page cache. So you will need enough RAM to hold a
block, plus overheads, plus the RAM used during queries. A good starting
point is 12 hours worth of sample ingestion, so for 100,000 samples per
second that would be around [8 GB]{.keep-together}.

Prometheus is relatively light on CPU.[]{#ch21.xhtml#idm45207087295664
primary="CPUs" secondary="requirements for Prometheus"
data-type="indexterm"} A quick benchmark on our machine (which has an
i7-3770k CPU) shows only 0.25 CPUs being used to ingest 100,000 samples
per second. But that is just ingestion---you will want additional CPU
power to cover querying and recording rules. Due to CPU spikes from Go's
garbage collection, you should always have at least one core more than
you think you need.

Network bandwidth is another
consideration.[]{#ch21.xhtml#idm45207087294048
primary="network bandwidth" data-type="indexterm"} Prometheus 2.x can
handle ingesting millions of samples per second, which is similar to the
one-machine limit of many other similar systems. Prometheus usually uses
compression when scraping, so it uses somewhere around 20 bytes of
network traffic to transfer a sample. With a million samples per second,
that's 160 Mbps of network traffic. That is a good chunk of a gigabit
network card, which may be all you have for an entire rack of machines.

Another resource to keep in mind is file
descriptors.[]{#ch21.xhtml#idm45207087292768 primary="file descriptors"
data-type="indexterm"} We could give you the equation and factors, but
these days file descriptors are not a scarce resource, so we'd say set
your file ulimit to a million and not worry about it.

::: {data-type="tip"}
###### Tip

Ulimit changes for file descriptors have an annoying habit of not
applying, depending on how exactly you start a service. Prometheus logs
the file ulimit at startup, and you can also check the value of
`process_max_fds` on */metrics*.
:::

These numbers are just starting points. You should benchmark and verify
these against your setup. We would generally recommended leaving room
for your Prometheus to double in terms of resource usage to give you
time to get new hardware as you grow, and it also gives you a buffer to
deal with sudden cardinality increases.[]{#ch21.xhtml#idm45207087289520
primary="deploying Prometheus" secondary="running Prometheus"
startref="ix_depPrmrunhdw" tertiary="hardware"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087288000
primary="hardware" startref="ix_hardw" data-type="indexterm"}
:::
:::

::: {.section pdf-bookmark="Configuration Management" data-type="sect2"}
::: {#ch21.xhtml#idm45207087286800 .sect2}
## Configuration Management

Prometheus does one thing and does it well---that being metrics-based
monitoring. []{#ch21.xhtml#idm45207087285360
primary="configuration management"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087284592
primary="deploying Prometheus" secondary="running Prometheus"
tertiary="configuration management" data-type="indexterm"}Prometheus
does not try to fulfill the role of configuration management, secret
management, or service database. To that extent, Prometheus aims to get
out of your way and allow you to use standard configuration management
approaches, rather than forcing you to learn and work around some
Prometheus-specific configuration management contrivance.

If you do not yet have a configuration management tool, we would
recommend Ansible for more traditional
environments.[]{#ch21.xhtml#idm45207087282720 primary="Ansible"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087282016
primary="Kubernetes" secondary="configuration management"
data-type="indexterm"} For Kubernetes, [Pulumi](https://oreil.ly/TwdL-)
looks promising, but there are literally tens of tools in this space.

Just because Prometheus allows for standard approaches does not mean it
will automatically work perfectly in your environment. Being generic
means avoiding the temptation to cater to platform-specific nuances. It
means that if you have a mature setup, Prometheus should be quite easy
to deploy. You could view Prometheus as a maturity test for your
configuration management, because Prometheus is a standard Unix binary
that works in the ways you'd expect.[]{#ch21.xhtml#idm45207087279744
primary="SIGTERM signal"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087279040
primary="SIGHUP signal" data-type="indexterm"} It accepts `SIGTERM`,
`SIGHUP`, logs to standard error, and uses simple text files for
configuration.^[15](#ch21.xhtml#idm45207087277376){#ch21.xhtml#idm45207087277376-marker
data-type="noteref"}^

For example, Prometheus rule files (discussed in
[Chapter 17](#ch17.xhtml#promql_rules_chapter){data-type="xref"}) can
only come from files on disk.[]{#ch21.xhtml#idm45207087272848
primary="rule files" data-type="indexterm"} If you want to have an API
where you can submit rules, there is nothing stopping you from building
such a
system,^[16](#ch21.xhtml#idm45207087271920){#ch21.xhtml#idm45207087271920-marker
data-type="noteref"}^ and having it output the rule files in standard
YAML format. Prometheus does not offer such an API itself, as how, for
example, would you ensure Prometheus had rules immediately after a
reboot? By only offering files on disk, you will find debugging is
simpler since you know exactly what input Prometheus is working from.
Those with simpler setups don't have to worry about more intricate
configuration management concepts, and those who wish to do something
fancier have an interface that permits them to do whatever they like.
Put another way, the cost of more complex and nonstandard setups is
borne by those with such setups, not by everyone else.

In simpler setups you can get away with having a static
*prometheus.yml*. But as you expand you will need to template it using
your configuration management system, at a minimum to specify a
different `external_labels` per Prometheus, as Prometheus itself has no
templating abilities for configuration files. If you haven't fully
progressed to having a configuration management system
yet,^[17](#ch21.xhtml#idm45207087268816){#ch21.xhtml#idm45207087268816-marker
data-type="noteref"}^ some runtime environments can provide environment
variables to the applications running under them. You could use tools
like `sed` or
`envsubst`^[18](#ch21.xhtml#idm45207087267280){#ch21.xhtml#idm45207087267280-marker
data-type="noteref"}^ to do rudimentary templating. On the far end of
sophistication you have tools like the Prometheus Operator from
Prometheus-Community (briefly mentioned in
[Chapter 9](#ch09.xhtml#containers_k8_chapter){data-type="xref"}), which
will completely manage not only your configuration file but also your
Prometheus server running on Kubernetes.

In [Chapter 10](#ch10.xhtml#common_exporters_chapter){data-type="xref"}
we mentioned that exporters should live right beside the application
they are exporting metrics from.[]{#ch21.xhtml#idm45207087264240
primary="exporters" data-type="indexterm"} You should take the same
approach with any daemons that provide configuration data for
Prometheus, such as if you are using file service discovery (discussed
in ["File"](#ch08.xhtml#file_sd){data-type="xref"}). By having such
daemons run beside each Prometheus, you will only be affected by the
machine running Prometheus having issues, and not other machines that
you are relying on to provide key functionality.

If you want to test changes to your Prometheus configuration, you can
easily spin up a test Prometheus with the new configuration. Since
Prometheus is pull-based, your targets don't have to know or care about
what is monitoring them. When doing this it would be wise to remove any
Alertmanagers or remote write endpoints from the configuration file.
:::
:::

::: {.section pdf-bookmark="Networks and Authentication" data-type="sect2"}
::: {#ch21.xhtml#prometheus_network .sect2}
## Networks and Authentication

Prometheus is designed with the idea that it is on the same network as
the targets it is monitoring, and can contact them directly over HTTP
and request their metrics.[]{#ch21.xhtml#ix_depPrmrunntw
primary="deploying Prometheus" secondary="running Prometheus"
tertiary="networks and authentication"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087258512
primary="deploying Prometheus" secondary="running Prometheus"
tertiary="networks and authentication"
data-type="indexterm"}[]{#ch21.xhtml#ix_netw primary="networks"
data-type="indexterm"} This is known as pull-based monitoring, and comes
with advantages such as `up` indicating if a scrape worked, being able
to run a test Prometheus without having to configure all your targets to
push to it, and more tactical options for handling sudden load
increases, as covered in ["Managing
Performance"](#ch21.xhtml#managing_performance){data-type="xref"}.

If you have a network setup where there is NAT or a firewall in the way,
you should try to run a Prometheus server behind it so that it can
directly access the targets. There are also options like
[PushProx](https://oreil.ly/nq_fp), SSH tunnels, or having Prometheus
use a proxy via the `proxy_url` configuration field.

::: {.warning data-type="warning"}
###### Warning

Do not try to use the Pushgateway to work around network architecture,
or more generally to try to convert Prometheus to a push-based
system.[]{#ch21.xhtml#idm45207087252192 primary="push"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087251488
primary="Pushgateway" secondary="improper use of" data-type="indexterm"}
:::

As was already covered in
["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}, the
Pushgateway is for service-level batch jobs to push metrics to once just
before they exit. It is not designed for application instances to
regularly push metrics to, and you should never be pushing metrics that
end up with an `instance` label to the Pushgateway. Trying to use the
Pushgateway in this fashion will create a
bottleneck,^[19](#ch21.xhtml#idm45207087248704){#ch21.xhtml#idm45207087248704-marker
data-type="noteref"}^ the timestamps of the samples will not be correct
(which will lead to graph artifacts), and you lose the `up` metric so
it's harder to distinguish whether a process has died on purpose or due
to a failure. The Pushgateway also has no logic to expire old data,
because for service-level batch jobs for which the last run of a cronjob
was a month ago doesn't change the validity of the last success time
metric that cronjob pushed.[]{#ch21.xhtml#idm45207087247376
primary="pull" data-type="indexterm"}

Pull is at the very core of Prometheus; work with it rather than against
it.

Prometheus offers some service-side security support, as described in
[Chapter 20](#ch20.xhtml#security_chapter){data-type="xref"}.[]{#ch21.xhtml#idm45207087244608
primary="server-side security"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087243904
primary="reverse proxy" secondary="running Prometheus behind"
data-type="indexterm"} It is, however, common to secure Prometheus
behind a reverse proxy, which enables more flexibility or more
functionalities, such as native Let's Encrypt support, which is out of
scope for the Prometheus server. That would usually be using a reverse
proxy such as [Traefik](https://traefik.io/traefik),
[Caddy](https://caddyserver.com), [nginx](https://nginx.org), or
[httpd](https://httpd.apache.org), which offer a wide range of
security-related features. You may also want the reverse proxy to block
access to the admin and lifecycle endpoints to protect against
Cross-Site Request Forgery (XSRF), and use HTTP headers to protect
against Cross-Site Scripting (XSS).

When running Prometheus[]{#ch21.xhtml#idm45207087239632
primary="--web.external-url flag" primary-sortas="web.external"
data-type="indexterm"} behind a reverse proxy, you should pass
Prometheus the URL under which it is available via the
`--web.external-url` flag, so that the Prometheus UI and the generator
URL in alerts work correctly. If your reverse proxy changes the HTTP
path before sending it on to Prometheus, set the `--web.route-prefix`
flag to the prefix of the new paths.[]{#ch21.xhtml#idm45207087237488
primary="--web.route-prefix flag" primary-sortas="web.route"
data-type="indexterm"}

::: {.note data-type="note"}
###### Note

Like Prometheus, the Alertmanager also has `--web.external-url` and
`--web.route-prefix` flags.
:::

While Prometheus and the[]{#ch21.xhtml#idm45207087233936
primary="authentication" data-type="indexterm"} Alertmanager don't
support authentication for serving, they do support it for talking to
other systems, including alerting, notification, most service discovery
mechanisms, remote read, remote write, and scraping, as was covered in
["How to
Scrape"](#ch08.xhtml#how_to_scrape){data-type="xref"}.[]{#ch21.xhtml#idm45207087232304
primary="networks" startref="ix_netw"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087231328
primary="deploying Prometheus" secondary="running Prometheus"
startref="ix_depPrmrunntw" tertiary="networks and authentication"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087229776
primary="deploying Prometheus" secondary="running Prometheus"
startref="ix_depPrmrun" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Planning for Failure" data-type="sect1"}
::: {#ch21.xhtml#planning_for_failure .sect1}
# Planning for Failure

In distributed systems, failure is a fact of life.
[]{#ch21.xhtml#ix_depPrmfail primary="deploying Prometheus"
secondary="planning for a failure"
data-type="indexterm"}[]{#ch21.xhtml#ix_fail primary="failures"
secondary="planning for" data-type="indexterm"}Prometheus does not take
the path of attempting a clustered design to handle machine failures,
since such designs are very tricky to get right and turn out to be less
reliable than nonclustered solutions more often than you'd expect. Nor
does Prometheus attempt to backfill data if a scrape failed. If the
scrape failure was due to overload, backfilling when load goes back down
a bit could only cause the overload to happen again. It's better when
monitoring systems have predictable load and don't exacerbate outages.

Due to the preceding design, if a scrape fails, `up` will be `0` for
that scrape, and you will have a gap in your time series. But this is
not something you should worry about.[]{#ch21.xhtml#idm45207087222112
primary="gaps" secondary="due to failed scrapes" data-type="indexterm"}
You will not care about the vast majority of your samples, gaps
included, a week after they are collected (if not sooner). For
monitoring, Prometheus takes the stance that it's more important that
your monitoring is generally reliable and available, rather than 100%
accurate. For metrics-based monitoring, 99.9% accuracy is fine for most
purposes. It is more useful for you to know that latency increased by a
millisecond than whether that increase was to 101.2 or 101.3
milliseconds `rate` is resilient to the occasional failed scrape, as
long as your range is at least four times the scrape interval, as
discussed in ["rate"](#ch16.xhtml#rate){data-type="xref"}.

When discussing reliability, the first []{#ch21.xhtml#idm45207087219392
primary="reliability" data-type="indexterm"}question you should ask is
how reliable do you need your monitoring to be? If you are monitoring a
system that has a 99.9% SLA, then there's no point spending your time
and effort designing and maintaining a monitoring system that will be
99.9999% available. Even if you could build such a system, neither the
internet connections that your users use nor the response of the humans
who are on call are that reliable.

Taking an example, in Europe it is common to use SMS for paging as it is
generally fast, cheap, and reliable. However, for a few hours every year
it grinds to a halt when the entire country wishes each other Happy New
Year, which makes it at most 99.95% reliable over a year. You can have
contingencies in place to handle things like this, but as you try backup
paging devices and escalating to the secondary on call, the minutes are
ticking away. As mentioned in
["for"](#ch18.xhtml#for){data-type="xref"}, if you have an issue that
requires a resolution in under 5 minutes, you should automate it rather
than hope your on call engineers will be able to handle it in time.

In this context we'd like to talk about reliable
alerting.[]{#ch21.xhtml#idm45207087216576 primary="alerting"
secondary="reliable" data-type="indexterm"} If a Prometheus dies for
some reason, you should have it automatically restart, and disruption
should be minimal beyond `for` state resetting (as discussed in
["for"](#ch18.xhtml#for){data-type="xref"}).^[20](#ch21.xhtml#idm45207087214032){#ch21.xhtml#idm45207087214032-marker
data-type="noteref"}^ But if the machine Prometheus is on dies and
Prometheus cannot restart, you won't have alerts until you replace it.
If you are using a cluster scheduler such as Kubernetes, you can expect
this to happen promptly, which may well
suffice.^[21](#ch21.xhtml#idm45207087213200){#ch21.xhtml#idm45207087213200-marker
data-type="noteref"}^ If replacement is a more manual process, this
probably won't be acceptable.

The good news is that you can easily make alerting more reliable by
eliminating the single point of failure
(SPOF).[]{#ch21.xhtml#idm45207087212032
primary="single point of failure (SPOF)" data-type="indexterm"} If you
run two identical Prometheus servers, then as long as one of them is
running you will have alerts, and the Alertmanager will automatically
deduplicate the alerts because they will have identical
labels.[]{#ch21.xhtml#idm45207087211232 primary="external labels"
secondary="unique, for every Prometheus"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087210272
primary="alert_relabel_configs" data-type="indexterm"}

As mentioned in ["External
Labels"](#ch18.xhtml#external_labels){data-type="xref"}, every
Prometheus should have unique external labels, so to maintain that
constraint you can use `alert_relabel_configs` (as discussed in
["Configuring Alertmanagers in
Prometheus"](#ch18.xhtml#configuring_alertmanagers){data-type="xref"}):

``` {data-type="programlisting"}
global:
  external_labels:
    region: dublin1
alerting:
  alertmanagers:
   - static_configs:
      - targets: ['localhost:9093']
  alert_relabel_configs:
    - source_labels: [region]
      regex: (.+)\d+
      target_label: region
```

This will remove the `1` from `dublin1` before sending the alert to the
Alertmanager. The second Prometheus would have a `region` label of
`dublin2` as an external label.

I've mentioned now a few times that external labels should be unique
across all your Prometheus servers. This is so that if you have multiple
Prometheus servers in a setup like the preceding one and you are either
using remote write or federation from them, the metrics from the
different Prometheus servers won't clash. Even in perfect conditions,
different Prometheus servers will see slightly different data, which
could be misinterpreted as a counter reset, for example. In less optimal
conditions, such as a network partition, each of your redundant
Prometheus servers could see wildly different information.

This brings me to the question of reliability for dashboards,
federation, and remote write. There is no general way you can
automatically synthesize the "correct" data from the different
Prometheus servers, and going via load balancer for Grafana or
federation would lead to artifacts. I suggest taking the easy way out
and only dashboarding/federating/writing from one of the Prometheus
servers, and if it is down, live with the gap. In the rare event that
the gap covers a period you care about, you can always look at the data
in the other Prometheus by hand.

For global Prometheus servers, as discussed in ["Going Global with
Federation"](#ch21.xhtml#federation){data-type="xref"}, the trade-offs
are a bit different. []{#ch21.xhtml#idm45207087201840
primary="global Prometheus servers"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087201104
primary="federation" secondary="global Prometheus servers"
data-type="indexterm"}As global Prometheus servers are monitoring across
failure zones, it is plausible that the global server could be down for
hours or days if there was, for example, a major power outage in the
datacenter. This is fine for the datacenter Prometheus servers since
they aren't running, but neither is anything they were going to be
monitoring. We recommend that you always run at least two global
Prometheus servers in different datacenters and in dashboards making
graphs available from all of the global servers. Similarly for remote
write.^[22](#ch21.xhtml#idm45207087200016){#ch21.xhtml#idm45207087200016-marker
data-type="noteref"}^ It is the responsibility of the person using the
dashboards to interpret the data from the differing
sources.[]{#ch21.xhtml#idm45207087199104 primary="deploying Prometheus"
secondary="planning for a failure" startref="ix_depPrmfail"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087197888
primary="failures" secondary="planning for" startref="ix_fail"
data-type="indexterm"}

::: {.section pdf-bookmark="Alertmanager Clustering" data-type="sect2"}
::: {#ch21.xhtml#alertmanager_clustering .sect2}
## Alertmanager Clustering

You will want to run one centralized Alertmanager setup for your
entire[]{#ch21.xhtml#idm45207087194800 primary="deploying Prometheus"
secondary="planning for a failure" tertiary="Alertmanager clustering"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087193472
primary="Alertmanager" secondary="clustering"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087192528
primary="clustering (Alertmanager)"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087191840
primary="failures" secondary="planning for"
tertiary="Alertmanager clustering" data-type="indexterm"} organization,
so that everyone has one place to look at alerts and silences, and you
get the maximum benefits from alert grouping. Unless you have a small
setup, you can take advantage of the Alertmanager's clustering feature,
whose architecture is shown in
[Figure 21-2](#ch21.xhtml#am_ha_diagram){data-type="xref"}.

<figure>
<div id="ch21.xhtml#am_ha_diagram" class="figure">
<img src="assets/pur2_2102.png" width="600" height="207"
alt="Clustering architecture for the Alertmanager." />
<h6><span class="label">Figure 21-2. </span>Clustering architecture for
the Alertmanager</h6>
</div>
</figure>

The Alertmanager uses [HashiCorp's
memberlist](https://oreil.ly/fbRs_)^[23](#ch21.xhtml#idm45207087186624){#ch21.xhtml#idm45207087186624-marker
data-type="noteref"}^ to gossip information about notifications and
silences.^[24](#ch21.xhtml#idm45207087185888){#ch21.xhtml#idm45207087185888-marker
data-type="noteref"}^ This is not a consensus-based design, so there is
no need to have an odd number of Alertmanagers. This is what is known as
an AP, or Availability and Partition-tolerant, design, so as long as
your Prometheus can talk to at least one Alertmanager that can
successfully send notifications, your notifications will get through.
When there are rare issues such as network partitions, you may get
duplicate notifications, but that's better than not getting
notifications at all.

For the clustering to work, every Prometheus must send its alerts to
every Alertmanager. How it works is that the Alertmanagers order
themselves. The first Alertmanager sends notifications normally, and if
successful, gossips that the notification was sent. The second
Alertmanager has a small delay before sending notifications. If it
doesn't get the gossip that the first Alertmanager sent the
notification, then it will send the notification. The third Alertmanager
will have a slightly longer delay and so on. The Alertmanagers should
all have identical *alertmanager.yml* files, but the worst that should
happen if they don't is that duplicate notifications will be sent.

To get it running with Alertmanager version 0.24.0 on two machines
called `foo` and `bar`, you would start the Alertmanager as follows:

``` {data-type="programlisting"}
# On the machine foo
alertmanager --cluster.peer bar:9094

# On the machine bar
alertmanager --cluster.peer foo:9094
```

The easiest way for you to test if clustering is working is to create a
silence on one Alertmanager and see if it appears on the other
Alertmanager. There will also be a list of all members of the cluster on
the Alertmanager's Status page.
:::
:::

::: {.section pdf-bookmark="Meta- and Cross-Monitoring" data-type="sect2"}
::: {#ch21.xhtml#metamonitoring .sect2}
## Meta- and Cross-Monitoring

Thus far we have covered monitoring many different types of systems, but
among those we have not covered monitoring your monitoring system.
[]{#ch21.xhtml#idm45207087178784 primary="failures"
secondary="planning for" tertiary="meta- and cross-monitoring"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087177472
primary="deploying Prometheus" secondary="planning for a failure"
tertiary="meta- and cross-monitoring" data-type="indexterm"}It is fairly
standard to have each of your Prometheus servers scrape itself, but that
doesn't help you when that Prometheus is having issues.
[]{#ch21.xhtml#idm45207087175952 primary="metamonitoring"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087175280
primary="monitoring" secondary="metamonitoring"
data-type="indexterm"}How you monitor your monitoring is known as
*metamonitoring*.

The general approach to metamonitoring for you to take is to have one
Prometheus per datacenter that monitors all of the other Prometheus
servers in that datacenter. This doesn't have to be a Prometheus server
dedicated to this purpose as Prometheus is pretty cheap to monitor, and
even if you have a setup where each team is entirely responsible for
running their own Prometheus servers, it is still wise to offer
metamonitoring as a central shared service.

A global Prometheus can then scrape all of your per-datacenter
metamonitoring Prometheus servers, likely both for */metrics* and
federating aggregated metrics about all of the Prometheus servers in
your organization.

This still leaves the question of how you should monitor the global
Prometheus servers. *Cross-monitoring* is metamonitoring
where[]{#ch21.xhtml#idm45207087171296 primary="cross-monitoring"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087170592
primary="monitoring" secondary="cross-monitoring" data-type="indexterm"}
Prometheus servers monitor each other, rather than the usual
metamonitoring hierarchy where Prometheus servers at the same "level"
monitor each other. For example, you will usually have two global
Prometheus servers scrape each other's */metrics* and alert if the other
Prometheus is down. You could also have the datacenter Prometheus
servers alerting on the global Prometheus
servers.^[25](#ch21.xhtml#idm45207087168976){#ch21.xhtml#idm45207087168976-marker
data-type="noteref"}^

Even with all this meta- and cross-monitoring, you are still depending
on Prometheus to monitor Prometheus. In the absolute worst case, a bug
could take out all of your Prometheus servers at the same time, so it
would be wise to have alerting that can catch that. One approach would
be an end-to-end alerting test. An always firing alert would
continuously fire a notification via your paging provider, which feeds
into a dead man's switch. The dead man's switch would then page
you^[26](#ch21.xhtml#idm45207087166928){#ch21.xhtml#idm45207087166928-marker
data-type="noteref"}^ if it doesn't receive a notification for too long
a period. This would test your Prometheus, Alertmanager, network, and
paging provider.

When designing your metamonitoring, don't forget to scrape other
monitoring-related components, such as the Alertmanager and the
*/metrics* of Blackbox/SNMP-style exporters.
:::
:::
:::
:::

::: {.section pdf-bookmark="Managing Performance" data-type="sect1"}
::: {#ch21.xhtml#managing_performance .sect1}
# Managing Performance

Unless you have a particularly small and unchanging setup, running into
performance issues is more of a when than an if.[]{#ch21.xhtml#ix_perf
primary="performance" secondary="managing"
data-type="indexterm"}[]{#ch21.xhtml#ix_depPrmperf
primary="deploying Prometheus" secondary="managing performance"
data-type="indexterm"} As discussed in
["Cardinality"](#ch05.xhtml#cardinality_section){data-type="xref"} and
elsewhere, high cardinality metrics are likely to be the primary cause
of the performance problems you encounter.

You may also encounter recording rules and dashboards using overly
expensive queries, such as those with range vectors over long durations,
as mentioned in
[["Histogram"](#ch13.xhtml#histogram_intro){data-type="xref"}]{.keep-together}.
You can use the Rules status page, as you saw in
[Figure 17-1](#ch17.xhtml#rules_status){data-type="xref"}, to find
expensive recording
rules.^[27](#ch21.xhtml#idm45207087157072){#ch21.xhtml#idm45207087157072-marker
data-type="noteref"}^

::: {.section pdf-bookmark="Detecting a Problem" data-type="sect2"}
::: {#ch21.xhtml#idm45207087156352 .sect2}
## Detecting a Problem

Prometheus exposes a variety of metrics about its own performance, so
you don't just have to rely on noticing that your dashboards have gotten
sluggish or are timing out.[]{#ch21.xhtml#idm45207087154704
primary="performance" secondary="managing"
tertiary="detecting a problem"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087153456
primary="deploying Prometheus" secondary="managing performance"
tertiary="detecting a problem" data-type="indexterm"} While metrics can
and do change names and meanings from version to version, it is unusual
for a metric to go away completely.

`prometheus_rule_group_iterations_missed_total` can indicate that some
rule groups are taking too long to evaluate.
[]{#ch21.xhtml#idm45207087151232 primary="groups"
secondary="problem with rule groups" data-type="indexterm"}Comparing
`prometheus_rule_group_​`[`last_duration_seconds`]{.keep-together}
against `prometheus_rule_group_interval_seconds` can tell you which
group is at fault and if it is a recent change in behavior.

`prometheus_notifications_dropped_total` indicates issues talking to the
Alertmanager, and if `prometheus_notifications_queue_length` is
approaching `prometheus_notifications_queue_capacity`, you may start
losing alerts.

Each service discovery mechanism tends to have a metric such as
`prometheus_​`[`sd_file_read_errors_total`]{.keep-together} and
`prometheus_sd_ec2_refresh_failures_​`[`total`]{.keep-together}
indicating problems. []{#ch21.xhtml#idm45207087144240
primary="service discovery" secondary="problems in"
data-type="indexterm"}You should keep an eye on the counters for the SD
mechanisms you use.

`prometheus_rule_evaluation_failures_total`,
`prometheus_tsdb_compactions_​`[`failed_total`]{.keep-together}, and
`prometheus_tsdb_wal_corruptions_total` indicate that something has gone
wrong in the storage layer.[]{#ch21.xhtml#idm45207087140832
primary="storage layer, problem in" data-type="indexterm"} In the worst
case you can always stop Prometheus,
delete^[28](#ch21.xhtml#idm45207087139888){#ch21.xhtml#idm45207087139888-marker
data-type="noteref"}^ the storage directory, and start it back up again.
:::
:::

::: {.section pdf-bookmark="Finding Expensive Metrics and Targets" data-type="sect2"}
::: {#ch21.xhtml#idm45207087138960 .sect2}
## Finding Expensive Metrics and Targets

As was mentioned in ["by"](#ch14.xhtml#by){data-type="xref"}, you can
use queries such as:

``` {data-type="programlisting"}
topk(10, count by(__name__)({__name__=~".+"}))
```

to find metrics with high cardinality.[]{#ch21.xhtml#idm45207087135408
primary="targets" secondary="finding expensive targets"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087134368 primary="metrics"
secondary="finding expensive metrics"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087133408
primary="performance" secondary="managing"
tertiary="finding expensive targets and metrics"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087132176
primary="deploying Prometheus" secondary="managing performance"
tertiary="finding expensive targets and metrics" data-type="indexterm"}
You could also aggregate by `job` to find which applications are
responsible for the most time series. But these are potentially very
expensive queries as they touch every time series and accordingly should
be used with caution.

In addition to `up`, Prometheus adds three other samples for every
target scrape. `scrape_samples_scraped` is the number of samples that
were on the */metrics*. As this is a single time series per target, it
is much cheaper to work with than the previous PromQL expression.
`scrape_samples_post_metric_relabeling` is similar, but it excludes
samples that were dropped by `metric_relabel_configs`.

The final special sample added is `scrape_duration_seconds`, which is
how long that scrape took. This can be useful to check if timeouts are
occurring if it is reaching the timeout value, or as an indication that
a target is getting overloaded.

::: {.section pdf-bookmark="Hashmod" data-type="sect3"}
::: {#ch21.xhtml#hashmod .sect3}
### Hashmod

If your Prometheus is so overloaded by data from scrapes that you cannot
run queries, there is a way to scrape a subset of your
targets.[]{#ch21.xhtml#idm45207087124288 primary="performance"
secondary="managing" tertiary="hashmod relabel action"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087123040
primary="deploying Prometheus" secondary="managing performance"
tertiary="hashmod relabel action"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087121824
primary="scraping" secondary="scraping subset of targets"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087120912 primary="targets"
secondary="scraping subset of" data-type="indexterm"} There is another
relabel action called `hashmod` that calculates the hash of a label and
takes its modulus. []{#ch21.xhtml#idm45207087119392
primary="hashmod relabel action"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087118608
primary="relabeling" secondary="hashmod action"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087117664
primary="drop (relabel action)" data-type="indexterm"}Combined with the
`drop` relabel action, you could use this to scrape an arbitrary 10% of
your [targets]{.keep-together}:

``` {data-type="programlisting"}
scrape_configs:
 - job_name: my_job
   # Service discovery etc. goes here.
   relabel_configs:
    - source_labels: [__address__]
      modulus:       10
      target_label:  __tmp_hash
      action:        hashmod
    - source_labels: [__tmp_hash]
      regex:         0
      action:        keep
```

With only 10% of the targets to scrape, if you can spin up a test
Prometheus, you should now be able to find out which metric is to blame.
If only some targets are causing the problem, you can change which 10%
of targets to scrape by changing the `regex` to `1`, `2`, and so on up
to `9`.
:::
:::
:::
:::

::: {.section pdf-bookmark="Reducing Load" data-type="sect2"}
::: {#ch21.xhtml#reducing_load_chap_twenty .sect2}
## Reducing Load

Once you have identified expensive metrics, you have a few
options.[]{#ch21.xhtml#idm45207087110624 primary="load, reducing"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087109840
primary="performance" secondary="managing" tertiary="reducing load"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087108624
primary="deploying Prometheus" secondary="managing performance"
tertiary="reducing load" data-type="indexterm"} The first thing to do is
try to fix the metric in the source code to reduce its cardinality.

While you're waiting[]{#ch21.xhtml#idm45207087106784
primary="metric_relabel_configs" data-type="indexterm"} for that to
happen, you have several tactical options. The first is to drop the
metric at ingestion time using `metric_relabel_configs`:

``` {data-type="programlisting"}
scrape_configs:
 - job_name: some_application
   static_configs:
    - targets:
      - localhost:1234
   metric_relabel_configs:
    - source_labels: [__name__]
      regex: expensive_metric_name
      action: drop
```

This still transfers the metric over the network and parses it, but it's
still cheaper than ingesting it into the storage
layer.^[29](#ch21.xhtml#idm45207087103904){#ch21.xhtml#idm45207087103904-marker
data-type="noteref"}^

If particular applications are being problematic you can also drop those
targets with relabeling.[]{#ch21.xhtml#idm45207087099168
primary="scrape_interval"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087098496
primary="evaluation_interval" data-type="indexterm"}

The final option is to increase the `scrape_interval` and
`evaluation_interval` for the Prometheus. This can buy you some
breathing room, but keep in mind that it's not practical to increase
these beyond 2 minutes. Changing the scrape interval may also break some
PromQL expressions that depend on it having a specific value.

There is one other option in the scrape config that can be of use to you
called [`sample_limit`]{.keep-together}. If the number of samples after
`metric_relabel_configs`^[30](#ch21.xhtml#idm45207087094832){#ch21.xhtml#idm45207087094832-marker
data-type="noteref"}^ is higher than `sample_limit`, then the scrape
will fail and the samples will not be
ingested.[]{#ch21.xhtml#idm45207087093264
primary="scrape_samples_post_metric_relabeling"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087092416
primary="sample_limit" data-type="indexterm"} This is disabled by
default but can act as an emergency relief valve in the event that one
of your targets blows up in cardinality, such as by adding a metric with
a customer identifier as a label, for example. This is not a setting to
micromanage or to attempt to build some form of quota system on top of;
if you are going to use it, choose a single generous value that will
rarely need bumping.

We advise having enough buffer room in your Prometheus to be able to
handle a moderate spurt in cardinality and targets.
:::
:::

::: {.section pdf-bookmark="Horizontal Sharding" data-type="sect2"}
::: {#ch21.xhtml#idm45207087112096 .sect2}
## Horizontal Sharding

If you are running into scaling[]{#ch21.xhtml#idm45207087089488
primary="horizontal sharding"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087088784
primary="performance" secondary="managing"
tertiary="horizontal sharding"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087087568
primary="deploying Prometheus" secondary="managing performance"
tertiary="horizontal sharding" data-type="indexterm"} challenges due to
`instance` cardinality rather than instrumentation label cardinality,
there is a way to horizontally shard Prometheus using the `hashmod`
relabel action you saw in
["Hashmod"](#ch21.xhtml#hashmod){data-type="xref"}.
[]{#ch21.xhtml#idm45207087084432 primary="hashmod relabel action"
data-type="indexterm"}This is an approach that is only typically needed
if you have many thousands of targets of a single type
[]{#ch21.xhtml#idm45207087083472 primary="vertical sharding"
data-type="indexterm"}of application, as vertical sharding is a far
simpler way to scale Prometheus (as discussed in ["Growing
Prometheus"](#ch21.xhtml#growing_prometheus){data-type="xref"}).

The approach to horizontal sharding is to have a master Prometheus and
several scraping Prometheus servers. []{#ch21.xhtml#idm45207087081376
primary="scraping" secondary="scraping Prometheus servers"
data-type="indexterm"}Your scraping Prometheus servers each scrape a
subset of the targets:

``` {data-type="programlisting"}
global:
  external_labels:
    env: prod
    scraper: 2
scrape_configs:
 - job_name: my_job
   # Service discovery etc. goes here.
   relabel_configs:
    - source_labels: [__address__]
      modulus:       4
      target_label:  __tmp_hash
      action:        hashmod
    - source_labels: [__tmp_hash]
      regex:         2 # This is the 3rd scraper.
      action:        keep
```

Here you can see there are four scrapers from the `modulus` setting.
[]{#ch21.xhtml#idm45207087078704 primary="modulus setting"
data-type="indexterm"}Each scraper should have a unique external label,
plus the external labels of the master Prometheus. The master Prometheus
can then use the remote read endpoint of Prometheus itself to
transparently pull in data from the scrapers:

``` {data-type="programlisting"}
global:
  external_labels:
    env: prod
remote_read:
 - url: http://scraper0:9090/api/v1/read
   read_recent: true
 - url: http://scraper1:9090/api/v1/read
   read_recent: true
 - url: http://scraper2:9090/api/v1/read
   read_recent: true
 - url: http://scraper3:9090/api/v1/read
   read_recent: true
```

Remote read has an optimization []{#ch21.xhtml#idm45207087076768
primary="read_recent: true"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087076064
primary="remote read" data-type="indexterm"}where it will try not to
read in data it should already have locally, which makes sense if it is
being used with remote write to work with a long-term storage system.
`read_recent: true` disables this. Due to the external labels, the
metrics from each scraper will have a `scraper` label matching where
they came from.[]{#ch21.xhtml#idm45207087074368 primary="scraper labels"
data-type="indexterm"}

All the same caveats as with federation, covered in ["Going Global with
Federation"](#ch21.xhtml#federation){data-type="xref"}, apply here.
[]{#ch21.xhtml#idm45207087072384 primary="federation"
data-type="indexterm"}This is not a way to have one Prometheus that can
let you transparently access all of your Prometheus servers. In fact, it
would actually be a great way to take out all of your monitoring
simultaneously through a single expensive query. When using this it is
best to aggregate what you can inside the scrapers (following ["Reducing
Cardinality"](#ch17.xhtml#reducing_cardinality){data-type="xref"}), to
reduce the amount of data that the master needs to pull in from the
scrapers.

You should be generous with the number of scrapers and aim to only have
to increase every few years. When you do increase it, you should at
least double the number of scrapers to avoid having to increase the
number again soon.[]{#ch21.xhtml#idm45207087070368 primary="performance"
secondary="managing" startref="ix_perf"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087069120
primary="deploying Prometheus" secondary="managing performance"
startref="ix_depPrmperf" data-type="indexterm"}
:::
:::
:::
:::

::: {.section pdf-bookmark="Managing Change" data-type="sect1"}
::: {#ch21.xhtml#idm45207087067648 .sect1}
# Managing Change

Over time you will find that you need to change the structure of your
target labels due to changes in the architecture of your
systems.[]{#ch21.xhtml#idm45207087066064 primary="change, managing"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087065360
primary="deploying Prometheus" secondary="managing change"
data-type="indexterm"} Which applications will host the metrics used for
capacity planning will change over time as your applications split and
merge as a natural part of development. Metrics will appear and
disappear from release to release.

You have the option of using `metric_relabel_configs` to rename metrics
and cram the new hierarchy into your existing target labels. But over
time you would find that these tweaks and hacks accumulate and
ultimately cause more confusion than you may have been trying to prevent
by trying to keep things the same.

We would advise accepting that changes like this are a natural part of
the evolution of your system, and as with gaps due to failed scrapes,
you usually find that you don't care much about the old names after the
fact.[]{#ch21.xhtml#idm45207087062976 primary="gaps"
secondary="due to failed scrapes" data-type="indexterm"}

Long-term processes such as capacity planning, on the other hand, do
care about history. At the least you should note the names of the
metrics over time and possibly consider using the approach in ["Rules
for APIs"](#ch17.xhtml#rules_for_apis){data-type="xref"} in your global
Prometheus if the changes are a bit too frequent to manage by hand.

In this chapter you learned how to approach a Prometheus deployment, and
in what order to add Prometheus monitoring to your system, how to
architect and run Prometheus, and how to handle performance problems
when they arise.
:::
:::

::: {.section pdf-bookmark="Getting Help" data-type="sect1"}
::: {#ch21.xhtml#idm45207087060272 .sect1}
# Getting Help

Even after reading everything up to this point, you may have questions
that are not covered here. []{#ch21.xhtml#idm45207087058512
primary="deploying Prometheus" secondary="getting help"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087057536
primary="help, sources of" data-type="indexterm"}There are a number of
places you can ask questions. The Community Page of the [Prometheus
website](https://oreil.ly/lqZT8) lists the official communication method
of the Prometheus projects, such as the
[prometheus-users](https://oreil.ly/CTPNp) mailing list, which is also
available for user questions. There are also unofficial venues for
questions, including the [Prometheus tag on
StackOverflow](https://oreil.ly/MGxtQ), the #prometheus channel on the
[CNCF Slack](https://slack.cncf.io), and the [PrometheusMonitoring
subreddit](https://oreil.ly/nDPxu). Finally, there are several companies
and individuals offering commercial support listed on the [Support &
Training](https://oreil.ly/XEg_F) page, including Brian's company,
[Robust Perception](https://oreil.ly/X9OYd), and Julien's company,
[O11y](https://o11y.eu).

We hope you have found this and all of the preceding chapters useful and
that Prometheus will help to make your life easier through metrics-based
monitoring.[]{#ch21.xhtml#idm45207087050416
primary="deploying Prometheus" startref="ix_depPrm"
data-type="indexterm"}
:::
:::

::: {data-type="footnotes"}
^[1](#ch21.xhtml#idm45207087560160-marker)^ A rollout is the process of
releasing a new version of a software application or system to users.

^[2](#ch21.xhtml#idm45207087558304-marker)^ If you are on Windows, use
the Windows Exporter instead of the Node Exporter.

^[3](#ch21.xhtml#idm45207087508336-marker)^ We continue to be amazed by
the seductive power of a pretty dashboard, especially over other factors
such as if the metrics in the dashboard are in any way useful. Do not
underestimate this when trying to convince others to use Prometheus.

^[4](#ch21.xhtml#idm45207087493488-marker)^ Monitoring across failure
domain boundaries, such as across datacenters, is possible but messy as
you introduce a whole slew of network-related failure modes. If you have
hundreds of tiny datacenters with only a handful of machines each, one
Prometheus per region/continent can be an acceptable trade-off.

^[5](#ch21.xhtml#idm45207087484784-marker)^ For example, it is sane only
to have one target label hierarchy within a Prometheus. If a team has a
different idea of what a region is than everyone else, they should run
their own Prometheus.

^[6](#ch21.xhtml#idm45207087402816-marker)^ The */federate* endpoint
automatically includes an empty instance label in its output for any
metrics lacking an `instance` label, in the same way the Pushgateway
does, as mentioned in
["Pushgateway"](#ch04.xhtml#pushgateway){data-type="xref"}.

^[7](#ch21.xhtml#idm45207087393280-marker)^ Let's say that you were
aggregating up every metric from an application with a hundred instances
and a global Prometheus was pulling these aggregated metrics. For the
same resources that a datacenter Prometheus uses, the global Prometheus
could federate metrics from a hundred datacenters. In reality the global
Prometheus can handle far more, as not all metrics would be aggregated.

^[8](#ch21.xhtml#idm45207087360400-marker)^ There are no exact numbers,
but we would consider 10,000 time series as starting to get large.

^[9](#ch21.xhtml#idm45207087344672-marker)^ Indeed, Brian has heard
various different monitoring systems report that around 90% of metrics
data is not used after the first 24 hours. The problem, of course, is
knowing in advance which 90% you'll never need again.

^[10](#ch21.xhtml#idm45207087340768-marker)^ As discussed in ["Going
Global with Federation"](#ch21.xhtml#federation){data-type="xref"}, the
global Prometheus will only have aggregated metrics.

^[11](#ch21.xhtml#idm45207087338320-marker)^ This works in the same way
as the `match[]` URL parameter for federation.

^[12](#ch21.xhtml#idm45207087324144-marker)^ Usually a multiweek cache.

^[13](#ch21.xhtml#idm45207087306592-marker)^ For Prometheus 1.x, use the
`prometheus_local_storage_ingested_samples_total` metric instead.

^[14](#ch21.xhtml#idm45207087300800-marker)^ The retention size might
not take into account blocks being compacted, so it is best to not set
it at 100% of your storage capacity. Using size-based retention also
means that you don't know how far in the past Prometheus will keep the
data as your number of time series grows. In this case, the TSDB metric
`prometheus_tsdb_lowest_timestamp` can be handy in alerting rules.

^[15](#ch21.xhtml#idm45207087277376-marker)^ Windows users can use HTTP
instead of `SIGTERM` and `SIGHUP`, which requires
[]{#ch21.xhtml#idm45207087275776 primary="--web.enable-lifecycle flag"
primary-sortas="web.enable" data-type="indexterm"}the
`--web.enable-lifecycle` flag to be specified.

^[16](#ch21.xhtml#idm45207087271920-marker)^ Such a system actually
exists, [mixtool server](https://oreil.ly/Dqc2B), but it is highly
experimental.

^[17](#ch21.xhtml#idm45207087268816-marker)^ To avoid confusion, systems
like Docker, Docker Compose, and Kubernetes are not configuration
management systems; they are potential outputs for a configuration
management system.

^[18](#ch21.xhtml#idm45207087267280-marker)^ Part of the gettext
library.

^[19](#ch21.xhtml#idm45207087248704-marker)^ For the same reasons that
you want to run a StatsD Exporter per application instance, rather than
one per datacenter.

^[20](#ch21.xhtml#idm45207087214032-marker)^ For this reason we
recommend you design your critical alerts to be up and running in a
fresh Prometheus within an hour, if not sooner.

^[21](#ch21.xhtml#idm45207087213200-marker)^ If using network storage
such as Amazon EBS, the Prometheus may even continue on with the data of
the previous run.

^[22](#ch21.xhtml#idm45207087200016-marker)^ Global Prometheus servers
are at the top of the federation hierarchy, so nothing generally
federates from them.

^[23](#ch21.xhtml#idm45207087186624-marker)^ Prior to 0.15.0, the
Alertmanager used the Weaveworks Mesh library.

^[24](#ch21.xhtml#idm45207087185888-marker)^ Aside from gossiping, the
Alertmanager also stores data on local disk, so even in a nonclustered
setup you won't lose state by restarting the Alertmanager.

^[25](#ch21.xhtml#idm45207087168976-marker)^ With all these alerts ready
to fire when a global Prometheus goes down, you should to ensure that
they all have the same labels and get automatically deduplicated at the
Alertmanager. An explicit alert label of `datacenter: global` (or
whatever you use as a datacenter label) to prevent the datacenter
Prometheus's `datacenter` external label applying is one approach you
could take.

^[26](#ch21.xhtml#idm45207087166928-marker)^ Preferably not solely via
your usual paging provider, since that could be what has failed.

^[27](#ch21.xhtml#idm45207087157072-marker)^ By looking at the duration,
for example.

^[28](#ch21.xhtml#idm45207087139888-marker)^ Or rename, to back up
existing data.

^[29](#ch21.xhtml#idm45207087103904-marker)^ The Java and Python clients
support fetching specific time series using URL parameters such as
*/metrics?metric\[\]=process_cpu_seconds_total*.
[]{#ch21.xhtml#idm45207087102752 primary="Java"
secondary="client library"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087101776 primary="Python"
secondary="client library"
data-type="indexterm"}[]{#ch21.xhtml#idm45207087100832
primary="client libraries"
secondary="fetching time series using URL parameters"
data-type="indexterm"}This may not always work for custom collectors,
but it can save a lot of resources on both sides of the scrape if there
are only a small number of specific metrics you want.

^[30](#ch21.xhtml#idm45207087094832-marker)^ Which is to say, the value
of `scrape_samples_post_metric_relabeling`.
:::
:::
:::

[]{#ix01.xhtml}

::: {#ix01.xhtml#sbo-rt-content}
::: {.section .index data-type="index"}
::: {#ix01.xhtml#idm45207087048688 .index}
# Index

::: {data-type="index"}
::: {data-type="indexdiv"}
### [Symbols]{gentext="indexsymbols"}

-   [!= (negative equality matcher)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092624560){data-type="index:locator"}
-   [!\~ (negative regular expression matcher)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092611712){data-type="index:locator"}
-   [() (parentheses), using to change order of
    evaluation]{data-type="index-term"}, [Operator
    Precedence](#ch15.xhtml#idm45207091008928){data-type="index:locator"}
-   [.\* prefixing/suffixing regular
    expressions]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092617152){data-type="index:locator"}
-   [; (semicolon) separator in source_labels]{data-type="index-term"},
    [Choosing What to
    Scrape](#ch08.xhtml#idm45207099536496){data-type="index:locator"}
-   [= (equality matcher)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092630096){data-type="index:locator"}
-   [== operator]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118771712){data-type="index:locator"}
-   [=\~ (regular expression matcher)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092621072){data-type="index:locator"}
-   [@ (at) modifier]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092490240){data-type="index:locator"},
    [topk and
    bottomk](#ch14.xhtml#idm45207091422560){data-type="index:locator"}
-   [\^ (exponent) operator]{data-type="index-term"},
    [sqrt](#ch16.xhtml#idm45207090912784){data-type="index:locator"}
-   [\| (pipe symbol), alternation operator]{data-type="index-term"},
    [Choosing What to
    Scrape](#ch08.xhtml#idm45207099657184){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### A

-   [abs function]{data-type="index-term"},
    [abs](#ch16.xhtml#idm45207090950816){data-type="index:locator"}
-   [absent function]{data-type="index-term"}, [Missing Series, absent,
    and
    absent_over_time](#ch16.xhtml#idm45207090714096){data-type="index:locator"}
-   [absent_over_time function]{data-type="index-term"}, [Missing
    Series, absent, and
    absent_over_time](#ch16.xhtml#idm45207090711680){data-type="index:locator"}
-   [\_\_address\_\_ labels]{data-type="index-term"},
    [replace](#ch08.xhtml#idm45207098983632){data-type="index:locator"}
    -   [relabeling]{data-type="index-term"}, [job, instance, and
        \_\_address\_\_](#ch08.xhtml#idm45207098896064){data-type="index:locator"}
-   [agent mode]{data-type="index-term"}, [Long-Term
    Storage](#ch21.xhtml#idm45207087315856){data-type="index:locator"}
-   [agents (Consul)]{data-type="index-term"},
    [Consul](#ch08.xhtml#idm45207100001376){data-type="index:locator"},
    [Consul](#ch10.xhtml#idm45207096588752){data-type="index:locator"}
-   [aggregation]{data-type="index-term"},
    [Aggregating](#ch05.xhtml#idm45207101484976){data-type="index:locator"}
    -   [basics of in PromQL]{data-type="index-term"}, [Aggregation
        Basics](#ch13.xhtml#ix_aggPQL){data-type="index:locator"}-[Selectors](#ch13.xhtml#idm45207092656016){data-type="index:locator"}
        -   [counter]{data-type="index-term"},
            [Counter](#ch13.xhtml#idm45207092753328){data-type="index:locator"}
        -   [gauge]{data-type="index-term"},
            [Gauge](#ch13.xhtml#ix_aggPQLgauge){data-type="index:locator"}-[Counter](#ch13.xhtml#idm45207092756976){data-type="index:locator"}
        -   [summary]{data-type="index-term"},
            [Summary](#ch13.xhtml#idm45207092725824){data-type="index:locator"}
    -   [functions for aggregation over time]{data-type="index-term"},
        [Aggregation Over
        Time](#ch16.xhtml#ix_aggovrtm){data-type="index:locator"}-[Aggregation
        Over
        Time](#ch16.xhtml#idm45207090463968){data-type="index:locator"},
        [Composing Range Vector
        Functions](#ch17.xhtml#idm45207090171408){data-type="index:locator"}
    -   [level of in recording rule names]{data-type="index-term"},
        [Naming of Recording
        Rules](#ch17.xhtml#idm45207090114176){data-type="index:locator"}
    -   [preaggregating every application
        metric]{data-type="index-term"}, [How Not to Use
        Rules](#ch17.xhtml#idm45207090148384){data-type="index:locator"}
    -   [using in recording rules]{data-type="index-term"}, [Reducing
        Cardinality](#ch17.xhtml#idm45207090190288){data-type="index:locator"}
-   [aggregation operators]{data-type="index-term"}, [Aggregation
    Operators](#ch14.xhtml#ix_aggops){data-type="index:locator"}-[count_values](#ch14.xhtml#idm45207091369472){data-type="index:locator"}
    -   [avg]{data-type="index-term"},
        [avg](#ch14.xhtml#idm45207091514784){data-type="index:locator"}
    -   [count]{data-type="index-term"},
        [count](#ch14.xhtml#idm45207091546704){data-type="index:locator"}
    -   [count_values]{data-type="index-term"},
        [count_values](#ch14.xhtml#idm45207091390384){data-type="index:locator"}
    -   [group]{data-type="index-term"},
        [group](#ch14.xhtml#idm45207091490896){data-type="index:locator"}
    -   [grouping]{data-type="index-term"},
        [Grouping](#ch14.xhtml#ix_aggopsgrp){data-type="index:locator"}-[by](#ch14.xhtml#idm45207091573760){data-type="index:locator"}
    -   [max and min]{data-type="index-term"}, [min and
        max](#ch14.xhtml#idm45207091455200){data-type="index:locator"}
    -   [quantile]{data-type="index-term"},
        [quantile](#ch14.xhtml#idm45207091416560){data-type="index:locator"}
    -   [stddev]{data-type="index-term"}, [stddev and
        stdvar](#ch14.xhtml#idm45207091478816){data-type="index:locator"}
    -   [stdvar]{data-type="index-term"}, [stddev and
        stdvar](#ch14.xhtml#idm45207091464032){data-type="index:locator"}
    -   [sum]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091564640){data-type="index:locator"}
    -   [topk and bottomk]{data-type="index-term"}, [topk and
        bottomk](#ch14.xhtml#idm45207091439360){data-type="index:locator"}
-   [alert field]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090042528){data-type="index:locator"}
-   [Alert status page]{data-type="index-term"},
    [for](#ch18.xhtml#idm45207089995984){data-type="index:locator"}
-   [alerting]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#idm45207119694752){data-type="index:locator"},
    [Alerting](#ch02.xhtml#ix_alrt2){data-type="index:locator"}-[Alerting](#ch02.xhtml#idm45207116031312){data-type="index:locator"},
    [Alerting](#ch18.xhtml#idm45207090063856){data-type="index:locator"}
    -   [choosing alert thresholds]{data-type="index-term"},
        [for](#ch18.xhtml#idm45207089993600){data-type="index:locator"}
    -   [how to approach, further information
        on]{data-type="index-term"}, [What Are Good
        Alerts?](#ch18.xhtml#idm45207089875600){data-type="index:locator"}
    -   [missing job and]{data-type="index-term"}, [Missing Series,
        absent, and
        absent_over_time](#ch16.xhtml#idm45207090691648){data-type="index:locator"}
    -   [predict_linear function for resource limit
        alerts]{data-type="index-term"},
        [predict_linear](#ch16.xhtml#idm45207090535264){data-type="index:locator"}
    -   [preservation of job and instance labels
        for]{data-type="index-term"},
        [without](#ch14.xhtml#idm45207091623520){data-type="index:locator"}
    -   [reliable]{data-type="index-term"}, [Planning for
        Failure](#ch21.xhtml#idm45207087216576){data-type="index:locator"}
-   [alerting field]{data-type="index-term"}, [Configuring Alertmanagers
    in
    Prometheus](#ch18.xhtml#idm45207089853136){data-type="index:locator"}
-   [alerting rules]{data-type="index-term"}, [Recording Rules and
    Alerts](#ch01.xhtml#idm45207118967856){data-type="index:locator"},
    [Alerting](#ch02.xhtml#idm45207118791280){data-type="index:locator"},
    [Alerting
    Rules](#ch18.xhtml#ix_alrtru){data-type="index:locator"}-[What Are
    Good
    Alerts?](#ch18.xhtml#idm45207089872768){data-type="index:locator"}
    -   [annotations and templates]{data-type="index-term"},
        [Annotations and
        Templates](#ch18.xhtml#ix_alrtruanntmpl){data-type="index:locator"}-[Annotations
        and
        Templates](#ch18.xhtml#idm45207089889776){data-type="index:locator"}
    -   [for field]{data-type="index-term"},
        [for](#ch18.xhtml#idm45207090003856){data-type="index:locator"}
    -   [good alerts]{data-type="index-term"}, [What Are Good
        Alerts?](#ch18.xhtml#idm45207089885392){data-type="index:locator"}
    -   [labels for]{data-type="index-term"}, [Alert
        Labels](#ch18.xhtml#ix_alrtrulbl){data-type="index:locator"}-[Alert
        Labels](#ch18.xhtml#idm45207089933856){data-type="index:locator"}
-   [Alertmanager]{data-type="index-term"}, [Alert
    Management](#ch01.xhtml#idm45207118963744){data-type="index:locator"},
    [Alertmanager](#ch19.xhtml#ix_Alrtmgr2){data-type="index:locator"}-[Alertmanager
    Web
    Interface](#ch19.xhtml#idm45207087886208){data-type="index:locator"}
    -   [clustering]{data-type="index-term"}, [Alertmanager
        Clustering](#ch21.xhtml#idm45207087193472){data-type="index:locator"}
    -   [configuration file]{data-type="index-term"}, [Configuration
        File](#ch19.xhtml#ix_Alrtmgr2cfg){data-type="index:locator"}-[Inhibitions](#ch19.xhtml#idm45207087905920){data-type="index:locator"}
        -   [inhibitions]{data-type="index-term"},
            [Inhibitions](#ch19.xhtml#idm45207087922800){data-type="index:locator"}
        -   [minimal configuration example]{data-type="index-term"},
            [Configuration
            File](#ch19.xhtml#idm45207089790832){data-type="index:locator"}
        -   [receivers]{data-type="index-term"},
            [Receivers](#ch19.xhtml#ix_Alrtmgr2cfgrec){data-type="index:locator"}-[Resolved
            notifications](#ch19.xhtml#idm45207087927360){data-type="index:locator"}
        -   [routing tree]{data-type="index-term"}, [Routing
            Tree](#ch19.xhtml#ix_Alrtmgr2cfgrt){data-type="index:locator"}-[Receivers](#ch19.xhtml#idm45207088948208){data-type="index:locator"}
    -   [configuring]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118525616){data-type="index:locator"},
        [Configuring Alertmanagers in
        Prometheus](#ch18.xhtml#ix_alrtmgrcfg){data-type="index:locator"}-[External
        Labels](#ch18.xhtml#idm45207089832736){data-type="index:locator"}
        -   [external labels]{data-type="index-term"}, [External
            Labels](#ch18.xhtml#idm45207089845568){data-type="index:locator"}
    -   [downloading and installing]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118530144){data-type="index:locator"}
    -   [notification pipeline]{data-type="index-term"}, [Notification
        Pipeline](#ch19.xhtml#idm45207089824464){data-type="index:locator"}
    -   [Prometheus and Alertmanager
        architecture]{data-type="index-term"},
        [Alerting](#ch18.xhtml#idm45207090060176){data-type="index:locator"}
    -   [starting]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207117484528){data-type="index:locator"}
    -   [telling Prometheus which one]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118763536){data-type="index:locator"}
    -   [web interface]{data-type="index-term"}, [Alertmanager Web
        Interface](#ch19.xhtml#ix_Alrtmgr2web){data-type="index:locator"}-[Alertmanager
        Web
        Interface](#ch19.xhtml#idm45207087887456){data-type="index:locator"}
-   [alertmanagers field]{data-type="index-term"}, [Configuring
    Alertmanagers in
    Prometheus](#ch18.xhtml#idm45207089858384){data-type="index:locator"}
-   [alertname labels]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090033680){data-type="index:locator"}
-   [alerts]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119276336){data-type="index:locator"}
    -   [defining multiple alerts with different thresholds and
        labels]{data-type="index-term"}, [Alert
        Labels](#ch18.xhtml#idm45207089947632){data-type="index:locator"}
    -   [firing alert on Alerts page]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207113093696){data-type="index:locator"}
    -   [good alerts]{data-type="index-term"}, [What Are Good
        Alerts?](#ch18.xhtml#idm45207089886368){data-type="index:locator"}
    -   [owners for]{data-type="index-term"}, [Alert
        Labels](#ch18.xhtml#idm45207089943568){data-type="index:locator"},
        [Routing
        Tree](#ch19.xhtml#idm45207089558512){data-type="index:locator"}
-   [Alerts list]{data-type="index-term"}, [Notification
    templates](#ch19.xhtml#idm45207088234112){data-type="index:locator"}
-   [ALERTS metric]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090031920){data-type="index:locator"}
-   [ALERTS_FOR_STATE metric]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090031184){data-type="index:locator"}
-   [alert_relabel_configs]{data-type="index-term"}, [Configuring
    Alertmanagers in
    Prometheus](#ch18.xhtml#idm45207089852464){data-type="index:locator"},
    [Planning for
    Failure](#ch21.xhtml#idm45207087210272){data-type="index:locator"}
-   [aliasing]{data-type="index-term"}, [Time
    Controls](#ch06.xhtml#idm45207100906256){data-type="index:locator"}
-   [aligned data]{data-type="index-term"}, [Aligned
    data](#ch13.xhtml#idm45207091804576){data-type="index:locator"}
-   [Amazon EC2]{data-type="index-term"} ([see]{gentext="see"} EC2)
-   [and operator]{data-type="index-term"}, [and
    operator](#ch15.xhtml#idm45207091053600){data-type="index:locator"},
    [Alerting
    Rules](#ch18.xhtml#idm45207090014096){data-type="index:locator"},
    [Annotations and
    Templates](#ch18.xhtml#idm45207089921632){data-type="index:locator"}
-   [annotations (alert)]{data-type="index-term"}, [Annotations and
    Templates](#ch18.xhtml#ix_annalrt){data-type="index:locator"}-[Annotations
    and
    Templates](#ch18.xhtml#idm45207089890720){data-type="index:locator"}
-   [Ansible]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100519744){data-type="index:locator"},
    [Configuration
    Management](#ch21.xhtml#idm45207087282720){data-type="index:locator"}
    -   [using its templating to create targets for Node
        Exporter]{data-type="index-term"},
        [Static](#ch08.xhtml#idm45207100479840){data-type="index:locator"}
-   [APIs]{data-type="index-term"}, [HTTP
    API](#ch13.xhtml#idm45207092469312){data-type="index:locator"}
    -   ([see also]{gentext="see"} HTTP API)
    -   [rules for]{data-type="index-term"}, [Rules for
        APIs](#ch17.xhtml#idm45207090158592){data-type="index:locator"}
-   [application logs]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207118500928){data-type="index:locator"}
-   [applications, metric names coming from]{data-type="index-term"},
    [Library](#ch03.xhtml#idm45207116420624){data-type="index:locator"}
-   [architecture of Prometheus]{data-type="index-term"}, [Prometheus
    Architecture](#ch01.xhtml#ix_archPr){data-type="index:locator"}-[Long-Term
    Storage](#ch01.xhtml#idm45207118952896){data-type="index:locator"}
-   [arithmetic mean]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091516192){data-type="index:locator"}
-   [arithmetic operators]{data-type="index-term"}, [Arithmetic
    Operators](#ch15.xhtml#ix_arith){data-type="index:locator"}-[Arithmetic
    Operators](#ch15.xhtml#idm45207091313008){data-type="index:locator"},
    [One-to-One](#ch15.xhtml#idm45207091193456){data-type="index:locator"}
    -   [summary of]{data-type="index-term"}, [Arithmetic
        Operators](#ch15.xhtml#idm45207091331968){data-type="index:locator"}
-   [at (@) modifier]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092489568){data-type="index:locator"},
    [topk and
    bottomk](#ch14.xhtml#idm45207091421824){data-type="index:locator"}
-   [atan2 operator]{data-type="index-term"}, [Trigonometric
    Operator](#ch15.xhtml#idm45207091306128){data-type="index:locator"}
-   [authentication]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#idm45207087233936){data-type="index:locator"}
    -   [enabling Basic Authentication]{data-type="index-term"},
        [Enabling Basic
        Authentication](#ch20.xhtml#ix_authBasic){data-type="index:locator"}
    -   [in HTTP SD]{data-type="index-term"},
        [HTTP](#ch08.xhtml#idm45207100058528){data-type="index:locator"}
    -   [for Kubernetes API servers scrapes]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207097146848){data-type="index:locator"}
    -   [options in scrape config]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#idm45207098247680){data-type="index:locator"}
-   [authorization]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098175392){data-type="index:locator"},
    [Node](#ch09.xhtml#idm45207097316928){data-type="index:locator"},
    [Enabling Basic
    Authentication](#ch20.xhtml#idm45207087693952){data-type="index:locator"}
-   [average load]{data-type="index-term"}, [Loadavg
    Collector](#ch07.xhtml#idm45207100614528){data-type="index:locator"}
-   [averages]{data-type="index-term"}
    -   [attempt to average averages]{data-type="index-term"},
        [avg](#ch14.xhtml#idm45207091497424){data-type="index:locator"},
        [Naming of Recording
        Rules](#ch17.xhtml#idm45207090085712){data-type="index:locator"}
    -   [calculating average event size using
        histograms]{data-type="index-term"},
        [Histogram](#ch13.xhtml#idm45207092665264){data-type="index:locator"}
    -   [calculating with summary]{data-type="index-term"},
        [Summary](#ch13.xhtml#idm45207092706880){data-type="index:locator"}
-   [avg operator]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091515456){data-type="index:locator"}
-   [avg without expression]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100763616){data-type="index:locator"},
    [Gauge](#ch13.xhtml#idm45207092763440){data-type="index:locator"}
-   [avg_over_time]{data-type="index-term"},
    [Enum](#ch05.xhtml#idm45207101406480){data-type="index:locator"},
    [Range
    Vector](#ch13.xhtml#idm45207092558704){data-type="index:locator"},
    [holt_winters](#ch16.xhtml#idm45207090512560){data-type="index:locator"},
    [Aggregation Over
    Time](#ch16.xhtml#idm45207090494656){data-type="index:locator"}
    -   [range vectors as input]{data-type="index-term"},
        [Functions](#ch16.xhtml#idm45207090995264){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### B

-   [backend-ticket receiver]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089280624){data-type="index:locator"}
-   [base units]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207119385648){data-type="index:locator"},
    [Units](#ch03.xhtml#idm45207116457600){data-type="index:locator"}
    -   [quantiles and percentiles]{data-type="index-term"}, [The
        Histogram](#ch03.xhtml#idm45207119103712){data-type="index:locator"}
    -   [seconds as base unit for time]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113695328){data-type="index:locator"}
-   [Basic Authentication, enabling]{data-type="index-term"}, [Enabling
    Basic
    Authentication](#ch20.xhtml#ix_BscAuth){data-type="index:locator"}
-   [basic_auth]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098243568){data-type="index:locator"}
-   [batch jobs]{data-type="index-term"}, [Service
    instrumentation](#ch03.xhtml#idm45207116511680){data-type="index:locator"},
    [Pushgateway](#ch04.xhtml#idm45207102610992){data-type="index:locator"}
    -   [alerting on not succeeding recently]{data-type="index-term"},
        [Alerting
        Rules](#ch18.xhtml#idm45207090009648){data-type="index:locator"}
    -   [idempotency for]{data-type="index-term"}, [Service
        instrumentation](#ch03.xhtml#idm45207116507104){data-type="index:locator"}
    -   [recording when Cassandra backups
        completed]{data-type="index-term"}, [Textfile
        Collector](#ch07.xhtml#idm45207100578720){data-type="index:locator"}
    -   [service-level, Pushgateway metrics cache
        for]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102606080){data-type="index:locator"}
-   [bcrypt cost]{data-type="index-term"}, [Enabling Basic
    Authentication](#ch20.xhtml#idm45207087686048){data-type="index:locator"}
-   [billing]{data-type="index-term"}, [What Prometheus Is
    Not](#ch01.xhtml#idm45207113439824){data-type="index:locator"},
    [labelmap](#ch08.xhtml#idm45207098700928){data-type="index:locator"}
-   [binary operators]{data-type="index-term"}, [Binary
    Operators](#ch15.xhtml#ix_binop){data-type="index:locator"}-[Operator
    Precedence](#ch15.xhtml#idm45207091007184){data-type="index:locator"}
    -   [operator precedence]{data-type="index-term"}, [Operator
        Precedence](#ch15.xhtml#idm45207091028896){data-type="index:locator"}
    -   [vector matching]{data-type="index-term"}, [Vector
        Matching](#ch15.xhtml#ix_binopvec){data-type="index:locator"}-[and
        operator](#ch15.xhtml#idm45207091032592){data-type="index:locator"}
        -   [many-to-many and logical
            operators]{data-type="index-term"}, [Many-to-Many and
            Logical
            Operators](#ch15.xhtml#ix_binopvecmtom){data-type="index:locator"}-[and
            operator](#ch15.xhtml#idm45207091037680){data-type="index:locator"}
        -   [many-to-one and group_left]{data-type="index-term"},
            [Many-to-One and
            group_left](#ch15.xhtml#ix_binopvecmtoone){data-type="index:locator"}-[Many-to-One
            and
            group_left](#ch15.xhtml#idm45207091127232){data-type="index:locator"}
        -   [one-to-one]{data-type="index-term"},
            [One-to-One](#ch15.xhtml#idm45207091228464){data-type="index:locator"}
    -   [working with scalars]{data-type="index-term"}, [Working with
        Scalars](#ch15.xhtml#ix_binopsclr){data-type="index:locator"}-[Vector
        Matching](#ch15.xhtml#idm45207091239872){data-type="index:locator"}
        -   [arithmetic operators]{data-type="index-term"}, [Arithmetic
            Operators](#ch15.xhtml#ix_binopsclrarith){data-type="index:locator"}-[Arithmetic
            Operators](#ch15.xhtml#idm45207091316048){data-type="index:locator"}
        -   [comparison operators]{data-type="index-term"}, [Comparison
            Operators](#ch15.xhtml#ix_binopsclrcmp){data-type="index:locator"}-[bool
            modifier](#ch15.xhtml#idm45207091245184){data-type="index:locator"}
        -   [trigonometric operator atan2]{data-type="index-term"},
            [Trigonometric
            Operator](#ch15.xhtml#idm45207091307360){data-type="index:locator"}
-   [Blackbox exporters]{data-type="index-term"},
    [Blackbox](#ch10.xhtml#ix_blckbx){data-type="index:locator"}-[Prometheus
    Configuration](#ch10.xhtml#idm45207095679568){data-type="index:locator"},
    [Other Monitoring
    Systems](#ch11.xhtml#idm45207095552464){data-type="index:locator"}
    -   [catching filed scrapes or failed
        probes]{data-type="index-term"},
        [for](#ch18.xhtml#idm45207089972736){data-type="index:locator"}
    -   [default registry and]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093386016){data-type="index:locator"}
    -   [DNS name resolution]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096057120){data-type="index:locator"}
    -   [DNS probes]{data-type="index-term"},
        [DNS](#ch10.xhtml#ix_blckbxDNS){data-type="index:locator"}-[DNS](#ch10.xhtml#idm45207095925408){data-type="index:locator"}
    -   [downloading and running]{data-type="index-term"},
        [Blackbox](#ch10.xhtml#idm45207096137344){data-type="index:locator"}
    -   [HTTP probes]{data-type="index-term"},
        [HTTP](#ch10.xhtml#ix_blckbxHTTP){data-type="index:locator"}-[HTTP](#ch10.xhtml#idm45207095950480){data-type="index:locator"}
    -   [ICMP probes]{data-type="index-term"},
        [ICMP](#ch10.xhtml#ix_blckbxICMP){data-type="index:locator"}-[ICMP](#ch10.xhtml#idm45207096026480){data-type="index:locator"}
    -   [Prometheus configuration]{data-type="index-term"}, [Prometheus
        Configuration](#ch10.xhtml#idm45207095918416){data-type="index:locator"}
    -   [TCP probes]{data-type="index-term"},
        [TCP](#ch10.xhtml#ix_blckbxTCP){data-type="index:locator"}-[TCP](#ch10.xhtml#idm45207095987264){data-type="index:locator"}
-   [bool modifier]{data-type="index-term"}, [bool
    modifier](#ch15.xhtml#idm45207091271776){data-type="index:locator"}
    -   [using to compare scalars]{data-type="index-term"}, [bool
        modifier](#ch15.xhtml#idm45207091251056){data-type="index:locator"}
-   [boolean values]{data-type="index-term"},
    [Enum](#ch05.xhtml#idm45207101412848){data-type="index:locator"}
-   [bottomk operator]{data-type="index-term"}, [min and
    max](#ch14.xhtml#idm45207091453584){data-type="index:locator"},
    [topk and
    bottomk](#ch14.xhtml#idm45207091428464){data-type="index:locator"}
-   [bridges]{data-type="index-term"},
    [Bridges](#ch04.xhtml#idm45207102351312){data-type="index:locator"}
-   [buckets (in histograms)]{data-type="index-term"},
    [Buckets](#ch03.xhtml#idm45207118977872){data-type="index:locator"},
    [How Much Should I
    Instrument?](#ch03.xhtml#idm45207116483120){data-type="index:locator"},
    [Histogram](#ch13.xhtml#idm45207092685312){data-type="index:locator"}
    -   [cumulative histograms]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115783968){data-type="index:locator"}
    -   [dropping to reduce cardinality]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207097915872){data-type="index:locator"}
    -   [using rate before sum on]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091556336){data-type="index:locator"}
-   [by clause]{data-type="index-term"},
    [by](#ch14.xhtml#idm45207091611040){data-type="index:locator"}
    -   [count by]{data-type="index-term"}, [Unique label
        values](#ch14.xhtml#idm45207091523920){data-type="index:locator"}
    -   [group by]{data-type="index-term"},
        [group](#ch14.xhtml#idm45207091487504){data-type="index:locator"}
    -   [sum by]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101431344){data-type="index:locator"}
    -   [versus without clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091599616){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### C

-   [caches]{data-type="index-term"},
    [Tracing](#ch01.xhtml#idm45207119672928){data-type="index:locator"},
    [Metrics](#ch01.xhtml#idm45207118489264){data-type="index:locator"},
    [How Much Should I
    Instrument?](#ch03.xhtml#idm45207116479776){data-type="index:locator"}
    -   [memory usage of]{data-type="index-term"}, [The
        Gauge](#ch03.xhtml#idm45207117402800){data-type="index:locator"}
    -   [metrics for cache overall and cache
        misses]{data-type="index-term"}, [Library
        instrumentation](#ch03.xhtml#idm45207116500736){data-type="index:locator"}
    -   [tracking size or number of files in]{data-type="index-term"},
        [Callbacks](#ch03.xhtml#idm45207113581392){data-type="index:locator"}
-   [cAdvisor]{data-type="index-term"},
    [cAdvisor](#ch09.xhtml#ix_cAd){data-type="index:locator"}-[Kubernetes](#ch09.xhtml#idm45207097519120){data-type="index:locator"}
    -   [container CPU metrics]{data-type="index-term"},
        [CPU](#ch09.xhtml#idm45207097619856){data-type="index:locator"}
    -   [container labels]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097560512){data-type="index:locator"}
    -   [container memory metrics]{data-type="index-term"},
        [Memory](#ch09.xhtml#idm45207097578176){data-type="index:locator"}
    -   [embedded in Kubelet]{data-type="index-term"},
        [Node](#ch09.xhtml#idm45207097305344){data-type="index:locator"}
-   [callbacks]{data-type="index-term"},
    [Callbacks](#ch03.xhtml#idm45207113579504){data-type="index:locator"}
-   [cardinality]{data-type="index-term"},
    [Cardinality](#ch05.xhtml#idm45207101027648){data-type="index:locator"}
    -   [performance problems with]{data-type="index-term"}, [Growing
        Prometheus](#ch21.xhtml#idm45207087488144){data-type="index:locator"}
    -   [reducing with recording rules]{data-type="index-term"},
        [Reducing
        Cardinality](#ch17.xhtml#idm45207090202048){data-type="index:locator"},
        [How Not to Use
        Rules](#ch17.xhtml#idm45207090144576){data-type="index:locator"}
-   [case, changing for label values]{data-type="index-term"},
    [Case](#ch08.xhtml#idm45207098654944){data-type="index:locator"}
-   [Cassandra]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#idm45207100577648){data-type="index:locator"}
-   [ca_file]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097381776){data-type="index:locator"}
-   [ceil function]{data-type="index-term"}, [ceil and
    floor](#ch16.xhtml#idm45207090903488){data-type="index:locator"}
-   [certificate authority]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097381072){data-type="index:locator"}
    -   [self-signed, creating using OpenSSL]{data-type="index-term"},
        [Enabling
        TLS](#ch20.xhtml#idm45207087866384){data-type="index:locator"}
-   [certificates, TLS]{data-type="index-term"}, [Enabling
    TLS](#ch20.xhtml#idm45207087867216){data-type="index:locator"}
-   [cgroups]{data-type="index-term"}
    -   [hierarchy of]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097561584){data-type="index:locator"}
    -   [metrics about]{data-type="index-term"} ([see]{gentext="see"}
        cAdvisor)
-   [change of base function]{data-type="index-term"}, [ln, log2, and
    log10](#ch16.xhtml#idm45207090932624){data-type="index:locator"}
-   [change, managing]{data-type="index-term"}, [Managing
    Change](#ch21.xhtml#idm45207087066064){data-type="index:locator"}
-   [changes function]{data-type="index-term"},
    [changes](#ch16.xhtml#idm45207090569616){data-type="index:locator"}
-   [characters (in metric names)]{data-type="index-term"},
    [Characters](#ch03.xhtml#idm45207116470512){data-type="index:locator"}
-   [Chebyshev\'s inequality]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091470928){data-type="index:locator"}
-   [check config (promtool)]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090310432){data-type="index:locator"}
-   [check metrics]{data-type="index-term"}, [check
    metrics](#ch04.xhtml#idm45207102119856){data-type="index:locator"}
-   [check rules]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090311104){data-type="index:locator"}
-   [check-config (amtool)]{data-type="index-term"}, [Configuration
    File](#ch19.xhtml#idm45207089794992){data-type="index:locator"}
-   [Chef]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100519072){data-type="index:locator"}
-   [child metrics]{data-type="index-term"},
    [Metric](#ch05.xhtml#idm45207101856928){data-type="index:locator"},
    [Child](#ch05.xhtml#ix_chld){data-type="index:locator"}-[Child](#ch05.xhtml#idm45207101499328){data-type="index:locator"}
    -   [of node_hwmon_sensor_label]{data-type="index-term"},
        [Many-to-One and
        group_left](#ch15.xhtml#idm45207091145696){data-type="index:locator"}
-   [child routes]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089612688){data-type="index:locator"}
-   [chokepoints in your applications]{data-type="index-term"},
    [Planning a
    Rollout](#ch21.xhtml#idm45207087507344){data-type="index:locator"}
-   [clamp function]{data-type="index-term"}, [clamp, clamp_max, and
    clamp_min](#ch16.xhtml#idm45207090884288){data-type="index:locator"}
-   [clamp_max function]{data-type="index-term"}, [clamp, clamp_max, and
    clamp_min](#ch16.xhtml#idm45207090883616){data-type="index:locator"}
-   [clamp_min function]{data-type="index-term"}, [clamp, clamp_max, and
    clamp_min](#ch16.xhtml#idm45207090882944){data-type="index:locator"}
-   [client libraries]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207116099776){data-type="index:locator"},
    [Client
    Libraries](#ch01.xhtml#idm45207113063584){data-type="index:locator"},
    [Instrumentation](#ch03.xhtml#idm45207116018752){data-type="index:locator"},
    [Custom
    Collectors](#ch12.xhtml#idm45207094813616){data-type="index:locator"}
    -   [exposition in]{data-type="index-term"} ([see]{gentext="see"}
        exposition)
    -   [fetching time series using URL
        parameters]{data-type="index-term"}, [Reducing
        Load](#ch21.xhtml#idm45207087100832){data-type="index:locator"}
    -   [metrics related to runtime]{data-type="index-term"},
        [Library](#ch03.xhtml#idm45207116430912){data-type="index:locator"}
    -   [official versus unofficial]{data-type="index-term"}, [Client
        Libraries](#ch01.xhtml#idm45207113060832){data-type="index:locator"}
    -   [registration of metrics with]{data-type="index-term"}, [The
        Counter](#ch03.xhtml#idm45207117181024){data-type="index:locator"}
-   [Cloud Native Computing Foundation (CNCF)]{data-type="index-term"},
    [What Is
    Prometheus?](#ch01.xhtml#idm45207116100688){data-type="index:locator"},
    [Kubernetes](#ch09.xhtml#idm45207097514944){data-type="index:locator"},
    [Long-Term
    Storage](#ch21.xhtml#idm45207087323072){data-type="index:locator"}
-   [Cloudwatch Exporter]{data-type="index-term"},
    [Guidelines](#ch12.xhtml#idm45207092816592){data-type="index:locator"}
-   [cluster labels]{data-type="index-term"},
    [without](#ch14.xhtml#idm45207091633152){data-type="index:locator"}
-   [clustered storage system]{data-type="index-term"}, [Long-Term
    Storage](#ch01.xhtml#idm45207118955488){data-type="index:locator"},
    [Long-Term
    Storage](#ch21.xhtml#idm45207087334576){data-type="index:locator"}
-   [clustering]{data-type="index-term"},
    [Storage](#ch01.xhtml#idm45207119361680){data-type="index:locator"}
-   [clustering (Alertmanager)]{data-type="index-term"}, [Alertmanager
    Clustering](#ch21.xhtml#idm45207087192528){data-type="index:locator"}
-   [Collect method]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094265680){data-type="index:locator"}
-   [collectd]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095578352){data-type="index:locator"}
-   [\--collector.diskstats.device-exclude
    flag]{data-type="index-term"}, [Diskstats
    Collector](#ch07.xhtml#idm45207100724448){data-type="index:locator"}
-   [\--collector.textfie.directory flag]{data-type="index-term"},
    [Using the Textfile
    Collector](#ch07.xhtml#idm45207100569968){data-type="index:locator"}
-   [CollectorRegistry.collect]{data-type="index-term"},
    [Bridges](#ch04.xhtml#idm45207102308128){data-type="index:locator"}
-   [CollectorRegistry.metricFamilySamples]{data-type="index-term"},
    [Bridges](#ch04.xhtml#idm45207102309584){data-type="index:locator"}
-   [collectors]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100767872){data-type="index:locator"}
    -   ([see also]{gentext="see"} Node Exporter)
    -   [custom]{data-type="index-term"},
        [Exporters](#ch01.xhtml#idm45207118479040){data-type="index:locator"},
        [Enum](#ch05.xhtml#idm45207101400432){data-type="index:locator"},
        [Custom
        Collectors](#ch12.xhtml#ix_collcus){data-type="index:locator"}-[Guidelines](#ch12.xhtml#idm45207092873072){data-type="index:locator"}
        -   [labels for metrics]{data-type="index-term"},
            [Labels](#ch12.xhtml#idm45207093209344){data-type="index:locator"}
-   [compaction duration, time series database]{data-type="index-term"},
    [Histogram](#ch13.xhtml#idm45207092684640){data-type="index:locator"}
-   [comparison operators]{data-type="index-term"}, [Comparison
    Operators](#ch15.xhtml#ix_cmpops){data-type="index:locator"}-[bool
    modifier](#ch15.xhtml#idm45207091243696){data-type="index:locator"},
    [One-to-One](#ch15.xhtml#idm45207091192720){data-type="index:locator"}
    -   [bool modifier and]{data-type="index-term"}, [bool
        modifier](#ch15.xhtml#idm45207091271104){data-type="index:locator"}
-   [configuration]{data-type="index-term"}
    -   [asking Prometheus to reload]{data-type="index-term"}, [Using
        Recording
        Rules](#ch17.xhtml#idm45207090392208){data-type="index:locator"}
    -   [checking with promtool check rules]{data-type="index-term"},
        [Using Recording
        Rules](#ch17.xhtml#idm45207090313776){data-type="index:locator"}
-   [configuration files, use of YAML]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207113467136){data-type="index:locator"}
-   [configuration management]{data-type="index-term"}, [Configuration
    Management](#ch21.xhtml#idm45207087285360){data-type="index:locator"}
-   [configuration management systems]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100520432){data-type="index:locator"}
-   [connection refused error]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118789936){data-type="index:locator"}
-   [console templates]{data-type="index-term"}, [Dashboarding with
    Grafana](#ch06.xhtml#idm45207100997008){data-type="index:locator"}
-   [ConstMetrics]{data-type="index-term"},
    [Exporters](#ch01.xhtml#idm45207118479744){data-type="index:locator"}
-   [Consul]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100523440){data-type="index:locator"},
    [Service Discovery
    Mechanisms](#ch08.xhtml#idm45207100497232){data-type="index:locator"}
    -   [keeping only Consul services with prod
        tag]{data-type="index-term"},
        [Lists](#ch08.xhtml#idm45207098507008){data-type="index:locator"}
    -   [monitoring Consul]{data-type="index-term"},
        [Consul](#ch08.xhtml#idm45207099913136){data-type="index:locator"}
    -   [production tags for production
        services]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099448976){data-type="index:locator"}
    -   [service discovery]{data-type="index-term"},
        [Consul](#ch08.xhtml#idm45207100003696){data-type="index:locator"}
        -   [using replace to relabel team
            label]{data-type="index-term"},
            [replace](#ch08.xhtml#idm45207098982752){data-type="index:locator"}
    -   [writing Consul Telemetry exporter]{data-type="index-term"},
        [Consul
        Telemetry](#ch12.xhtml#ix_Conswrexp){data-type="index:locator"}-[Consul
        Telemetry](#ch12.xhtml#idm45207094818880){data-type="index:locator"}
-   [consul_up]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094264816){data-type="index:locator"}
-   [container orchestrators]{data-type="index-term"},
    [Kubernetes](#ch09.xhtml#idm45207097514304){data-type="index:locator"}
    -   ([see also]{gentext="see"} Kubernetes)
-   [containers]{data-type="index-term"}, [Containers and
    Kubernetes](#ch09.xhtml#ix_cntnr){data-type="index:locator"}-[Kubernetes](#ch09.xhtml#idm45207097516960){data-type="index:locator"}
    -   [cAdvisor]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#ix_cntnrcAdv){data-type="index:locator"}-[Kubernetes](#ch09.xhtml#idm45207097518176){data-type="index:locator"}
    -   [CPU metrics]{data-type="index-term"},
        [CPU](#ch09.xhtml#idm45207097616720){data-type="index:locator"}
    -   [labels]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097562560){data-type="index:locator"}
    -   [memory usage metrics]{data-type="index-term"},
        [Memory](#ch09.xhtml#idm45207097603648){data-type="index:locator"}
-   [context deadline exceeded error]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118784576){data-type="index:locator"}
-   [context of events]{data-type="index-term"}, [Categories of
    Monitoring](#ch01.xhtml#idm45207119373808){data-type="index:locator"}
-   [context switches]{data-type="index-term"}, [Stat
    Collector](#ch07.xhtml#idm45207100641280){data-type="index:locator"}
-   [continue setting (routing)]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089320528){data-type="index:locator"},
    [Receivers](#ch19.xhtml#idm45207088593696){data-type="index:locator"}
-   [continuous profiling]{data-type="index-term"},
    [Profiling](#ch01.xhtml#idm45207119679536){data-type="index:locator"}
-   [count]{data-type="index-term"},
    [Info](#ch05.xhtml#idm45207101046896){data-type="index:locator"},
    [Cardinality](#ch05.xhtml#idm45207101022112){data-type="index:locator"}
    -   [about]{data-type="index-term"},
        [count](#ch14.xhtml#idm45207091545728){data-type="index:locator"}
    -   [by clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091590848){data-type="index:locator"}
    -   [count by]{data-type="index-term"}, [Uname
        Collector](#ch07.xhtml#idm45207100628704){data-type="index:locator"},
        [Unique label
        values](#ch14.xhtml#idm45207091522944){data-type="index:locator"}
    -   [counting unique label values]{data-type="index-term"}, [Unique
        label
        values](#ch14.xhtml#idm45207091536208){data-type="index:locator"}
    -   [dividing by sum]{data-type="index-term"},
        [Summary](#ch13.xhtml#idm45207092705728){data-type="index:locator"}
    -   [histogram metric]{data-type="index-term"},
        [Histogram](#ch13.xhtml#idm45207092666320){data-type="index:locator"}
    -   [using count_values with]{data-type="index-term"},
        [count_values](#ch14.xhtml#idm45207091377664){data-type="index:locator"}
    -   [using rate before sum on]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091557296){data-type="index:locator"}
-   [counters]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207113337792){data-type="index:locator"},
    [The
    Counter](#ch03.xhtml#ix_cntr3){data-type="index:locator"}-[Counting
    Size](#ch03.xhtml#idm45207117410384){data-type="index:locator"},
    [Counters](#ch16.xhtml#ix_counts){data-type="index:locator"}-[resets](#ch16.xhtml#idm45207090579504){data-type="index:locator"}
    -   [aggregating]{data-type="index-term"},
        [Counter](#ch13.xhtml#idm45207092755488){data-type="index:locator"}
    -   [attempting to increase by negative
        number]{data-type="index-term"}, [Counting
        Size](#ch03.xhtml#idm45207117413312){data-type="index:locator"}
    -   [Consul]{data-type="index-term"}, [Consul
        Telemetry](#ch12.xhtml#idm45207095339728){data-type="index:locator"}
    -   [container CPU metrics]{data-type="index-term"},
        [CPU](#ch09.xhtml#idm45207097610000){data-type="index:locator"}
    -   [counting exceptions]{data-type="index-term"}, [Counting
        Exceptions](#ch03.xhtml#idm45207117162128){data-type="index:locator"}
    -   [counting size]{data-type="index-term"}, [Counting
        Size](#ch03.xhtml#idm45207113161360){data-type="index:locator"}
    -   [increase function]{data-type="index-term"},
        [increase](#ch16.xhtml#idm45207090618272){data-type="index:locator"}
    -   [irate function]{data-type="index-term"},
        [irate](#ch16.xhtml#idm45207090605568){data-type="index:locator"}
    -   [multiprocess mode and]{data-type="index-term"}, [Multiprocess
        with
        Gunicorn](#ch04.xhtml#idm45207114244912){data-type="index:locator"}
    -   [Node Exporter diskstat metrics]{data-type="index-term"},
        [Diskstats
        Collector](#ch07.xhtml#idm45207100721248){data-type="index:locator"}
    -   [processing in custom collector]{data-type="index-term"},
        [Custom
        Collectors](#ch12.xhtml#idm45207093825968){data-type="index:locator"}
    -   [rate function]{data-type="index-term"},
        [rate](#ch16.xhtml#idm45207090641712){data-type="index:locator"}
    -   [resets function]{data-type="index-term"},
        [resets](#ch16.xhtml#idm45207090589536){data-type="index:locator"}
    -   [text exposition format for]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102170480){data-type="index:locator"}
    -   [unit testing in Python]{data-type="index-term"}, [Unit Testing
        Instrumentation](#ch03.xhtml#idm45207115711392){data-type="index:locator"}
    -   [using rate before sum with]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091559216){data-type="index:locator"}
-   [CounterValue]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094064896){data-type="index:locator"}
-   [count_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090495328){data-type="index:locator"}
-   [count_values operator]{data-type="index-term"},
    [count_values](#ch14.xhtml#idm45207091392704){data-type="index:locator"}
    -   [use with count]{data-type="index-term"},
        [count_values](#ch14.xhtml#idm45207091376688){data-type="index:locator"}
-   [cpu collector]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100769488){data-type="index:locator"}
-   [cpu labels]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100766928){data-type="index:locator"}
-   [CPUs]{data-type="index-term"}
    -   [calculating 90th percentile of system mode CPU
        usage]{data-type="index-term"},
        [quantile](#ch14.xhtml#idm45207091412016){data-type="index:locator"}
    -   [counting number in each machine]{data-type="index-term"},
        [Unique label
        values](#ch14.xhtml#idm45207091534256){data-type="index:locator"}
    -   [metric for container CPUs]{data-type="index-term"},
        [CPU](#ch09.xhtml#idm45207097618848){data-type="index:locator"}
    -   [PDUs and]{data-type="index-term"}, [Service
        Discovery](#ch08.xhtml#idm45207100508928){data-type="index:locator"}
    -   [requirements for Prometheus]{data-type="index-term"},
        [Hardware](#ch21.xhtml#idm45207087295664){data-type="index:locator"}
-   [CPython]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207118325008){data-type="index:locator"}
-   [credentials_file]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098173888){data-type="index:locator"},
    [Node](#ch09.xhtml#idm45207097317536){data-type="index:locator"},
    [Endpointslice](#ch09.xhtml#idm45207097145536){data-type="index:locator"}
-   [cronjobs]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#idm45207100584208){data-type="index:locator"},
    [File](#ch08.xhtml#idm45207100126928){data-type="index:locator"}
    -   [causing CPU usage to spike]{data-type="index-term"}, [What Are
        Good
        Alerts?](#ch18.xhtml#idm45207089878928){data-type="index:locator"}
    -   [outputting to textfile collector]{data-type="index-term"},
        [Using the Textfile
        Collector](#ch07.xhtml#idm45207100547744){data-type="index:locator"}
-   [cross-monitoring]{data-type="index-term"}, [Meta- and
    Cross-Monitoring](#ch21.xhtml#idm45207087171296){data-type="index:locator"}
-   [cube root]{data-type="index-term"},
    [sqrt](#ch16.xhtml#idm45207090909056){data-type="index:locator"}
-   [cumulative histograms]{data-type="index-term"},
    [Buckets](#ch03.xhtml#idm45207115785616){data-type="index:locator"}
-   [cURL utility]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207114235472){data-type="index:locator"}
-   [custom collectors]{data-type="index-term"} ([see]{gentext="see"}
    collectors)
-   [custom registries]{data-type="index-term"}, [The
    Counter](#ch03.xhtml#idm45207117177568){data-type="index:locator"},
    [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207118272400){data-type="index:locator"},
    [Pushgateway](#ch04.xhtml#idm45207102521120){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### D

-   [dashboard annotation]{data-type="index-term"}, [Notification
    templates](#ch19.xhtml#idm45207088129168){data-type="index:locator"}
-   [dashboards]{data-type="index-term"},
    [Dashboards](#ch01.xhtml#idm45207119358704){data-type="index:locator"},
    [Dashboarding with
    Grafana](#ch06.xhtml#idm45207101008240){data-type="index:locator"},
    [Planning a
    Rollout](#ch21.xhtml#idm45207087509072){data-type="index:locator"}
    -   ([see also]{gentext="see"} Grafana)
    -   [avoiding wall of graphs]{data-type="index-term"}, [Avoiding the
        Wall of
        Graphs](#ch06.xhtml#idm45207100952960){data-type="index:locator"}
    -   [creating using Grafana template
        variables]{data-type="index-term"}, [Template
        Variables](#ch06.xhtml#idm45207100844064){data-type="index:locator"}
    -   [with graph and Stat panels in Grafana]{data-type="index-term"},
        [Stat
        Panel](#ch06.xhtml#idm45207100890720){data-type="index:locator"}
    -   [making faster]{data-type="index-term"}, [Reducing
        Cardinality](#ch17.xhtml#idm45207090203024){data-type="index:locator"}
    -   [new Grafana dashboard]{data-type="index-term"}, [Dashboards and
        Panels](#ch06.xhtml#idm45207100961072){data-type="index:locator"}
    -   [Promdash and console templates]{data-type="index-term"},
        [Dashboarding with
        Grafana](#ch06.xhtml#idm45207101002048){data-type="index:locator"}
    -   [with Stat panels and Table panel]{data-type="index-term"},
        [Table
        Panel](#ch06.xhtml#idm45207100868000){data-type="index:locator"}
-   [data sources]{data-type="index-term"}, [Data
    Source](#ch06.xhtml#idm45207100970896){data-type="index:locator"}
-   [date functions]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090813632){data-type="index:locator"}
-   [days]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092549040){data-type="index:locator"}
-   [days_in_month function]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090810240){data-type="index:locator"}
-   [day_of_month]{data-type="index-term"}, [minute, hour, day_of_week,
    day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090775728){data-type="index:locator"}
-   [day_of_week function]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090811584){data-type="index:locator"}
-   [day_of_year function]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090810912){data-type="index:locator"}
-   [debug logs]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207118498816){data-type="index:locator"}
    -   [scrape errors on]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118782848){data-type="index:locator"}
-   [debugging]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#idm45207119692640){data-type="index:locator"}
-   [dec method]{data-type="index-term"}, [Using
    Gauges](#ch03.xhtml#idm45207117324816){data-type="index:locator"}
-   [default namespace]{data-type="index-term"},
    [Endpointslice](#ch09.xhtml#idm45207096928224){data-type="index:locator"}
-   [default registry]{data-type="index-term"},
    [Exposition](#ch04.xhtml#idm45207116405248){data-type="index:locator"}
    -   ([see also]{gentext="see"} registry)
-   [default route]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089682400){data-type="index:locator"}
    -   [error to use matchers on]{data-type="index-term"}, [Routing
        Tree](#ch19.xhtml#idm45207089561600){data-type="index:locator"}
-   [DefaultExports.initialize]{data-type="index-term"},
    [HTTPServer](#ch04.xhtml#idm45207103214704){data-type="index:locator"}
-   [degrees and radians, converting between]{data-type="index-term"},
    [Trigonometric
    Functions](#ch16.xhtml#idm45207090847936){data-type="index:locator"}
-   [delete_from_gateway]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102510640){data-type="index:locator"}
-   [delta function]{data-type="index-term"},
    [delta](#ch16.xhtml#idm45207090530528){data-type="index:locator"}
-   [dependencies]{data-type="index-term"}
    -   [Java servlet client library]{data-type="index-term"},
        [Servlet](#ch04.xhtml#idm45207102950352){data-type="index:locator"}
    -   [simpleclient in Java]{data-type="index-term"},
        [HTTPServer](#ch04.xhtml#idm45207103212160){data-type="index:locator"}
-   [deploying Prometheus]{data-type="index-term"}, [Putting It All
    Together](#ch21.xhtml#ix_depPrm){data-type="index:locator"}-[Getting
    Help](#ch21.xhtml#idm45207087050416){data-type="index:locator"}
    -   [federation]{data-type="index-term"}, [Going Global with
        Federation](#ch21.xhtml#ix_depPrmfed){data-type="index:locator"}-[Going
        Global with
        Federation](#ch21.xhtml#idm45207087352528){data-type="index:locator"}
    -   [getting help]{data-type="index-term"}, [Getting
        Help](#ch21.xhtml#idm45207087058512){data-type="index:locator"}
    -   [growing Prometheus]{data-type="index-term"}, [Growing
        Prometheus](#ch21.xhtml#idm45207087495264){data-type="index:locator"}
    -   [long-term storage]{data-type="index-term"}, [Long-Term
        Storage](#ch21.xhtml#ix_depPrmstor){data-type="index:locator"}-[Long-Term
        Storage](#ch21.xhtml#idm45207087318560){data-type="index:locator"}
    -   [managing change]{data-type="index-term"}, [Managing
        Change](#ch21.xhtml#idm45207087065360){data-type="index:locator"}
    -   [managing performance]{data-type="index-term"}, [Managing
        Performance](#ch21.xhtml#ix_depPrmperf){data-type="index:locator"}-[Horizontal
        Sharding](#ch21.xhtml#idm45207087069120){data-type="index:locator"}
        -   [detecting a problem]{data-type="index-term"}, [Detecting a
            Problem](#ch21.xhtml#idm45207087153456){data-type="index:locator"}
        -   [finding expensive targets and
            metrics]{data-type="index-term"}, [Finding Expensive Metrics
            and
            Targets](#ch21.xhtml#idm45207087132176){data-type="index:locator"}
        -   [hashmod relabel action]{data-type="index-term"},
            [Hashmod](#ch21.xhtml#idm45207087123040){data-type="index:locator"}
        -   [horizontal sharding]{data-type="index-term"}, [Horizontal
            Sharding](#ch21.xhtml#idm45207087087568){data-type="index:locator"}
        -   [reducing load]{data-type="index-term"}, [Reducing
            Load](#ch21.xhtml#idm45207087108624){data-type="index:locator"}
    -   [planning a rollout]{data-type="index-term"}, [Planning a
        Rollout](#ch21.xhtml#ix_depPrmpln){data-type="index:locator"}-[Planning
        a
        Rollout](#ch21.xhtml#idm45207087498320){data-type="index:locator"}
    -   [planning for a failure]{data-type="index-term"}, [Planning for
        Failure](#ch21.xhtml#ix_depPrmfail){data-type="index:locator"}-[Planning
        for
        Failure](#ch21.xhtml#idm45207087199104){data-type="index:locator"}
        -   [Alertmanager clustering]{data-type="index-term"},
            [Alertmanager
            Clustering](#ch21.xhtml#idm45207087194800){data-type="index:locator"}
        -   [meta- and cross-monitoring]{data-type="index-term"}, [Meta-
            and
            Cross-Monitoring](#ch21.xhtml#idm45207087177472){data-type="index:locator"}
    -   [running Prometheus]{data-type="index-term"}, [Running
        Prometheus](#ch21.xhtml#ix_depPrmrun){data-type="index:locator"}-[Networks
        and
        Authentication](#ch21.xhtml#idm45207087229776){data-type="index:locator"}
        -   [configuration management]{data-type="index-term"},
            [Configuration
            Management](#ch21.xhtml#idm45207087284592){data-type="index:locator"}
        -   [hardware]{data-type="index-term"},
            [Hardware](#ch21.xhtml#ix_depPrmrunhdw){data-type="index:locator"}-[Hardware](#ch21.xhtml#idm45207087289520){data-type="index:locator"}
        -   [networks and authentication]{data-type="index-term"},
            [Networks and
            Authentication](#ch21.xhtml#ix_depPrmrunntw){data-type="index:locator"}-[Networks
            and
            Authentication](#ch21.xhtml#idm45207087231328){data-type="index:locator"},
            [Networks and
            Authentication](#ch21.xhtml#idm45207087258512){data-type="index:locator"}
-   [deriv function]{data-type="index-term"},
    [Offset](#ch13.xhtml#idm45207092505360){data-type="index:locator"},
    [deriv](#ch16.xhtml#idm45207090555040){data-type="index:locator"}
    -   [using instead of delta]{data-type="index-term"},
        [delta](#ch16.xhtml#idm45207090527152){data-type="index:locator"}
-   [Desc type]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094274736){data-type="index:locator"},
    [Labels](#ch12.xhtml#idm45207093106320){data-type="index:locator"}
-   [Describe method]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094276736){data-type="index:locator"}
-   [destination (notifications)]{data-type="index-term"}, [Notification
    templates](#ch19.xhtml#idm45207088071552){data-type="index:locator"}
    -   [setting based on email_to label]{data-type="index-term"},
        [Notification
        templates](#ch19.xhtml#idm45207088066528){data-type="index:locator"}
-   [device labels]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100743568){data-type="index:locator"},
    [Netdev
    Collector](#ch07.xhtml#idm45207100694240){data-type="index:locator"},
    [Gauge](#ch13.xhtml#idm45207092784176){data-type="index:locator"},
    [Grouping](#ch14.xhtml#idm45207091643824){data-type="index:locator"}
    -   [recording rules and]{data-type="index-term"}, [How Not to Use
        Rules](#ch17.xhtml#idm45207090150368){data-type="index:locator"}
    -   [removing using sum without]{data-type="index-term"},
        [Counter](#ch13.xhtml#idm45207092741072){data-type="index:locator"}
-   [df command]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100750912){data-type="index:locator"}
-   [disk I/O]{data-type="index-term"}, [Running the Node
    Exporter](#ch02.xhtml#idm45207118071872){data-type="index:locator"},
    [Node
    Exporter](#ch07.xhtml#idm45207100796368){data-type="index:locator"},
    [Diskstats
    Collector](#ch07.xhtml#idm45207100727472){data-type="index:locator"},
    [Diskstats
    Collector](#ch07.xhtml#idm45207100702000){data-type="index:locator"}
-   [diskstats collector]{data-type="index-term"}, [Diskstats
    Collector](#ch07.xhtml#idm45207100726064){data-type="index:locator"}
-   [distributed tracing]{data-type="index-term"},
    [Tracing](#ch01.xhtml#idm45207114297312){data-type="index:locator"}
-   [DNS]{data-type="index-term"}
    -   [kube-dns]{data-type="index-term"},
        [Pod](#ch09.xhtml#idm45207096783648){data-type="index:locator"}
    -   [name resolution used by Prometheus and Blackbox
        exporters]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096058176){data-type="index:locator"}
    -   [probing with Blackbox exporter]{data-type="index-term"},
        [DNS](#ch10.xhtml#ix_DNSprb){data-type="index:locator"}-[DNS](#ch10.xhtml#idm45207095924160){data-type="index:locator"}
-   [Docker]{data-type="index-term"}
    -   [container labels]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097555840){data-type="index:locator"}
    -   [id labels from]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#idm45207097650656){data-type="index:locator"}
    -   [installing Grafana with]{data-type="index-term"},
        [Installation](#ch06.xhtml#idm45207100989104){data-type="index:locator"}
    -   [running cAdvisor with]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#idm45207097663392){data-type="index:locator"}
    -   [running Node Exporter within]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100786960){data-type="index:locator"}
-   [dotted string notation]{data-type="index-term"},
    [StatsD](#ch11.xhtml#idm45207095439680){data-type="index:locator"}
-   [DOWN state, alerting on]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118790608){data-type="index:locator"}
-   [drop (relabel action)]{data-type="index-term"}, [Choosing What to
    Scrape](#ch08.xhtml#idm45207099537104){data-type="index:locator"},
    [metric_relabel_configs](#ch08.xhtml#idm45207098008768){data-type="index:locator"},
    [Hashmod](#ch21.xhtml#idm45207087117664){data-type="index:locator"}
-   [Dropwizard metrics]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095547248){data-type="index:locator"}
    -   [counters]{data-type="index-term"}, [Consul
        Telemetry](#ch12.xhtml#idm45207095145664){data-type="index:locator"}
-   [durations]{data-type="index-term"}
    -   [instrumenting, not excluding failures]{data-type="index-term"},
        [Service
        instrumentation](#ch03.xhtml#idm45207116515360){data-type="index:locator"}
    -   [metrics on]{data-type="index-term"}, [Service
        instrumentation](#ch03.xhtml#idm45207115645552){data-type="index:locator"}
        -   [probe_duration_seconds]{data-type="index-term"},
            [ICMP](#ch10.xhtml#idm45207096062704){data-type="index:locator"}
    -   [not adding metric for duration of every
        function]{data-type="index-term"}, [How Much Should I
        Instrument?](#ch03.xhtml#idm45207116485552){data-type="index:locator"}
    -   [in Prometheus as used in PromQL]{data-type="index-term"},
        [Range
        Vector](#ch13.xhtml#idm45207092553984){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### E

-   [eBPF (enhanced Berkeley Packet Filters)]{data-type="index-term"},
    [Profiling](#ch01.xhtml#idm45207119681840){data-type="index:locator"}
-   [EC2 (Elastic Compute Cloud)]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100522096){data-type="index:locator"},
    [EC2](#ch08.xhtml#ix_EC2){data-type="index:locator"}-[EC2](#ch08.xhtml#idm45207099845904){data-type="index:locator"},
    [Lists](#ch08.xhtml#idm45207098508960){data-type="index:locator"}
    -   [relabeling tags using labelmap action]{data-type="index-term"},
        [labelmap](#ch08.xhtml#idm45207098784032){data-type="index:locator"}
    -   [team tags]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099449856){data-type="index:locator"}
-   [Elasticsearch]{data-type="index-term"},
    [Parsers](#ch04.xhtml#idm45207102218256){data-type="index:locator"}
-   [email alerts]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118519472){data-type="index:locator"},
    [Configuration
    File](#ch19.xhtml#idm45207089789616){data-type="index:locator"}
-   [email_to labels]{data-type="index-term"}, [Notification
    templates](#ch19.xhtml#idm45207088068896){data-type="index:locator"}
-   [end]{data-type="index-term"},
    [query_range](#ch13.xhtml#idm45207091958384){data-type="index:locator"},
    [Aligned
    data](#ch13.xhtml#idm45207091665440){data-type="index:locator"}
-   [end function]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092479920){data-type="index:locator"}
-   [endpoints (Kubernetes)]{data-type="index-term"}, [Service
    Discovery](#ch09.xhtml#idm45207097429456){data-type="index:locator"},
    [Endpointslice](#ch09.xhtml#idm45207097258256){data-type="index:locator"}
-   [endpointslice service discovery]{data-type="index-term"},
    [Endpointslice](#ch09.xhtml#idm45207097261328){data-type="index:locator"},
    [Pod](#ch09.xhtml#idm45207096787328){data-type="index:locator"}
-   [enums]{data-type="index-term"},
    [Enum](#ch05.xhtml#idm45207101420656){data-type="index:locator"}
-   [env labels]{data-type="index-term"},
    [without](#ch14.xhtml#idm45207091633888){data-type="index:locator"},
    [Alert
    Labels](#ch18.xhtml#idm45207089953584){data-type="index:locator"},
    [Grouping](#ch19.xhtml#idm45207089077840){data-type="index:locator"}
    -   [in Slack]{data-type="index-term"}, [Notification
        templates](#ch19.xhtml#idm45207088338736){data-type="index:locator"}
-   [epsilon]{data-type="index-term"}, [Comparison
    Operators](#ch15.xhtml#idm45207091278320){data-type="index:locator"}
-   [equality matcher (=)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092629392){data-type="index:locator"}
-   [escaping characters in exposition format]{data-type="index-term"},
    [Escaping](#ch04.xhtml#idm45207102138832){data-type="index:locator"}
-   [evaluation_interval]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090277328){data-type="index:locator"},
    [Reducing
    Load](#ch21.xhtml#idm45207087098496){data-type="index:locator"}
    -   [changing to an hour, reasons not to]{data-type="index-term"},
        [How Not to Use
        Rules](#ch17.xhtml#idm45207090139456){data-type="index:locator"}
-   [events]{data-type="index-term"}, [Categories of
    Monitoring](#ch01.xhtml#idm45207119581568){data-type="index:locator"}
    -   [counters tracking]{data-type="index-term"}, [Using the
        Expression
        Browser](#ch02.xhtml#idm45207113335456){data-type="index:locator"}
-   [exceptions, counting]{data-type="index-term"}, [Counting
    Exceptions](#ch03.xhtml#idm45207117162800){data-type="index:locator"}
-   [exp function]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091508624){data-type="index:locator"},
    [exp](#ch16.xhtml#idm45207090927008){data-type="index:locator"}
-   [exponent operator (\^)]{data-type="index-term"},
    [sqrt](#ch16.xhtml#idm45207090912048){data-type="index:locator"}
-   [exporters]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207116098032){data-type="index:locator"},
    [Exporters](#ch01.xhtml#idm45207118482288){data-type="index:locator"},
    [Common
    Exporters](#ch10.xhtml#ix_exptr){data-type="index:locator"}-[Prometheus
    Configuration](#ch10.xhtml#idm45207095678624){data-type="index:locator"},
    [Configuration
    Management](#ch21.xhtml#idm45207087264240){data-type="index:locator"}
    -   [Blackbox]{data-type="index-term"},
        [Blackbox](#ch10.xhtml#ix_expBlck){data-type="index:locator"}-[Prometheus
        Configuration](#ch10.xhtml#idm45207095680816){data-type="index:locator"}
    -   [cAdvisor]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#ix_expcAd){data-type="index:locator"}-[Kubernetes](#ch09.xhtml#idm45207097471008){data-type="index:locator"}
    -   [considering in Prometheus rollout]{data-type="index-term"},
        [Planning a
        Rollout](#ch21.xhtml#idm45207087514016){data-type="index:locator"}
    -   [Consul]{data-type="index-term"},
        [Consul](#ch10.xhtml#ix_expCon){data-type="index:locator"}-[MySQLd](#ch10.xhtml#idm45207096572352){data-type="index:locator"}
    -   [Consul Exporter]{data-type="index-term"},
        [Consul](#ch08.xhtml#idm45207099911344){data-type="index:locator"}
    -   [Consul metrics exporter written in Python
        3]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093381760){data-type="index:locator"}
    -   [default ports]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096480016){data-type="index:locator"}
    -   [default registry and]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093386992){data-type="index:locator"}
    -   [exposing timestamps, staleness and]{data-type="index-term"},
        [Instant
        Vector](#ch13.xhtml#idm45207092580192){data-type="index:locator"}
    -   [Grok]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#ix_expGrok){data-type="index:locator"}-[Blackbox](#ch10.xhtml#idm45207096160128){data-type="index:locator"}
    -   [guideline for]{data-type="index-term"}, [Common
        Exporters](#ch10.xhtml#idm45207096614384){data-type="index:locator"}
    -   [kube-state-metrics]{data-type="index-term"},
        [kube-state-metrics](#ch09.xhtml#idm45207096704864){data-type="index:locator"}
    -   [MySQLd]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#ix_expMyS){data-type="index:locator"}-[Grok
        Exporter](#ch10.xhtml#idm45207096405168){data-type="index:locator"}
    -   [Node Exporter]{data-type="index-term"} ([see]{gentext="see"}
        Node Exporter)
    -   [other monitoring systems]{data-type="index-term"}, [Other
        Monitoring
        Systems](#ch11.xhtml#idm45207095580496){data-type="index:locator"},
        [StatsD](#ch11.xhtml#idm45207095392960){data-type="index:locator"}
    -   [Prometheus configuration for Blackbox probe URL
        parameters]{data-type="index-term"}, [Prometheus
        Configuration](#ch10.xhtml#idm45207095919472){data-type="index:locator"}
    -   [SaaS monitoring systems]{data-type="index-term"}, [Other
        Monitoring
        Systems](#ch11.xhtml#idm45207095559008){data-type="index:locator"}
    -   [server-side security]{data-type="index-term"}, [Security
        Features Provided by
        Prometheus](#ch20.xhtml#idm45207087874464){data-type="index:locator"}
    -   [writing]{data-type="index-term"}, [Writing
        Exporters](#ch12.xhtml#ix_expwr){data-type="index:locator"}-[Guidelines](#ch12.xhtml#idm45207092814992){data-type="index:locator"}
        -   [Consul Telemetry]{data-type="index-term"}, [Consul
            Telemetry](#ch12.xhtml#ix_expwrCT){data-type="index:locator"}-[Consul
            Telemetry](#ch12.xhtml#idm45207094817664){data-type="index:locator"}
        -   [custom collectors]{data-type="index-term"}, [Custom
            Collectors](#ch12.xhtml#ix_expwrCC){data-type="index:locator"}-[Guidelines](#ch12.xhtml#idm45207092874592){data-type="index:locator"}
        -   [guidelines for]{data-type="index-term"},
            [Guidelines](#ch12.xhtml#idm45207092871856){data-type="index:locator"}
-   [exposition]{data-type="index-term"},
    [Exposition](#ch04.xhtml#ix_expo){data-type="index:locator"}-[Timestamps](#ch04.xhtml#idm45207102063152){data-type="index:locator"}
    -   [from batch jobs using Pushgateway]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#ix_expobatch){data-type="index:locator"}-[Pushgateway](#ch04.xhtml#idm45207102355120){data-type="index:locator"}
    -   [custom collector]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093538112){data-type="index:locator"}
    -   [from Go client libraries]{data-type="index-term"},
        [Go](#ch04.xhtml#idm45207114220928){data-type="index:locator"}
    -   [from Java client libraries]{data-type="index-term"},
        [Java](#ch04.xhtml#ix_expoJava){data-type="index:locator"}-[Pushgateway](#ch04.xhtml#idm45207102622848){data-type="index:locator"}
    -   [from Python client libraries]{data-type="index-term"},
        [Exposition](#ch04.xhtml#ix_expoPy){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114224624){data-type="index:locator"}
        -   [parsers]{data-type="index-term"},
            [Parsers](#ch04.xhtml#idm45207102295776){data-type="index:locator"}
    -   [using bridges]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102350608){data-type="index:locator"}
-   [exposition formats]{data-type="index-term"}
    -   [OpenMetrics]{data-type="index-term"},
        [OpenMetrics](#ch04.xhtml#ix_expfmtOM){data-type="index:locator"}-[Timestamps](#ch04.xhtml#idm45207102064368){data-type="index:locator"}
        -   [labels]{data-type="index-term"},
            [Labels](#ch04.xhtml#idm45207102079392){data-type="index:locator"}
        -   [metric types]{data-type="index-term"}, [Metric
            Types](#ch04.xhtml#idm45207102101232){data-type="index:locator"}
        -   [timestamps]{data-type="index-term"},
            [Timestamps](#ch04.xhtml#idm45207102073216){data-type="index:locator"}
    -   [parsers in Python client libraries]{data-type="index-term"},
        [Parsers](#ch04.xhtml#idm45207102297408){data-type="index:locator"}
    -   [Prometheus text format]{data-type="index-term"}, [Text
        Exposition
        Format](#ch04.xhtml#ix_expfmtPrm){data-type="index:locator"}-[OpenMetrics](#ch04.xhtml#idm45207102111280){data-type="index:locator"}
        -   [escaping characters in]{data-type="index-term"},
            [Escaping](#ch04.xhtml#idm45207102140048){data-type="index:locator"}
        -   [labels]{data-type="index-term"},
            [Labels](#ch04.xhtml#idm45207102151168){data-type="index:locator"}
        -   [metric types]{data-type="index-term"}, [Metric
            Types](#ch04.xhtml#idm45207102181648){data-type="index:locator"}
        -   [timestamps]{data-type="index-term"},
            [Timestamps](#ch04.xhtml#idm45207102130992){data-type="index:locator"}
        -   [using promtool check metrics]{data-type="index-term"},
            [check
            metrics](#ch04.xhtml#idm45207102121168){data-type="index:locator"}
    -   [supported by Prometheus]{data-type="index-term"},
        [Exposition](#ch04.xhtml#idm45207116407792){data-type="index:locator"}
-   [expression browser]{data-type="index-term"},
    [Dashboards](#ch01.xhtml#idm45207119357008){data-type="index:locator"},
    [Running
    Prometheus](#ch02.xhtml#idm45207119755792){data-type="index:locator"}
    -   [range vector in Console tab]{data-type="index-term"}, [Range
        Vector](#ch13.xhtml#idm45207092561312){data-type="index:locator"}
    -   [using]{data-type="index-term"}, [Using the Expression
        Browser](#ch02.xhtml#ix_expbr){data-type="index:locator"}-[Using
        the Expression
        Browser](#ch02.xhtml#idm45207118082192){data-type="index:locator"}
-   [external labels]{data-type="index-term"}, [External
    Labels](#ch18.xhtml#idm45207089844288){data-type="index:locator"},
    [Long-Term
    Storage](#ch21.xhtml#idm45207087325232){data-type="index:locator"}
    -   [unique, for every Prometheus]{data-type="index-term"},
        [Planning for
        Failure](#ch21.xhtml#idm45207087211232){data-type="index:locator"}
-   [external_labels]{data-type="index-term"}, [Target
    Labels](#ch08.xhtml#idm45207099346432){data-type="index:locator"},
    [Alert
    Labels](#ch18.xhtml#idm45207089950576){data-type="index:locator"},
    [Routing
    Tree](#ch19.xhtml#idm45207089557536){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### F

-   [failures]{data-type="index-term"}
    -   [metrics for total and failures]{data-type="index-term"},
        [Library
        instrumentation](#ch03.xhtml#idm45207116498608){data-type="index:locator"}
    -   [planning for]{data-type="index-term"}, [Planning for
        Failure](#ch21.xhtml#ix_fail){data-type="index:locator"}-[Planning
        for
        Failure](#ch21.xhtml#idm45207087197888){data-type="index:locator"}
        -   [Alertmanager clustering]{data-type="index-term"},
            [Alertmanager
            Clustering](#ch21.xhtml#idm45207087191840){data-type="index:locator"}
        -   [meta- and cross-monitoring]{data-type="index-term"}, [Meta-
            and
            Cross-Monitoring](#ch21.xhtml#idm45207087178784){data-type="index:locator"}
-   [fallback (or default) route]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089683088){data-type="index:locator"}
-   [FDsNearLimit alert]{data-type="index-term"}, [Alert
    Labels](#ch18.xhtml#idm45207089946576){data-type="index:locator"}
-   [federation]{data-type="index-term"}, [Reducing
    Cardinality](#ch17.xhtml#idm45207090188672){data-type="index:locator"},
    [Horizontal
    Sharding](#ch21.xhtml#idm45207087072384){data-type="index:locator"}
    -   [global Prometheus servers]{data-type="index-term"}, [Planning
        for
        Failure](#ch21.xhtml#idm45207087201104){data-type="index:locator"}
    -   [going global with]{data-type="index-term"}, [Going Global with
        Federation](#ch21.xhtml#ix_fed){data-type="index:locator"}-[Going
        Global with
        Federation](#ch21.xhtml#idm45207087353744){data-type="index:locator"}
-   [file descriptors]{data-type="index-term"},
    [Hardware](#ch21.xhtml#idm45207087292768){data-type="index:locator"}
-   [file service discovery (file SD)]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100521120){data-type="index:locator"},
    [File](#ch08.xhtml#ix_fileSD){data-type="index:locator"}-[HTTP](#ch08.xhtml#idm45207100096160){data-type="index:locator"}
-   [files field]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090438416){data-type="index:locator"}
-   [filesystem collector]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100749536){data-type="index:locator"}
-   [filesystems]{data-type="index-term"},
    [Hardware](#ch21.xhtml#idm45207087299680){data-type="index:locator"}
    -   [node_filesystem_size_bytes metric]{data-type="index-term"},
        [Gauge](#ch13.xhtml#idm45207092781184){data-type="index:locator"}
    -   [returning total filesystem size on each
        machine]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091561168){data-type="index:locator"}
-   [file_sd_configs]{data-type="index-term"},
    [File](#ch08.xhtml#idm45207100193312){data-type="index:locator"},
    [Using Recording
    Rules](#ch17.xhtml#idm45207090437808){data-type="index:locator"}
-   [filtering]{data-type="index-term"}, [Comparison
    Operators](#ch15.xhtml#idm45207091287280){data-type="index:locator"}
    -   [avoiding in recording rule
        expressions]{data-type="index-term"}, [Alerting
        Rules](#ch18.xhtml#idm45207090038800){data-type="index:locator"}
    -   [bool modifier and]{data-type="index-term"}, [bool
        modifier](#ch15.xhtml#idm45207091272752){data-type="index:locator"}
-   [firing alerts]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090025808){data-type="index:locator"},
    [for](#ch18.xhtml#idm45207089996656){data-type="index:locator"},
    [Alert
    Labels](#ch18.xhtml#idm45207089944560){data-type="index:locator"}
-   [five-minute rate]{data-type="index-term"},
    [rate](#ch16.xhtml#idm45207090628288){data-type="index:locator"}
-   [float64]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207093454544){data-type="index:locator"}
-   [floating-point math]{data-type="index-term"}
    -   [comparisons with NaN]{data-type="index-term"}, [min and
        max](#ch14.xhtml#idm45207091444480){data-type="index:locator"}
    -   [dividing by zero, resulting in NaN]{data-type="index-term"},
        [Counting
        Exceptions](#ch03.xhtml#idm45207113260496){data-type="index:locator"}
-   [floating-point numbers]{data-type="index-term"}
    -   [64-bit, use by Prometheus]{data-type="index-term"}, [Counting
        Size](#ch03.xhtml#idm45207113176624){data-type="index:locator"}
    -   [using equality comparisons with]{data-type="index-term"},
        [Comparison
        Operators](#ch15.xhtml#idm45207091280624){data-type="index:locator"}
-   [for field]{data-type="index-term"},
    [for](#ch18.xhtml#idm45207090002208){data-type="index:locator"},
    [Alert
    Labels](#ch18.xhtml#idm45207089948720){data-type="index:locator"}
-   [fork syscalls]{data-type="index-term"}, [Stat
    Collector](#ch07.xhtml#idm45207100642064){data-type="index:locator"}
-   [formats]{data-type="index-term"}
    -   [Prometheus text format and
        OpenMetrics]{data-type="index-term"}, [What Is
        Prometheus?](#ch01.xhtml#idm45207119489216){data-type="index:locator"}
        -   ([see also]{gentext="see"} exposition formats)
    -   [third-party software exposing metrics in non-Prometheus
        format]{data-type="index-term"}, [What Is
        Prometheus?](#ch01.xhtml#idm45207116098944){data-type="index:locator"}
-   [frequency histograms]{data-type="index-term"},
    [count_values](#ch14.xhtml#idm45207091392000){data-type="index:locator"}
-   [frontend-pager receiver]{data-type="index-term"},
    [Receivers](#ch19.xhtml#idm45207088853040){data-type="index:locator"}
-   [fstype labels]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100742896){data-type="index:locator"},
    [Gauge](#ch13.xhtml#idm45207092783440){data-type="index:locator"},
    [Grouping](#ch14.xhtml#idm45207091643152){data-type="index:locator"}
-   [fully anchored regular expressions]{data-type="index-term"},
    [Choosing What to
    Scrape](#ch08.xhtml#idm45207099777920){data-type="index:locator"},
    [Matchers](#ch13.xhtml#idm45207092619664){data-type="index:locator"}
-   [functions]{data-type="index-term"},
    [Functions](#ch16.xhtml#ix_fnct){data-type="index:locator"}-[Aggregation
    Over Time](#ch16.xhtml#idm45207090460080){data-type="index:locator"}
    -   [aggregation over time]{data-type="index-term"}, [Aggregation
        Over
        Time](#ch16.xhtml#ix_fnctAOT){data-type="index:locator"}-[Aggregation
        Over
        Time](#ch16.xhtml#idm45207090465216){data-type="index:locator"}
    -   [changing gauges]{data-type="index-term"}, [Changing
        Gauges](#ch16.xhtml#ix_fnctchgauge){data-type="index:locator"}-[holt_winters](#ch16.xhtml#idm45207090505696){data-type="index:locator"}
        -   [changes function]{data-type="index-term"},
            [changes](#ch16.xhtml#idm45207090567696){data-type="index:locator"}
        -   [delta function]{data-type="index-term"},
            [delta](#ch16.xhtml#idm45207090529824){data-type="index:locator"}
        -   [deriv function]{data-type="index-term"},
            [deriv](#ch16.xhtml#idm45207090556288){data-type="index:locator"}
        -   [holt_winters function]{data-type="index-term"},
            [holt_winters](#ch16.xhtml#idm45207090515696){data-type="index:locator"}
        -   [predict_linear function]{data-type="index-term"},
            [predict_linear](#ch16.xhtml#idm45207090541984){data-type="index:locator"}
    -   [changing type]{data-type="index-term"}
        -   [scalar]{data-type="index-term"},
            [scalar](#ch16.xhtml#idm45207090971456){data-type="index:locator"}
        -   [vector]{data-type="index-term"}, [Changing
            Type](#ch16.xhtml#idm45207090985328){data-type="index:locator"}
    -   [composing range vector functions]{data-type="index-term"},
        [Composing Range Vector
        Functions](#ch17.xhtml#idm45207090179136){data-type="index:locator"}
    -   [counter]{data-type="index-term"},
        [Counters](#ch16.xhtml#ix_fnctcount){data-type="index:locator"}-[resets](#ch16.xhtml#idm45207090580720){data-type="index:locator"}
        -   [increase function]{data-type="index-term"},
            [increase](#ch16.xhtml#idm45207090616656){data-type="index:locator"}
        -   [irate function]{data-type="index-term"},
            [irate](#ch16.xhtml#idm45207090606784){data-type="index:locator"}
        -   [rate function]{data-type="index-term"},
            [rate](#ch16.xhtml#idm45207090639792){data-type="index:locator"}
        -   [resets function]{data-type="index-term"},
            [resets](#ch16.xhtml#idm45207090588592){data-type="index:locator"}
    -   [histogram_quantile]{data-type="index-term"}, [Histograms with
        histogram_quantile](#ch16.xhtml#idm45207090663952){data-type="index:locator"}
    -   [label_join]{data-type="index-term"},
        [label_join](#ch16.xhtml#idm45207090728080){data-type="index:locator"}
    -   [label_replace]{data-type="index-term"},
        [label_replace](#ch16.xhtml#idm45207090740784){data-type="index:locator"}
    -   [math]{data-type="index-term"},
        [Math](#ch16.xhtml#ix_fnctmth){data-type="index:locator"}-[Time
        and
        Date](#ch16.xhtml#idm45207090837200){data-type="index:locator"}
        -   [abs]{data-type="index-term"},
            [abs](#ch16.xhtml#idm45207090949168){data-type="index:locator"}
        -   [ceil and floor]{data-type="index-term"}, [ceil and
            floor](#ch16.xhtml#idm45207090904736){data-type="index:locator"}
        -   [clamp, clamp_max, and clamp_min]{data-type="index-term"},
            [clamp, clamp_max, and
            clamp_min](#ch16.xhtml#idm45207090885504){data-type="index:locator"}
        -   [exp]{data-type="index-term"},
            [exp](#ch16.xhtml#idm45207090925328){data-type="index:locator"}
        -   [ln, log2, and log10]{data-type="index-term"}, [ln, log2,
            and
            log10](#ch16.xhtml#idm45207090940096){data-type="index:locator"}
        -   [round]{data-type="index-term"},
            [round](#ch16.xhtml#idm45207090895200){data-type="index:locator"}
        -   [sgn]{data-type="index-term"},
            [sgn](#ch16.xhtml#idm45207090871232){data-type="index:locator"}
        -   [sqrt]{data-type="index-term"},
            [sqrt](#ch16.xhtml#idm45207090918400){data-type="index:locator"}
        -   [trigonometric functions]{data-type="index-term"},
            [Trigonometric
            Functions](#ch16.xhtml#idm45207090866496){data-type="index:locator"}
    -   [missing series, absent and
        absent_over_time]{data-type="index-term"}, [Missing Series,
        absent, and
        absent_over_time](#ch16.xhtml#idm45207090712720){data-type="index:locator"}
    -   [sorting with sort and sort_desc]{data-type="index-term"},
        [Sorting with sort and
        sort_desc](#ch16.xhtml#idm45207090680048){data-type="index:locator"}
    -   [time and date]{data-type="index-term"}, [Time and
        Date](#ch16.xhtml#ix_fncttmda){data-type="index:locator"}-[timestamp](#ch16.xhtml#idm45207090750640){data-type="index:locator"}
        -   [days_in_month]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090799936){data-type="index:locator"}
        -   [day_of_month]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090804576){data-type="index:locator"}
        -   [day_of_week]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090805792){data-type="index:locator"}
        -   [day_of_year]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090803360){data-type="index:locator"}
        -   [hour function]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090807008){data-type="index:locator"}
        -   [minute function]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090808224){data-type="index:locator"}
        -   [month function]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090798688){data-type="index:locator"}
        -   [time function]{data-type="index-term"},
            [time](#ch16.xhtml#idm45207090827584){data-type="index:locator"}
        -   [year function]{data-type="index-term"}, [minute, hour,
            day_of_week, day_of_month, day_of_year, days_in_month,
            month, and
            year](#ch16.xhtml#idm45207090797472){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### G

-   [gaps]{data-type="index-term"}
    -   [between data points you have and boundaries of the
        range.]{data-type="index-term"},
        [Counter](#ch13.xhtml#idm45207092744160){data-type="index:locator"}
    -   [due to failed scrapes]{data-type="index-term"}, [Planning for
        Failure](#ch21.xhtml#idm45207087222112){data-type="index:locator"},
        [Managing
        Change](#ch21.xhtml#idm45207087062976){data-type="index:locator"}
    -   [in exception ratio graph for no
        requests]{data-type="index-term"}, [Counting
        Exceptions](#ch03.xhtml#idm45207113262224){data-type="index:locator"}
-   [Gather method]{data-type="index-term"},
    [Bridges](#ch04.xhtml#idm45207102306112){data-type="index:locator"}
-   [GaugeHistograms]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102095504){data-type="index:locator"}
-   [gauges]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207113338528){data-type="index:locator"},
    [The Gauge](#ch03.xhtml#ix_gauge){data-type="index:locator"}-[The
    Summary](#ch03.xhtml#idm45207113552800){data-type="index:locator"}
    -   [aggregating]{data-type="index-term"},
        [Gauge](#ch13.xhtml#ix_gaugeagg){data-type="index:locator"}-[Counter](#ch13.xhtml#idm45207092758192){data-type="index:locator"}
    -   [changing, functions for]{data-type="index-term"}, [Changing
        Gauges](#ch16.xhtml#ix_gaugechg){data-type="index:locator"}-[holt_winters](#ch16.xhtml#idm45207090504416){data-type="index:locator"}
        -   [changes function]{data-type="index-term"},
            [changes](#ch16.xhtml#idm45207090568912){data-type="index:locator"}
        -   [delta function]{data-type="index-term"},
            [delta](#ch16.xhtml#idm45207090528608){data-type="index:locator"}
        -   [deriv function]{data-type="index-term"},
            [deriv](#ch16.xhtml#idm45207090554368){data-type="index:locator"}
        -   [holt_winters function]{data-type="index-term"},
            [holt_winters](#ch16.xhtml#idm45207090514480){data-type="index:locator"}
        -   [predict_linear function]{data-type="index-term"},
            [predict_linear](#ch16.xhtml#idm45207090543200){data-type="index:locator"}
    -   [Consul]{data-type="index-term"}, [Consul
        Telemetry](#ch12.xhtml#idm45207095338464){data-type="index:locator"}
    -   [multiprocess_mode configuration]{data-type="index-term"},
        [Multiprocess with
        Gunicorn](#ch04.xhtml#idm45207118201616){data-type="index:locator"}
    -   [needed by histogram_quantile]{data-type="index-term"},
        [Histograms with
        histogram_quantile](#ch16.xhtml#idm45207090655440){data-type="index:locator"}
    -   [processing in custom collector]{data-type="index-term"},
        [Custom
        Collectors](#ch12.xhtml#idm45207093963632){data-type="index:locator"}
    -   [text exposition format for]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102171392){data-type="index:locator"}
    -   [timestamp in text exposition format]{data-type="index-term"},
        [Timestamps](#ch04.xhtml#idm45207102127664){data-type="index:locator"}
    -   [used as enum, custom collector for]{data-type="index-term"},
        [Enum](#ch05.xhtml#idm45207101414640){data-type="index:locator"}
    -   [using callbacks]{data-type="index-term"},
        [Callbacks](#ch03.xhtml#idm45207113580448){data-type="index:locator"}
-   [GaugeValue]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207093964688){data-type="index:locator"}
-   [geometric mean]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091511952){data-type="index:locator"}
-   [get_sample_value function]{data-type="index-term"}, [Unit Testing
    Instrumentation](#ch03.xhtml#idm45207115713120){data-type="index:locator"}
-   [global Prometheus servers]{data-type="index-term"}, [Planning for
    Failure](#ch21.xhtml#idm45207087201840){data-type="index:locator"}
-   [global section (prometheus.yml)]{data-type="index-term"}, [External
    Labels](#ch18.xhtml#idm45207089840176){data-type="index:locator"}
-   [globs, use in filenames]{data-type="index-term"},
    [File](#ch08.xhtml#idm45207100192608){data-type="index:locator"}
-   [Go]{data-type="index-term"}
    -   [client library metrics for]{data-type="index-term"},
        [Library](#ch03.xhtml#idm45207116429872){data-type="index:locator"}
    -   [collectors written in]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207094278656){data-type="index:locator"}
    -   [custom collectors written in]{data-type="index-term"},
        [Exporters](#ch01.xhtml#idm45207118477456){data-type="index:locator"}
    -   [exposition from client libraries]{data-type="index-term"},
        [Go](#ch04.xhtml#idm45207114221888){data-type="index:locator"}
    -   [RE2 regular expression engine]{data-type="index-term"},
        [Choosing What to
        Scrape](#ch08.xhtml#idm45207099440960){data-type="index:locator"}
    -   [Registry.Gather]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102304736){data-type="index:locator"}
    -   [running Consul Telemetry exporter]{data-type="index-term"},
        [Consul
        Telemetry](#ch12.xhtml#idm45207094822496){data-type="index:locator"}
    -   [templating language]{data-type="index-term"}, [Dashboarding
        with
        Grafana](#ch06.xhtml#idm45207100996016){data-type="index:locator"}
    -   [templating system]{data-type="index-term"}, [Annotations and
        Templates](#ch18.xhtml#idm45207089924304){data-type="index:locator"}
    -   [WithLabelValues]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#idm45207102027488){data-type="index:locator"}
    -   [writing exporters in]{data-type="index-term"}, [Consul
        Telemetry](#ch12.xhtml#idm45207095350912){data-type="index:locator"}
-   [Grafana]{data-type="index-term"},
    [Dashboards](#ch01.xhtml#idm45207119355568){data-type="index:locator"},
    [Dashboarding with
    Grafana](#ch06.xhtml#ix_Grfn){data-type="index:locator"}-[Template
    Variables](#ch06.xhtml#idm45207100812816){data-type="index:locator"},
    [Going Global with
    Federation](#ch21.xhtml#idm45207087354416){data-type="index:locator"}
    -   [aligned data]{data-type="index-term"}, [Aligned
        data](#ch13.xhtml#idm45207091805552){data-type="index:locator"}
    -   [dashboards and panels]{data-type="index-term"}, [Dashboards and
        Panels](#ch06.xhtml#ix_Grfndspa){data-type="index:locator"}-[Avoiding
        the Wall of
        Graphs](#ch06.xhtml#idm45207100946304){data-type="index:locator"}
        -   [avoiding wall of graphs]{data-type="index-term"}, [Avoiding
            the Wall of
            Graphs](#ch06.xhtml#idm45207100951984){data-type="index:locator"}
    -   [data source]{data-type="index-term"}, [Data
        Source](#ch06.xhtml#idm45207100971840){data-type="index:locator"}
    -   [installing]{data-type="index-term"},
        [Installation](#ch06.xhtml#ix_Grfninst){data-type="index:locator"}-[Data
        Source](#ch06.xhtml#idm45207100973088){data-type="index:locator"}
    -   [reporting_enabled setting]{data-type="index-term"},
        [Dashboarding with
        Grafana](#ch06.xhtml#idm45207100999536){data-type="index:locator"}
    -   [Stat panel]{data-type="index-term"}, [Stat
        Panel](#ch06.xhtml#ix_GrfnStat){data-type="index:locator"}-[Stat
        Panel](#ch06.xhtml#idm45207100881664){data-type="index:locator"}
    -   [State timeline panel]{data-type="index-term"}, [State Timeline
        Panel](#ch06.xhtml#idm45207100860400){data-type="index:locator"}
    -   [Table panel]{data-type="index-term"}, [Table
        Panel](#ch06.xhtml#idm45207100874864){data-type="index:locator"}
    -   [template variables]{data-type="index-term"}, [Template
        Variables](#ch06.xhtml#idm45207100845040){data-type="index:locator"}
    -   [Time series panel]{data-type="index-term"}, [Time Series
        Panel](#ch06.xhtml#ix_GrfnTSP){data-type="index:locator"}-[Time
        Controls](#ch06.xhtml#idm45207100899632){data-type="index:locator"}
    -   [time shifting panel to different time
        range]{data-type="index-term"},
        [Offset](#ch13.xhtml#idm45207092497696){data-type="index:locator"}
-   [Graphite]{data-type="index-term"}, [A Brief and Incomplete History
    of
    Monitoring](#ch01.xhtml#idm45207119808928){data-type="index:locator"},
    [Other Monitoring
    Systems](#ch11.xhtml#idm45207095575760){data-type="index:locator"}
    -   [bridge]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102348704){data-type="index:locator"}
    -   [dotted string notation]{data-type="index-term"},
        [StatsD](#ch11.xhtml#idm45207095438976){data-type="index:locator"}
-   [Graphite Exporter]{data-type="index-term"}, [Planning a
    Rollout](#ch21.xhtml#idm45207087502608){data-type="index:locator"}
-   [graphs]{data-type="index-term"}
    -   [graph editor in Grafana]{data-type="index-term"}, [Time Series
        Panel](#ch06.xhtml#idm45207100939488){data-type="index:locator"}
    -   [limiting number on a dashboard]{data-type="index-term"},
        [Avoiding the Wall of
        Graphs](#ch06.xhtml#idm45207100949104){data-type="index:locator"}
-   [Grok Exporter]{data-type="index-term"}, [Grok
    Exporter](#ch10.xhtml#ix_Grok){data-type="index:locator"}-[Blackbox](#ch10.xhtml#idm45207096161104){data-type="index:locator"}
    -   [configuring for scraping by
        Prometheus]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096202096){data-type="index:locator"}
    -   [defining where to expose its metrics]{data-type="index-term"},
        [Grok
        Exporter](#ch10.xhtml#idm45207096208432){data-type="index:locator"}
    -   [grok.yml to parse logfile and produce
        metrics]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096394816){data-type="index:locator"}
    -   [metrics produced by]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096216320){data-type="index:locator"}
    -   [use of patterns based on regular
        expressions]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096220496){data-type="index:locator"}
-   [group operator]{data-type="index-term"},
    [group](#ch14.xhtml#idm45207091491600){data-type="index:locator"}
-   [grouping]{data-type="index-term"},
    [Grouping](#ch14.xhtml#ix_grpng){data-type="index:locator"}-[by](#ch14.xhtml#idm45207091570976){data-type="index:locator"}
    -   [of alerts in Alertmanager]{data-type="index-term"},
        [Notification
        Pipeline](#ch19.xhtml#idm45207089808976){data-type="index:locator"},
        [Grouping](#ch19.xhtml#idm45207089242848){data-type="index:locator"}
        -   [disabling]{data-type="index-term"},
            [Grouping](#ch19.xhtml#idm45207089069632){data-type="index:locator"}
    -   [by clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091612048){data-type="index:locator"}
    -   [without clause]{data-type="index-term"},
        [without](#ch14.xhtml#idm45207091637408){data-type="index:locator"}
-   [grouping keys]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102358368){data-type="index:locator"}
-   [groups]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102361328){data-type="index:locator"}
    -   [alerting rules in]{data-type="index-term"}, [Alerting
        Rules](#ch18.xhtml#idm45207090048880){data-type="index:locator"}
    -   [problem with rule groups]{data-type="index-term"}, [Detecting a
        Problem](#ch21.xhtml#idm45207087151232){data-type="index:locator"}
    -   [repeating notifications for]{data-type="index-term"},
        [Throttling and
        repetition](#ch19.xhtml#idm45207089033472){data-type="index:locator"}
    -   [in rule files]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#idm45207090296800){data-type="index:locator"},
        [Reducing
        Cardinality](#ch17.xhtml#idm45207090197056){data-type="index:locator"}
    -   [throttling notifications for]{data-type="index-term"},
        [Throttling and
        repetition](#ch19.xhtml#idm45207089038704){data-type="index:locator"}
-   [group_by]{data-type="index-term"},
    [Grouping](#ch19.xhtml#idm45207089240704){data-type="index:locator"},
    [Notification
    templates](#ch19.xhtml#idm45207087990272){data-type="index:locator"}
-   [group_interval]{data-type="index-term"}, [Throttling and
    repetition](#ch19.xhtml#idm45207089035968){data-type="index:locator"}
    -   [tweaking]{data-type="index-term"}, [Throttling and
        repetition](#ch19.xhtml#idm45207089020704){data-type="index:locator"}
-   [group_left]{data-type="index-term"},
    [Info](#ch05.xhtml#idm45207101101184){data-type="index:locator"},
    [Hwmon
    Collector](#ch07.xhtml#idm45207100658336){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#ix_grplft){data-type="index:locator"}-[Many-to-One
    and
    group_left](#ch15.xhtml#idm45207091129424){data-type="index:locator"},
    [or
    operator](#ch15.xhtml#idm45207091089088){data-type="index:locator"}
-   [group_right]{data-type="index-term"}, [Many-to-One and
    group_left](#ch15.xhtml#idm45207091131168){data-type="index:locator"}
-   [group_wait]{data-type="index-term"}, [Throttling and
    repetition](#ch19.xhtml#idm45207089036576){data-type="index:locator"}
-   [growing Prometheus]{data-type="index-term"}, [Growing
    Prometheus](#ch21.xhtml#idm45207087494288){data-type="index:locator"}
-   [guests, CPU usage by]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100755808){data-type="index:locator"}
-   [Gunicorn]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#ix_Guni){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114227088){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### H

-   [handler labels, aggregating away]{data-type="index-term"},
    [Summary](#ch13.xhtml#idm45207092715824){data-type="index:locator"}
-   [hardware]{data-type="index-term"},
    [Hardware](#ch21.xhtml#ix_hardw){data-type="index:locator"}-[Hardware](#ch21.xhtml#idm45207087288000){data-type="index:locator"}
-   [hashmod relabel action]{data-type="index-term"},
    [Hashmod](#ch21.xhtml#idm45207087119392){data-type="index:locator"},
    [Horizontal
    Sharding](#ch21.xhtml#idm45207087084432){data-type="index:locator"}
-   [health checks, standardizing across
    services]{data-type="index-term"},
    [TCP](#ch10.xhtml#idm45207096002800){data-type="index:locator"}
-   [health monitoring, HTTP SD]{data-type="index-term"},
    [HTTP](#ch08.xhtml#idm45207100061024){data-type="index:locator"}
-   [Helm, use to create components from Prometheus
    ecosystem]{data-type="index-term"}, [Alternative
    Deployments](#ch09.xhtml#idm45207096627168){data-type="index:locator"}
-   [HELP (metrics)]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102179136){data-type="index:locator"},
    [Metric
    Types](#ch04.xhtml#idm45207102093472){data-type="index:locator"},
    [Using the Textfile
    Collector](#ch07.xhtml#idm45207100553936){data-type="index:locator"}
-   [help, sources of]{data-type="index-term"}, [Getting
    Help](#ch21.xhtml#idm45207087057536){data-type="index:locator"}
-   [histograms]{data-type="index-term"}, [The
    Histogram](#ch03.xhtml#ix_hist){data-type="index:locator"}-[Unit
    Testing
    Instrumentation](#ch03.xhtml#idm45207115719648){data-type="index:locator"},
    [Cardinality](#ch05.xhtml#idm45207101023520){data-type="index:locator"},
    [Histogram](#ch13.xhtml#ix_hstagg){data-type="index:locator"}-[Selectors](#ch13.xhtml#idm45207092659776){data-type="index:locator"}
    -   [buckets]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207118978848){data-type="index:locator"},
        [How Much Should I
        Instrument?](#ch03.xhtml#idm45207116482480){data-type="index:locator"}
        -   [using rate before sum on]{data-type="index-term"},
            [sum](#ch14.xhtml#idm45207091555376){data-type="index:locator"}
    -   [cumulative]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115784912){data-type="index:locator"}
    -   [dropping buckets to reduce
        cardinality]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207097916720){data-type="index:locator"}
    -   [frequency histogram]{data-type="index-term"},
        [count_values](#ch14.xhtml#idm45207091391328){data-type="index:locator"}
    -   [HTTP request latency from Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096210912){data-type="index:locator"}
    -   [text exposition format for]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102168656){data-type="index:locator"}
-   [histogram_quantile function]{data-type="index-term"}, [The
    Histogram](#ch03.xhtml#idm45207118983472){data-type="index:locator"},
    [Metric
    Types](#ch04.xhtml#idm45207102153920){data-type="index:locator"},
    [Histogram](#ch13.xhtml#idm45207092682320){data-type="index:locator"},
    [quantile](#ch14.xhtml#idm45207091402688){data-type="index:locator"}
    -   [about]{data-type="index-term"}, [Histograms with
        histogram_quantile](#ch16.xhtml#idm45207090665072){data-type="index:locator"}
-   [holt_winters function]{data-type="index-term"},
    [holt_winters](#ch16.xhtml#idm45207090516400){data-type="index:locator"}
-   [Home Dashboard (Grafana)]{data-type="index-term"},
    [Installation](#ch06.xhtml#idm45207100977648){data-type="index:locator"}
-   [honor_labels (scrape config)]{data-type="index-term"}, [Label
    Clashes and
    honor_labels](#ch08.xhtml#idm45207097686768){data-type="index:locator"}
-   [horizontal sharding]{data-type="index-term"}, [Horizontal
    Sharding](#ch21.xhtml#idm45207087089488){data-type="index:locator"}
-   [host labels]{data-type="index-term"}, [Target
    Labels](#ch08.xhtml#idm45207099353104){data-type="index:locator"}
-   [hour function]{data-type="index-term"}, [and
    operator](#ch15.xhtml#idm45207091039296){data-type="index:locator"},
    [minute, hour, day_of_week, day_of_month, day_of_year,
    days_in_month, month, and
    year](#ch16.xhtml#idm45207090812256){data-type="index:locator"}
-   [hours]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092549712){data-type="index:locator"}
-   [HTTP]{data-type="index-term"}
    -   [exposition to Prometheus over]{data-type="index-term"},
        [Exposition](#ch04.xhtml#idm45207116409296){data-type="index:locator"}
    -   [probing with Blackbox exporter]{data-type="index-term"},
        [HTTP](#ch10.xhtml#ix_HTTPprb){data-type="index:locator"}-[HTTP](#ch10.xhtml#idm45207095949232){data-type="index:locator"}
    -   [receivers based on]{data-type="index-term"},
        [Receivers](#ch19.xhtml#idm45207088472832){data-type="index:locator"}
-   [HTTP API]{data-type="index-term"}, [HTTP
    API](#ch13.xhtml#ix_HTTPAPI){data-type="index:locator"}-[Aligned
    data](#ch13.xhtml#idm45207091662720){data-type="index:locator"}
    -   [aligned data]{data-type="index-term"}, [Aligned
        data](#ch13.xhtml#idm45207091803904){data-type="index:locator"}
    -   [query or query endpoint]{data-type="index-term"},
        [query](#ch13.xhtml#ix_HTTPAPIqry){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091976208){data-type="index:locator"}
    -   [query range endpoint or query_range]{data-type="index-term"},
        [query_range](#ch13.xhtml#ix_HTTPAPIqryrng){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091808832){data-type="index:locator"}
-   [HTTP Basic Authentication]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098246768){data-type="index:locator"}
-   [HTTP Bearer Token Authentication]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098246128){data-type="index:locator"}
-   [HTTP requests]{data-type="index-term"},
    [Tracing](#ch01.xhtml#idm45207119673952){data-type="index:locator"}
    -   [exposition format in response to]{data-type="index-term"},
        [Client
        Libraries](#ch01.xhtml#idm45207113057680){data-type="index:locator"}
    -   [logging latency for, using Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096212768){data-type="index:locator"}
    -   [metrics on]{data-type="index-term"},
        [Metrics](#ch01.xhtml#idm45207118492656){data-type="index:locator"}
    -   [metrics on broken out by path]{data-type="index-term"}, [What
        Are
        Labels?](#ch05.xhtml#idm45207102056032){data-type="index:locator"}
    -   [rejected]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118786720){data-type="index:locator"}
-   [HTTP server in Python (example)]{data-type="index-term"}, [A Simple
    Program](#ch03.xhtml#idm45207116008656){data-type="index:locator"}
-   [HTTP service discovery (HTTP SD)]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100518272){data-type="index:locator"},
    [HTTP](#ch08.xhtml#idm45207100095312){data-type="index:locator"}
-   [HttpServer class]{data-type="index-term"},
    [HTTPServer](#ch04.xhtml#idm45207103388128){data-type="index:locator"}
-   [HttpServlet class]{data-type="index-term"},
    [Servlet](#ch04.xhtml#idm45207103129376){data-type="index:locator"}
-   [http_config field]{data-type="index-term"},
    [Receivers](#ch19.xhtml#idm45207088471984){data-type="index:locator"}
-   [http_requests_total]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092488000){data-type="index:locator"}
-   [http_response_size_bytes]{data-type="index-term"},
    [Summary](#ch13.xhtml#idm45207092720160){data-type="index:locator"}
-   [http_response_size_bytes_count]{data-type="index-term"},
    [Summary](#ch13.xhtml#idm45207092717216){data-type="index:locator"}
-   [http_response_size_bytes_sum]{data-type="index-term"},
    [Summary](#ch13.xhtml#idm45207092710256){data-type="index:locator"}
-   [http_sd_configs]{data-type="index-term"},
    [HTTP](#ch08.xhtml#idm45207100059232){data-type="index:locator"}
-   [hwmon collector]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100668608){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### I

-   [I/O, disk]{data-type="index-term"}, [Running the Node
    Exporter](#ch02.xhtml#idm45207118072544){data-type="index:locator"},
    [Diskstats
    Collector](#ch07.xhtml#idm45207100726736){data-type="index:locator"},
    [Diskstats
    Collector](#ch07.xhtml#idm45207100701328){data-type="index:locator"}
-   [IAM user (Amazon)]{data-type="index-term"},
    [EC2](#ch08.xhtml#idm45207099903760){data-type="index:locator"}
-   [ICMP probing]{data-type="index-term"},
    [ICMP](#ch10.xhtml#ix_ICMP){data-type="index:locator"}-[ICMP](#ch10.xhtml#idm45207096027504){data-type="index:locator"}
    -   [extra privileges required for]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096077536){data-type="index:locator"}
    -   [failed probe from IPv6 target URL
        parameter]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096046704){data-type="index:locator"}
    -   [icmp module]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096050528){data-type="index:locator"}
    -   [pinging localhost]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096074432){data-type="index:locator"}
    -   [probing google.com with debug=true ending
        URL]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096038624){data-type="index:locator"}
-   [id labels]{data-type="index-term"},
    [cAdvisor](#ch09.xhtml#idm45207097654464){data-type="index:locator"}
-   [idempotency for batch jobs]{data-type="index-term"}, [Service
    instrumentation](#ch03.xhtml#idm45207116506128){data-type="index:locator"}
-   [idle time per second per CPU]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100761392){data-type="index:locator"}
-   [ignoring clause]{data-type="index-term"},
    [One-to-One](#ch15.xhtml#idm45207091216896){data-type="index:locator"}
    -   [use with and operator]{data-type="index-term"}, [and
        operator](#ch15.xhtml#idm45207091043408){data-type="index:locator"}
    -   [use with or operator]{data-type="index-term"}, [or
        operator](#ch15.xhtml#idm45207091102240){data-type="index:locator"}
    -   [using with group_left]{data-type="index-term"}, [Many-to-One
        and
        group_left](#ch15.xhtml#idm45207091138464){data-type="index:locator"}
-   [image labels]{data-type="index-term"},
    [Labels](#ch09.xhtml#idm45207097554864){data-type="index:locator"}
-   [imports]{data-type="index-term"}, [The
    Counter](#ch03.xhtml#idm45207117187344){data-type="index:locator"}
-   [inc method]{data-type="index-term"}, [Using
    Gauges](#ch03.xhtml#idm45207117325552){data-type="index:locator"}
-   [increase function]{data-type="index-term"},
    [Counter](#ch13.xhtml#idm45207092749936){data-type="index:locator"},
    [increase](#ch16.xhtml#idm45207090617328){data-type="index:locator"}
-   [InfluxDB]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095581104){data-type="index:locator"},
    [InfluxDB](#ch11.xhtml#idm45207095538016){data-type="index:locator"}
-   [info metrics]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102094304){data-type="index:locator"},
    [Info](#ch05.xhtml#idm45207101160768){data-type="index:locator"}
    -   [joining to another metric]{data-type="index-term"},
        [Info](#ch05.xhtml#idm45207101102032){data-type="index:locator"}
-   [ingress]{data-type="index-term"},
    [Ingress](#ch09.xhtml#idm45207096672144){data-type="index:locator"}
-   [inhibitions]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089818816){data-type="index:locator"},
    [Inhibitions](#ch19.xhtml#idm45207087923504){data-type="index:locator"}
-   [instance labels]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207118606848){data-type="index:locator"},
    [Uname
    Collector](#ch07.xhtml#idm45207100629760){data-type="index:locator"},
    [Grouping](#ch14.xhtml#idm45207091639936){data-type="index:locator"}
    -   [grouping alerts and]{data-type="index-term"},
        [Grouping](#ch19.xhtml#idm45207089073936){data-type="index:locator"}
    -   [including in sum without]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101450880){data-type="index:locator"}
    -   [on clause with]{data-type="index-term"}, [Many-to-One and
        group_left](#ch15.xhtml#idm45207091149888){data-type="index:locator"}
    -   [in recording rule output]{data-type="index-term"}, [How Not to
        Use
        Rules](#ch17.xhtml#idm45207090143552){data-type="index:locator"}
    -   [relabeling]{data-type="index-term"}, [job, instance, and
        \_\_address\_\_](#ch08.xhtml#idm45207098898016){data-type="index:locator"}
    -   [removing using sum without]{data-type="index-term"},
        [Gauge](#ch13.xhtml#idm45207092771616){data-type="index:locator"},
        [Counter](#ch13.xhtml#idm45207092736512){data-type="index:locator"}
    -   [service-level batch jobs and]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102569232){data-type="index:locator"}
-   [InstanceDown alert]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207113096528){data-type="index:locator"}
    -   [viewing in Alertmanager]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207117530880){data-type="index:locator"}
-   [instant rate]{data-type="index-term"} ([see]{gentext="see"} irate
    function)
-   [instant vector selector]{data-type="index-term"},
    [Selectors](#ch13.xhtml#idm45207092643584){data-type="index:locator"},
    [Instant
    Vector](#ch13.xhtml#idm45207092592320){data-type="index:locator"}
-   [instant vectors]{data-type="index-term"},
    [query](#ch13.xhtml#idm45207092351760){data-type="index:locator"},
    [query_range](#ch13.xhtml#idm45207091954752){data-type="index:locator"}
    -   [binary operators applied to]{data-type="index-term"}, [Binary
        Operators](#ch15.xhtml#idm45207091359632){data-type="index:locator"}
    -   [empty, input to aggregation operator]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091576720){data-type="index:locator"}
    -   [empty, returned by binary operator]{data-type="index-term"},
        [One-to-One](#ch15.xhtml#idm45207091219712){data-type="index:locator"}
    -   [gauge-changing functions returning]{data-type="index-term"},
        [Changing
        Gauges](#ch16.xhtml#idm45207090572704){data-type="index:locator"}
    -   [matching]{data-type="index-term"} ([see]{gentext="see"} vector
        matching)
    -   [quantile working across in aggregation
        group]{data-type="index-term"},
        [quantile](#ch14.xhtml#idm45207091399040){data-type="index:locator"}
    -   [return by PromQL functions]{data-type="index-term"},
        [Functions](#ch16.xhtml#idm45207090998144){data-type="index:locator"}
    -   [{} 0]{data-type="index-term"}, [Working with
        Scalars](#ch15.xhtml#idm45207091348128){data-type="index:locator"}
-   [instrumentation]{data-type="index-term"},
    [Instrumentation](#ch03.xhtml#ix_instr){data-type="index:locator"}-[Library](#ch03.xhtml#idm45207116415744){data-type="index:locator"},
    [Exposition](#ch04.xhtml#idm45207116403568){data-type="index:locator"},
    [Planning a
    Rollout](#ch21.xhtml#idm45207087510800){data-type="index:locator"}
    -   [counters]{data-type="index-term"}, [The
        Counter](#ch03.xhtml#instr3cntr){data-type="index:locator"}
        -   [counting exceptions]{data-type="index-term"}, [Counting
            Exceptions](#ch03.xhtml#idm45207117164048){data-type="index:locator"}
        -   [counting size]{data-type="index-term"}, [Counting
            Size](#ch03.xhtml#idm45207113160416){data-type="index:locator"}
    -   [deciding how much to instrument]{data-type="index-term"}, [How
        Much Should I
        Instrument?](#ch03.xhtml#idm45207116489040){data-type="index:locator"}
    -   [deciding what to instrument]{data-type="index-term"}, [What
        Should I
        Instrument?](#ch03.xhtml#idm45207115652720){data-type="index:locator"}
        -   [library instrumentation]{data-type="index-term"}, [Library
            instrumentation](#ch03.xhtml#idm45207116502048){data-type="index:locator"}
        -   [service instrumentation]{data-type="index-term"}, [Service
            instrumentation](#ch03.xhtml#idm45207115649680){data-type="index:locator"}
    -   [direct, use by info metric]{data-type="index-term"},
        [Info](#ch05.xhtml#idm45207101148256){data-type="index:locator"}
    -   [example Python program exposing Prometheus
        metrics]{data-type="index-term"}, [A Simple
        Program](#ch03.xhtml#ix_instrprg){data-type="index:locator"}
    -   [feeding data into non-Prometheus
        library]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102303792){data-type="index:locator"}
    -   [gauges]{data-type="index-term"}, [The
        Gauge](#ch03.xhtml#ix_instrgauge){data-type="index:locator"}-[The
        Summary](#ch03.xhtml#idm45207113554048){data-type="index:locator"}
    -   [Go program demonstrating]{data-type="index-term"},
        [Go](#ch04.xhtml#idm45207114219696){data-type="index:locator"}
    -   [histograms]{data-type="index-term"}, [The
        Histogram](#ch03.xhtml#ix_instrhist){data-type="index:locator"}-[Unit
        Testing
        Instrumentation](#ch03.xhtml#idm45207115720896){data-type="index:locator"}
    -   [for languages running on JVM]{data-type="index-term"},
        [Java](#ch04.xhtml#idm45207103392656){data-type="index:locator"}
    -   [naming metrics]{data-type="index-term"}, [What Should I Name My
        Metrics?](#ch03.xhtml#ix_instrnmmtr){data-type="index:locator"}-[Library](#ch03.xhtml#idm45207116418208){data-type="index:locator"}
    -   [Python batch job and pushing its metrics to
        Pushgateway]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102507168){data-type="index:locator"}
    -   [summary]{data-type="index-term"}, [The
        Summary](#ch03.xhtml#idm45207113551056){data-type="index:locator"}
    -   [unit testing]{data-type="index-term"}, [Unit Testing
        Instrumentation](#ch03.xhtml#idm45207115718464){data-type="index:locator"}
-   [instrumentation labels]{data-type="index-term"},
    [Instrumentation](#ch05.xhtml#ix_instrlbl){data-type="index:locator"}-[Aggregating](#ch05.xhtml#idm45207101485824){data-type="index:locator"}
    -   [child]{data-type="index-term"},
        [Child](#ch05.xhtml#ix_instrchld){data-type="index:locator"}-[Aggregating](#ch05.xhtml#idm45207101496128){data-type="index:locator"}
    -   [clashes with target labels, honor_labels
        and]{data-type="index-term"}, [Label Clashes and
        honor_labels](#ch08.xhtml#idm45207097739232){data-type="index:locator"}
    -   [device, fstype, and mountpoint]{data-type="index-term"},
        [Grouping](#ch14.xhtml#idm45207091644800){data-type="index:locator"}
    -   [metric]{data-type="index-term"},
        [Metric](#ch05.xhtml#idm45207101859408){data-type="index:locator"}
    -   [metrics without, working with]{data-type="index-term"}, [or
        operator](#ch15.xhtml#idm45207091083072){data-type="index:locator"}
    -   [mode]{data-type="index-term"}, [Unique label
        values](#ch14.xhtml#idm45207091528416){data-type="index:locator"}
    -   [multiple labels for a metric]{data-type="index-term"},
        [Multiple
        Labels](#ch05.xhtml#idm45207101849760){data-type="index:locator"}
-   [iptables command]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#idm45207100585376){data-type="index:locator"}
-   [IPv4]{data-type="index-term"},
    [ICMP](#ch10.xhtml#idm45207096032784){data-type="index:locator"}
-   [IPv6]{data-type="index-term"},
    [ICMP](#ch10.xhtml#idm45207096040448){data-type="index:locator"}
-   [irate function]{data-type="index-term"},
    [Counter](#ch13.xhtml#idm45207092749264){data-type="index:locator"},
    [irate](#ch16.xhtml#idm45207090607488){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### J

-   [Java]{data-type="index-term"}
    -   [client library]{data-type="index-term"}, [Reducing
        Load](#ch21.xhtml#idm45207087102752){data-type="index:locator"}
        -   [integration with Dropwizard]{data-type="index-term"},
            [Other Monitoring
            Systems](#ch11.xhtml#idm45207095546544){data-type="index:locator"}
    -   [CollectorRegistry.metricFamilySamples]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102308976){data-type="index:locator"}
    -   [exposition in client libraries]{data-type="index-term"},
        [Java](#ch04.xhtml#ix_Javaexpo){data-type="index:locator"}-[Pushgateway](#ch04.xhtml#idm45207102613120){data-type="index:locator"}
        -   [HttpServer class]{data-type="index-term"},
            [HTTPServer](#ch04.xhtml#idm45207103387392){data-type="index:locator"}
        -   [servlet]{data-type="index-term"},
            [Servlet](#ch04.xhtml#idm45207103130560){data-type="index:locator"}
    -   [JMX (Java Management eXtensions)]{data-type="index-term"},
        [Other Monitoring
        Systems](#ch11.xhtml#idm45207095570688){data-type="index:locator"}
    -   [labels method]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#idm45207102025872){data-type="index:locator"}
-   [jinja2 templating (Ansible)]{data-type="index-term"},
    [Static](#ch08.xhtml#idm45207100478144){data-type="index:locator"}
-   [JMX (Java Management eXtensions)]{data-type="index-term"}, [Other
    Monitoring
    Systems](#ch11.xhtml#idm45207095571408){data-type="index:locator"}
    -   [Dropwizard exposing metrics via]{data-type="index-term"},
        [Other Monitoring
        Systems](#ch11.xhtml#idm45207095542240){data-type="index:locator"}
-   [job labels]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207119394848){data-type="index:locator"},
    [Pushgateway](#ch04.xhtml#idm45207102360720){data-type="index:locator"},
    [File](#ch08.xhtml#idm45207100136400){data-type="index:locator"},
    [Grouping](#ch14.xhtml#idm45207091640608){data-type="index:locator"}
    -   [k8apiserver]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207097152784){data-type="index:locator"}
    -   [kubelet]{data-type="index-term"},
        [Node](#ch09.xhtml#idm45207097390512){data-type="index:locator"}
    -   [Kubernetes service names]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207096923136){data-type="index:locator"}
    -   [relabeling]{data-type="index-term"}, [job, instance, and
        \_\_address\_\_](#ch08.xhtml#idm45207098897008){data-type="index:locator"},
        [How to
        Scrape](#ch08.xhtml#idm45207098162896){data-type="index:locator"}
-   [job:process_cpu_seconds:rate5m]{data-type="index-term"}, [Reducing
    Cardinality](#ch17.xhtml#idm45207090192784){data-type="index:locator"}
-   [jobs]{data-type="index-term"}
    -   [duplicate]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#idm45207098163904){data-type="index:locator"}
    -   [getting average response size across all
        instances]{data-type="index-term"},
        [Summary](#ch13.xhtml#idm45207092700688){data-type="index:locator"}
-   [job_name]{data-type="index-term"},
    [Static](#ch08.xhtml#idm45207100345280){data-type="index:locator"}
-   [JSON]{data-type="index-term"},
    [File](#ch08.xhtml#idm45207100297104){data-type="index:locator"},
    [Consul
    Telemetry](#ch12.xhtml#idm45207095346160){data-type="index:locator"}
    -   [use by HTTP SD endpoints]{data-type="index-term"},
        [HTTP](#ch08.xhtml#idm45207100082288){data-type="index:locator"}
-   [JVM (Java Virtual Machine)]{data-type="index-term"},
    [Java](#ch04.xhtml#idm45207103391440){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### K

-   [keep (relabel action)]{data-type="index-term"}, [Choosing What to
    Scrape](#ch08.xhtml#idm45207099787104){data-type="index:locator"},
    [metric_relabel_configs](#ch08.xhtml#idm45207098009472){data-type="index:locator"}
-   [kill -HUP command]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090390912){data-type="index:locator"}
-   [kube-dns]{data-type="index-term"},
    [Pod](#ch09.xhtml#idm45207096784352){data-type="index:locator"}
-   [Kubelet]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097423648){data-type="index:locator"}
-   [Kubernetes]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100522768){data-type="index:locator"},
    [Kubernetes](#ch09.xhtml#ix_Kube){data-type="index:locator"}-[Alternative
    Deployments](#ch09.xhtml#idm45207096620512){data-type="index:locator"}
    -   [configuration management]{data-type="index-term"},
        [Configuration
        Management](#ch21.xhtml#idm45207087282016){data-type="index:locator"}
    -   [kube-state-metrics]{data-type="index-term"},
        [kube-state-metrics](#ch09.xhtml#idm45207096703920){data-type="index:locator"}
    -   [Prometheus deployment in,
        alternatives]{data-type="index-term"}, [Alternative
        Deployments](#ch09.xhtml#idm45207096628928){data-type="index:locator"}
    -   [running Prometheus in]{data-type="index-term"}, [Running in
        Kubernetes](#ch09.xhtml#ix_Kuberun){data-type="index:locator"}-[Running
        in
        Kubernetes](#ch09.xhtml#idm45207097434992){data-type="index:locator"}
    -   [service discovery]{data-type="index-term"}, [Service
        Discovery](#ch09.xhtml#ix_KubeSD){data-type="index:locator"}-[Ingress](#ch09.xhtml#idm45207096709264){data-type="index:locator"}
        -   [endpointslice]{data-type="index-term"},
            [Endpointslice](#ch09.xhtml#idm45207097260656){data-type="index:locator"}
        -   [ingress]{data-type="index-term"},
            [Ingress](#ch09.xhtml#idm45207096673392){data-type="index:locator"}
        -   [node]{data-type="index-term"},
            [Node](#ch09.xhtml#idm45207097422272){data-type="index:locator"}
        -   [pod]{data-type="index-term"},
            [Pod](#ch09.xhtml#idm45207096790544){data-type="index:locator"}
        -   [service role]{data-type="index-term"},
            [Service](#ch09.xhtml#idm45207097269120){data-type="index:locator"}
-   [kubernetes service]{data-type="index-term"},
    [Endpointslice](#ch09.xhtml#idm45207097194544){data-type="index:locator"}
-   [kubernetes_sd_configs]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097313648){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### L

-   [labeldrop (relabel action)]{data-type="index-term"}, [labeldrop and
    labelkeep](#ch08.xhtml#idm45207097822080){data-type="index:locator"},
    [Labels](#ch09.xhtml#idm45207097547664){data-type="index:locator"}
-   [labelkeep (relabel action)]{data-type="index-term"}, [labeldrop and
    labelkeep](#ch08.xhtml#idm45207097821440){data-type="index:locator"},
    [Labels](#ch09.xhtml#idm45207097546864){data-type="index:locator"}
-   [labelmap (relabel action)]{data-type="index-term"},
    [labelmap](#ch08.xhtml#idm45207098788416){data-type="index:locator"}
-   [labels]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119486320){data-type="index:locator"},
    [Labels](#ch05.xhtml#ix_lbls){data-type="index:locator"}-[Cardinality](#ch05.xhtml#idm45207101010880){data-type="index:locator"},
    [Labels](#ch16.xhtml#idm45207090746560){data-type="index:locator"},
    [Growing
    Prometheus](#ch21.xhtml#idm45207087489408){data-type="index:locator"}
    -   [about]{data-type="index-term"}, [What Are
        Labels?](#ch05.xhtml#idm45207102057600){data-type="index:locator"}
    -   [aggregating]{data-type="index-term"},
        [Counter](#ch13.xhtml#idm45207092730960){data-type="index:locator"}
    -   [aggregating with]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101484368){data-type="index:locator"}
    -   [alert]{data-type="index-term"}, [Alert
        Labels](#ch18.xhtml#ix_lblalrt){data-type="index:locator"}-[Alert
        Labels](#ch18.xhtml#idm45207089935104){data-type="index:locator"},
        [Annotations and
        Templates](#ch18.xhtml#idm45207089918592){data-type="index:locator"},
        [Annotations and
        Templates](#ch18.xhtml#idm45207089894592){data-type="index:locator"}
    -   [for Alertmanager]{data-type="index-term"}, [Notification
        Pipeline](#ch19.xhtml#idm45207089822400){data-type="index:locator"}
    -   [changes in, other monitoring system
        exporters]{data-type="index-term"},
        [StatsD](#ch11.xhtml#idm45207095391856){data-type="index:locator"}
    -   [clashes in and honor_labels]{data-type="index-term"}, [Label
        Clashes and
        honor_labels](#ch08.xhtml#idm45207097744272){data-type="index:locator"}
    -   [container]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097559568){data-type="index:locator"}
    -   [counting unique label values]{data-type="index-term"}, [Unique
        label
        values](#ch14.xhtml#idm45207091535216){data-type="index:locator"}
    -   [for custom collector metrics]{data-type="index-term"},
        [Labels](#ch12.xhtml#idm45207093107504){data-type="index:locator"}
    -   [displaying label values in Grafana]{data-type="index-term"},
        [Stat
        Panel](#ch06.xhtml#idm45207100886080){data-type="index:locator"}
    -   [exposition format, escaping characters in
        values]{data-type="index-term"},
        [Escaping](#ch04.xhtml#idm45207102137520){data-type="index:locator"}
    -   [Graphite bridge]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102346032){data-type="index:locator"}
    -   [grouping key for metrics from push to
        Pushgateway]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102360112){data-type="index:locator"}
    -   [instrumentation]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#ix_lblsinstr){data-type="index:locator"}-[Aggregating](#ch05.xhtml#idm45207101494880){data-type="index:locator"}
    -   [instrumentation and target]{data-type="index-term"},
        [Instrumentation and Target
        Labels](#ch05.xhtml#idm45207102045600){data-type="index:locator"}
    -   [limiting by, using selectors]{data-type="index-term"},
        [Selectors](#ch13.xhtml#idm45207092648128){data-type="index:locator"}
    -   [metric names and]{data-type="index-term"},
        [Name](#ch03.xhtml#idm45207116441520){data-type="index:locator"}
    -   [naming]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#idm45207101961360){data-type="index:locator"}
    -   [not using recording rules to fix bad
        labels]{data-type="index-term"}, [How Not to Use
        Rules](#ch17.xhtml#idm45207090137776){data-type="index:locator"}
    -   [in OpenMetrics format]{data-type="index-term"},
        [Labels](#ch04.xhtml#idm45207102080640){data-type="index:locator"}
    -   [patterns in]{data-type="index-term"}, [Label
        Patterns](#ch05.xhtml#ix_lblspatt){data-type="index:locator"}-[Info](#ch05.xhtml#idm45207101044704){data-type="index:locator"}
        -   [breaking changes and labels]{data-type="index-term"},
            [Info](#ch05.xhtml#idm45207101051120){data-type="index:locator"}
        -   [enum]{data-type="index-term"},
            [Enum](#ch05.xhtml#idm45207101421936){data-type="index:locator"}
        -   [info]{data-type="index-term"},
            [Info](#ch05.xhtml#idm45207101159360){data-type="index:locator"}
    -   [in Prometheus text exposition format]{data-type="index-term"},
        [Labels](#ch04.xhtml#idm45207102149920){data-type="index:locator"}
    -   [provided for targets by static config]{data-type="index-term"},
        [Static](#ch08.xhtml#idm45207100447360){data-type="index:locator"}
    -   [recording rules undoing benefits of]{data-type="index-term"},
        [How Not to Use
        Rules](#ch17.xhtml#idm45207090153024){data-type="index:locator"}
    -   [relabeling and]{data-type="index-term"}, [Service
        Discovery](#ch01.xhtml#idm45207118465232){data-type="index:locator"}
    -   [removing any you don\'t know about with by
        clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091591920){data-type="index:locator"}
    -   [set by Consul]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093828064){data-type="index:locator"}
    -   [specifying in labels field of rule
        file]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#idm45207090274960){data-type="index:locator"}
    -   [specifying to keep using by clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091609408){data-type="index:locator"}
    -   [target labels]{data-type="index-term"}, [Service
        Discovery](#ch08.xhtml#idm45207100512160){data-type="index:locator"}
    -   [use in exporters, gotchas]{data-type="index-term"},
        [Guidelines](#ch12.xhtml#idm45207092829360){data-type="index:locator"}
    -   [when to use]{data-type="index-term"}, [When to Use
        Labels](#ch05.xhtml#ix_lblsuse){data-type="index:locator"}-[Cardinality](#ch05.xhtml#idm45207101012128){data-type="index:locator"}
-   [labels method]{data-type="index-term"},
    [Instrumentation](#ch05.xhtml#idm45207102028160){data-type="index:locator"},
    [Child](#ch05.xhtml#idm45207101793616){data-type="index:locator"}
-   [label_join function]{data-type="index-term"},
    [label_join](#ch16.xhtml#idm45207090727104){data-type="index:locator"}
-   [label_replace function]{data-type="index-term"},
    [label_replace](#ch16.xhtml#idm45207090739728){data-type="index:locator"}
-   [last_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090476384){data-type="index:locator"}
-   [latency]{data-type="index-term"}
    -   [calculating average latency]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115725344){data-type="index:locator"}
    -   [latency SLAs and quantiles]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115734176){data-type="index:locator"}
    -   [logging for HTTP requests using Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096211824){data-type="index:locator"}
    -   [tracking for Hello World program
        (example)]{data-type="index-term"}, [The
        Summary](#ch03.xhtml#idm45207113547312){data-type="index:locator"}
-   [latency_too_high_threshold]{data-type="index-term"}, [Notification
    templates](#ch19.xhtml#idm45207088067648){data-type="index:locator"}
-   [le labels]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102158864){data-type="index:locator"},
    [metric_relabel_configs](#ch08.xhtml#idm45207097873440){data-type="index:locator"},
    [Histogram](#ch13.xhtml#idm45207092681600){data-type="index:locator"}
-   [least-squares regression]{data-type="index-term"},
    [deriv](#ch16.xhtml#idm45207090551808){data-type="index:locator"}
-   [left-associative (operators)]{data-type="index-term"}, [Operator
    Precedence](#ch15.xhtml#idm45207091011808){data-type="index:locator"}
-   [level (recording rule names)]{data-type="index-term"}, [Naming of
    Recording
    Rules](#ch17.xhtml#idm45207090114848){data-type="index:locator"}
-   [libraries]{data-type="index-term"}
    -   [instrumentation]{data-type="index-term"}, [Library
        instrumentation](#ch03.xhtml#idm45207116503024){data-type="index:locator"},
        [Exposition](#ch04.xhtml#idm45207116402896){data-type="index:locator"}
    -   [in metric names]{data-type="index-term"},
        [Library](#ch03.xhtml#idm45207116437648){data-type="index:locator"}
-   [Linux]{data-type="index-term"}
    -   [metrics on offer]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100788528){data-type="index:locator"}
    -   [profiling of kernel events]{data-type="index-term"},
        [Profiling](#ch01.xhtml#idm45207119682880){data-type="index:locator"}
-   [lists]{data-type="index-term"}
    -   [list comprehensions]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115849264){data-type="index:locator"}
    -   [produced by service discovery,
        relabeling]{data-type="index-term"},
        [Lists](#ch08.xhtml#idm45207098510000){data-type="index:locator"}
-   [ln function]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091509344){data-type="index:locator"},
    [ln, log2, and
    log10](#ch16.xhtml#idm45207090938880){data-type="index:locator"}
-   [load, reducing]{data-type="index-term"}, [Reducing
    Load](#ch21.xhtml#idm45207087110624){data-type="index:locator"}
-   [loadavg collector]{data-type="index-term"}, [Loadavg
    Collector](#ch07.xhtml#idm45207100616208){data-type="index:locator"}
-   [log10 function]{data-type="index-term"}, [ln, log2, and
    log10](#ch16.xhtml#idm45207090937536){data-type="index:locator"}
-   [log2 function]{data-type="index-term"}, [ln, log2, and
    log10](#ch16.xhtml#idm45207090938208){data-type="index:locator"}
-   [logging]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207114292544){data-type="index:locator"}
    -   [categories of]{data-type="index-term"},
        [Logging](#ch01.xhtml#idm45207114290224){data-type="index:locator"}
    -   [converting logs to metrics using Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096445392){data-type="index:locator"}
-   [logging systems]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207118496672){data-type="index:locator"}
-   [logical operators]{data-type="index-term"}, [Many-to-Many and
    Logical
    Operators](#ch15.xhtml#ix_logop){data-type="index:locator"}-[and
    operator](#ch15.xhtml#idm45207091038624){data-type="index:locator"}
    -   [and]{data-type="index-term"}, [and
        operator](#ch15.xhtml#idm45207091052864){data-type="index:locator"}
    -   [or]{data-type="index-term"}, [or
        operator](#ch15.xhtml#idm45207091107936){data-type="index:locator"}
    -   [unless]{data-type="index-term"}, [unless
        operator](#ch15.xhtml#idm45207091067696){data-type="index:locator"}
-   [Long Term Support (LTS) releases]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207113421296){data-type="index:locator"}
-   [long-term storage]{data-type="index-term"}, [Long-Term
    Storage](#ch01.xhtml#idm45207118957952){data-type="index:locator"},
    [Long-Term
    Storage](#ch21.xhtml#ix_LTstore){data-type="index:locator"}-[Long-Term
    Storage](#ch21.xhtml#idm45207087320752){data-type="index:locator"}
-   [lowercase (relabel action)]{data-type="index-term"},
    [Case](#ch08.xhtml#idm45207098651776){data-type="index:locator"}
-   [LTS]{data-type="index-term"} ([see]{gentext="see"} long-term
    storage)
-   [LTS (Long Term Support) releases]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207113420528){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### M

-   [machine roles approach]{data-type="index-term"},
    [Info](#ch05.xhtml#idm45207101160032){data-type="index:locator"}
-   [Management Information Base (MIBs)]{data-type="index-term"}, [Other
    Monitoring
    Systems](#ch11.xhtml#idm45207095567440){data-type="index:locator"}
-   [many-to-many vector matching]{data-type="index-term"},
    [Many-to-Many and Logical
    Operators](#ch15.xhtml#ix_mnymny){data-type="index:locator"}-[and
    operator](#ch15.xhtml#idm45207091035040){data-type="index:locator"}
-   [many-to-one vector matching]{data-type="index-term"}, [Many-to-One
    and
    group_left](#ch15.xhtml#ix_mnyone){data-type="index:locator"}-[Many-to-One
    and
    group_left](#ch15.xhtml#idm45207091125728){data-type="index:locator"}
-   [ManyInstancesDown alert]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090043200){data-type="index:locator"}
-   [matchers]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092636992){data-type="index:locator"},
    [Routing
    Tree](#ch19.xhtml#idm45207089650000){data-type="index:locator"}
    -   [error to use on default route]{data-type="index-term"},
        [Routing
        Tree](#ch19.xhtml#idm45207089518688){data-type="index:locator"}
-   [match_re]{data-type="index-term"},
    [Inhibitions](#ch19.xhtml#idm45207087914016){data-type="index:locator"}
-   [math functions]{data-type="index-term"},
    [Math](#ch16.xhtml#ix_math){data-type="index:locator"}-[Time and
    Date](#ch16.xhtml#idm45207090835984){data-type="index:locator"}
    -   [abs]{data-type="index-term"},
        [abs](#ch16.xhtml#idm45207090950112){data-type="index:locator"}
    -   [ceil and floor]{data-type="index-term"}, [ceil and
        floor](#ch16.xhtml#idm45207090902816){data-type="index:locator"}
    -   [clamp, clamp_max, and clamp_min]{data-type="index-term"},
        [clamp, clamp_max, and
        clamp_min](#ch16.xhtml#idm45207090882272){data-type="index:locator"}
    -   [exp]{data-type="index-term"},
        [exp](#ch16.xhtml#idm45207090926272){data-type="index:locator"}
    -   [ln, log2, and log10]{data-type="index-term"}, [ln, log2, and
        log10](#ch16.xhtml#idm45207090941104){data-type="index:locator"}
    -   [round]{data-type="index-term"},
        [round](#ch16.xhtml#idm45207090896848){data-type="index:locator"}
    -   [sgn]{data-type="index-term"},
        [sgn](#ch16.xhtml#idm45207090872176){data-type="index:locator"}
    -   [sqrt]{data-type="index-term"},
        [sqrt](#ch16.xhtml#idm45207090917184){data-type="index:locator"}
    -   [trigonometric functions]{data-type="index-term"},
        [Trigonometric
        Functions](#ch16.xhtml#idm45207090865248){data-type="index:locator"}
-   [matrix]{data-type="index-term"} ([see]{gentext="see"} range vector
    selector)
-   [max]{data-type="index-term"}, [min and
    max](#ch14.xhtml#idm45207091455872){data-type="index:locator"}
    -   [using with gauges]{data-type="index-term"},
        [Gauge](#ch13.xhtml#idm45207092768736){data-type="index:locator"}
-   [max_over_time]{data-type="index-term"},
    [Subqueries](#ch13.xhtml#idm45207092520448){data-type="index:locator"},
    [Aggregation Over
    Time](#ch16.xhtml#idm45207090491968){data-type="index:locator"},
    [Composing Range Vector
    Functions](#ch17.xhtml#idm45207090176608){data-type="index:locator"}
-   [mean]{data-type="index-term"},
    [avg](#ch14.xhtml#idm45207091512624){data-type="index:locator"}
-   [meminfo collector]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100684864){data-type="index:locator"}
-   [memory]{data-type="index-term"}
    -   [container metrics for]{data-type="index-term"},
        [Memory](#ch09.xhtml#idm45207097579024){data-type="index:locator"}
    -   [memory usage graph in Grafana]{data-type="index-term"}, [Time
        Series
        Panel](#ch06.xhtml#idm45207100930256){data-type="index:locator"}
    -   [results of
        process_resident_memory_bytes]{data-type="index-term"}, [Using
        the Expression
        Browser](#ch02.xhtml#idm45207119387920){data-type="index:locator"}
    -   [usage by Prometheus and Node Exporter]{data-type="index-term"},
        [Running the Node
        Exporter](#ch02.xhtml#idm45207118810432){data-type="index:locator"}
-   [metadata]{data-type="index-term"}
    -   [labels for Docker containers]{data-type="index-term"},
        [Labels](#ch09.xhtml#idm45207097552496){data-type="index:locator"}
    -   [mapping to targets using relabeling]{data-type="index-term"},
        [Relabeling](#ch08.xhtml#idm45207099797808){data-type="index:locator"}
    -   [node service discovery in Kubernetes]{data-type="index-term"},
        [Node](#ch09.xhtml#idm45207097310464){data-type="index:locator"}
    -   [provided by service discovery]{data-type="index-term"},
        [Service Discovery
        Mechanisms](#ch08.xhtml#idm45207100499328){data-type="index:locator"}
    -   [target discovered by EC2 SD]{data-type="index-term"},
        [EC2](#ch08.xhtml#idm45207099813312){data-type="index:locator"}
    -   [viewing for target labels in file SD]{data-type="index-term"},
        [File](#ch08.xhtml#idm45207100157568){data-type="index:locator"}
-   [metamonitoring]{data-type="index-term"}, [Meta- and
    Cross-Monitoring](#ch21.xhtml#idm45207087175952){data-type="index:locator"}
-   [method labels]{data-type="index-term"},
    [Aggregating](#ch05.xhtml#idm45207101480064){data-type="index:locator"}
    -   [removing using sum without]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101440976){data-type="index:locator"}
-   [metric (recording rule names)]{data-type="index-term"}, [Naming of
    Recording
    Rules](#ch17.xhtml#idm45207090111664){data-type="index:locator"}
-   [metric family]{data-type="index-term"},
    [Metric](#ch05.xhtml#idm45207101858304){data-type="index:locator"}
-   [metrics]{data-type="index-term"},
    [Metrics](#ch01.xhtml#idm45207118494352){data-type="index:locator"}
    -   [alerts on]{data-type="index-term"}, [What Are Good
        Alerts?](#ch18.xhtml#idm45207089881456){data-type="index:locator"}
    -   [automatic registration with client
        library]{data-type="index-term"}, [The
        Counter](#ch03.xhtml#idm45207117181968){data-type="index:locator"}
    -   [configuring types to collect with Node
        Exporter]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100781136){data-type="index:locator"}
    -   [Consul]{data-type="index-term"}, [Consul
        Telemetry](#ch12.xhtml#idm45207095347104){data-type="index:locator"}
    -   [from Consul Exporter]{data-type="index-term"},
        [Consul](#ch10.xhtml#idm45207096597472){data-type="index:locator"}
    -   [container]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#idm45207097655472){data-type="index:locator"}
        -   [CPU]{data-type="index-term"},
            [CPU](#ch09.xhtml#idm45207097617936){data-type="index:locator"}
    -   [converting logs to with Grok Exporter]{data-type="index-term"},
        [Grok
        Exporter](#ch10.xhtml#idm45207096446304){data-type="index:locator"}
    -   [definitions of]{data-type="index-term"}, [The
        Counter](#ch03.xhtml#idm45207117183552){data-type="index:locator"}
    -   [details handled by client libraries]{data-type="index-term"},
        [Client
        Libraries](#ch01.xhtml#idm45207113058672){data-type="index:locator"}
    -   [from exporters]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100790256){data-type="index:locator"}
    -   [exposed by Node Exporter]{data-type="index-term"}, [Running the
        Node
        Exporter](#ch02.xhtml#idm45207118076192){data-type="index:locator"},
        [Node
        Exporter](#ch07.xhtml#idm45207100777216){data-type="index:locator"}
    -   [exposition to Prometheus]{data-type="index-term"},
        [Exposition](#ch04.xhtml#idm45207116411168){data-type="index:locator"}
    -   [finding expensive metrics]{data-type="index-term"}, [Finding
        Expensive Metrics and
        Targets](#ch21.xhtml#idm45207087134368){data-type="index:locator"}
    -   [from Grok Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096217536){data-type="index:locator"},
        [Grok
        Exporter](#ch10.xhtml#idm45207096205664){data-type="index:locator"}
    -   [from Java client libraries]{data-type="index-term"},
        [HTTPServer](#ch04.xhtml#idm45207103295376){data-type="index:locator"}
    -   [from kube-state-metrics]{data-type="index-term"},
        [kube-state-metrics](#ch09.xhtml#idm45207096706112){data-type="index:locator"}
    -   [limits on Prometheus\' handling of]{data-type="index-term"},
        [How Much Should I
        Instrument?](#ch03.xhtml#idm45207116490080){data-type="index:locator"}
    -   [machine-level from other monitoring
        systems]{data-type="index-term"}, [Planning a
        Rollout](#ch21.xhtml#idm45207087555600){data-type="index:locator"}
    -   [/metrics page of Prometheus]{data-type="index-term"}, [Running
        Prometheus](#ch02.xhtml#idm45207116154112){data-type="index:locator"}
    -   [/metrics path]{data-type="index-term"},
        [WSGI](#ch04.xhtml#idm45207116244304){data-type="index:locator"}
    -   [from MySQLd Exporter]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096487440){data-type="index:locator"}
    -   [naming]{data-type="index-term"}, [What Should I Name My
        Metrics?](#ch03.xhtml#ix_mtrcname){data-type="index:locator"}-[Library](#ch03.xhtml#idm45207116416960){data-type="index:locator"},
        [Naming of Recording
        Rules](#ch17.xhtml#idm45207090082736){data-type="index:locator"}
    -   [not using recording rules to fix metric
        names]{data-type="index-term"}, [How Not to Use
        Rules](#ch17.xhtml#idm45207090136704){data-type="index:locator"}
    -   [problems from too much cardinality]{data-type="index-term"},
        [Growing
        Prometheus](#ch21.xhtml#idm45207087487296){data-type="index:locator"}
    -   [produced by Blackbox probes]{data-type="index-term"},
        [ICMP](#ch10.xhtml#idm45207096063648){data-type="index:locator"}
    -   [Pushgateway showing from Python batch
        job]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102420432){data-type="index:locator"}
    -   [relabeling]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207098020080){data-type="index:locator"}
    -   [for simple HTTP server in Python
        (example)]{data-type="index-term"}, [A Simple
        Program](#ch03.xhtml#idm45207116009904){data-type="index:locator"}
    -   [suffixes]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113643136){data-type="index:locator"}
    -   [total and failures, not success and
        failures]{data-type="index-term"}, [Library
        instrumentation](#ch03.xhtml#idm45207116497616){data-type="index:locator"}
    -   [types in OpenMetrics format]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102100016){data-type="index:locator"}
    -   [types in Prometheus text format]{data-type="index-term"},
        [Metric
        Types](#ch04.xhtml#idm45207102180048){data-type="index:locator"}
    -   [use of in monitoring systems]{data-type="index-term"}, [How Not
        to Use
        Rules](#ch17.xhtml#idm45207090147392){data-type="index:locator"}
-   [metrics (pod port)]{data-type="index-term"},
    [Pod](#ch09.xhtml#idm45207096782704){data-type="index:locator"}
-   [metrics.Samples]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207093773200){data-type="index:locator"}
-   [MetricsServlet class]{data-type="index-term"},
    [Servlet](#ch04.xhtml#idm45207103128704){data-type="index:locator"}
-   [metrics_app]{data-type="index-term"},
    [WSGI](#ch04.xhtml#idm45207116375824){data-type="index:locator"}
-   [metrics_path]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098254432){data-type="index:locator"},
    [Node](#ch09.xhtml#idm45207097300992){data-type="index:locator"}
-   [metric_relabel_configs]{data-type="index-term"},
    [metric_relabel_configs](#ch08.xhtml#idm45207098020784){data-type="index:locator"},
    [Labels](#ch16.xhtml#idm45207090743408){data-type="index:locator"},
    [Reducing
    Load](#ch21.xhtml#idm45207087106784){data-type="index:locator"}
    -   [versus relabel_configs]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207098011968){data-type="index:locator"}
-   [MIBs (Management Information Base)]{data-type="index-term"}, [Other
    Monitoring
    Systems](#ch11.xhtml#idm45207095566704){data-type="index:locator"}
-   [milliseconds]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092552768){data-type="index:locator"}
-   [min]{data-type="index-term"}, [min and
    max](#ch14.xhtml#idm45207091456544){data-type="index:locator"}
-   [Minikube]{data-type="index-term"},
    [Endpointslice](#ch09.xhtml#idm45207097195152){data-type="index:locator"}
-   [minute function]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090812928){data-type="index:locator"}
-   [minutes]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092550384){data-type="index:locator"}
-   [min_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090492640){data-type="index:locator"}
-   [missing series]{data-type="index-term"}, [Missing Series, absent,
    and
    absent_over_time](#ch16.xhtml#idm45207090713392){data-type="index:locator"}
-   [mmap utility]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207114246160){data-type="index:locator"}
-   [mode labels]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100766256){data-type="index:locator"},
    [One-to-One](#ch15.xhtml#idm45207091209104){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091176240){data-type="index:locator"}
    -   [counting without]{data-type="index-term"}, [Unique label
        values](#ch14.xhtml#idm45207091529392){data-type="index:locator"}
-   [modulo operator (%)]{data-type="index-term"}, [Arithmetic
    Operators](#ch15.xhtml#idm45207091322256){data-type="index:locator"}
-   [modulus setting]{data-type="index-term"}, [Horizontal
    Sharding](#ch21.xhtml#idm45207087078704){data-type="index:locator"}
-   [monitoring]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#ix_mntr){data-type="index:locator"}-[Metrics](#ch01.xhtml#idm45207118487776){data-type="index:locator"}
    -   [about]{data-type="index-term"}, [What Is
        Monitoring?](#ch01.xhtml#idm45207119270816){data-type="index:locator"}
    -   [brief history of]{data-type="index-term"}, [A Brief and
        Incomplete History of
        Monitoring](#ch01.xhtml#idm45207119491296){data-type="index:locator"}
    -   [categories of]{data-type="index-term"}, [Categories of
        Monitoring](#ch01.xhtml#idm45207119582624){data-type="index:locator"}
    -   [cross-monitoring]{data-type="index-term"}, [Meta- and
        Cross-Monitoring](#ch21.xhtml#idm45207087170592){data-type="index:locator"}
    -   [metamonitoring]{data-type="index-term"}, [Meta- and
        Cross-Monitoring](#ch21.xhtml#idm45207087175280){data-type="index:locator"}
-   [monitoring systems (other)]{data-type="index-term"}, [Working with
    Other Monitoring
    Systems](#ch11.xhtml#ix_monsys){data-type="index:locator"}-[StatsD](#ch11.xhtml#idm45207095362016){data-type="index:locator"}
    -   [about]{data-type="index-term"}, [Other Monitoring
        Systems](#ch11.xhtml#ix_monsysabt){data-type="index:locator"}-[Other
        Monitoring
        Systems](#ch11.xhtml#idm45207095541056){data-type="index:locator"}
    -   [existing instrumentation from]{data-type="index-term"},
        [Planning a
        Rollout](#ch21.xhtml#idm45207087505312){data-type="index:locator"}
    -   [having parsers for Prometheus text
        format]{data-type="index-term"},
        [Parsers](#ch04.xhtml#idm45207102219104){data-type="index:locator"}
    -   [InfluxDB]{data-type="index-term"},
        [InfluxDB](#ch11.xhtml#idm45207095537280){data-type="index:locator"}
    -   [Prometheus integration with]{data-type="index-term"}, [Other
        Monitoring
        Systems](#ch11.xhtml#idm45207095548208){data-type="index:locator"}
    -   [StatsD]{data-type="index-term"},
        [StatsD](#ch11.xhtml#ix_monsysSD){data-type="index:locator"}-[StatsD](#ch11.xhtml#idm45207095389776){data-type="index:locator"}
-   [month function]{data-type="index-term"}, [minute, hour,
    day_of_week, day_of_month, day_of_year, days_in_month, month, and
    year](#ch16.xhtml#idm45207090809568){data-type="index:locator"}
-   [mounted filesystems, metrics on]{data-type="index-term"},
    [Filesystem
    Collector](#ch07.xhtml#idm45207100750176){data-type="index:locator"}
-   [mountpoint labels]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100742224){data-type="index:locator"},
    [Gauge](#ch13.xhtml#idm45207092782768){data-type="index:locator"},
    [Grouping](#ch14.xhtml#idm45207091642480){data-type="index:locator"}
-   [mtime]{data-type="index-term"},
    [Timestamps](#ch07.xhtml#idm45207100537344){data-type="index:locator"}
-   [multiple labels for a metric]{data-type="index-term"}, [Multiple
    Labels](#ch05.xhtml#idm45207101848768){data-type="index:locator"}
-   [multiplication, 1 as identity element for]{data-type="index-term"},
    [or
    operator](#ch15.xhtml#idm45207091092640){data-type="index:locator"}
-   [multiprocess deployments]{data-type="index-term"},
    [StatsD](#ch11.xhtml#idm45207095396544){data-type="index:locator"}
-   [multiprocess mode with Gunicorn]{data-type="index-term"},
    [Multiprocess with
    Gunicorn](#ch04.xhtml#ix_mltiprc){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114228048){data-type="index:locator"}
-   [MultiProcessCollector]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207118292976){data-type="index:locator"}
-   [multiprocess_mode configuration (gauges)]{data-type="index-term"},
    [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207118202224){data-type="index:locator"}
-   [MustNewConstMetric function]{data-type="index-term"},
    [Exporters](#ch01.xhtml#idm45207118478096){data-type="index:locator"},
    [Custom
    Collectors](#ch12.xhtml#idm45207094259904){data-type="index:locator"},
    [Custom
    Collectors](#ch12.xhtml#idm45207093695040){data-type="index:locator"}
    -   [specifying label values in]{data-type="index-term"},
        [Labels](#ch12.xhtml#idm45207093105648){data-type="index:locator"}
-   [MustRegister function]{data-type="index-term"},
    [Go](#ch04.xhtml#idm45207103402960){data-type="index:locator"}
-   [MySQLd Exporter]{data-type="index-term"},
    [MySQLd](#ch10.xhtml#ix_MySQL){data-type="index:locator"}-[Grok
    Exporter](#ch10.xhtml#idm45207096449408){data-type="index:locator"}
    -   [configuring for scraping by
        Prometheus]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096477040){data-type="index:locator"}
    -   [downloading and running]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096496064){data-type="index:locator"}
    -   [metrics from]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096488384){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### N

-   [Nagios]{data-type="index-term"}, [A Brief and Incomplete History of
    Monitoring](#ch01.xhtml#idm45207119809600){data-type="index:locator"}
-   [Nagios Remote Program Execution (NRPE)]{data-type="index-term"},
    [Other Monitoring
    Systems](#ch11.xhtml#idm45207095551088){data-type="index:locator"}
-   [name (metrics)]{data-type="index-term"},
    [Name](#ch03.xhtml#idm45207116449072){data-type="index:locator"}
-   [name labels]{data-type="index-term"},
    [Labels](#ch09.xhtml#idm45207097554192){data-type="index:locator"},
    [Matchers](#ch13.xhtml#idm45207092602432){data-type="index:locator"}
-   [namespace labels]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097300256){data-type="index:locator"}
-   [naming labels]{data-type="index-term"},
    [Instrumentation](#ch05.xhtml#idm45207101960384){data-type="index:locator"}
-   [naming recording rules]{data-type="index-term"}, [Naming of
    Recording
    Rules](#ch17.xhtml#ix_namerecru){data-type="index:locator"}-[Naming
    of Recording
    Rules](#ch17.xhtml#idm45207090077984){data-type="index:locator"}
-   [NaN (not a number)]{data-type="index-term"}, [Counting
    Exceptions](#ch03.xhtml#idm45207113259520){data-type="index:locator"},
    [or
    operator](#ch15.xhtml#idm45207091091872){data-type="index:locator"}
    -   [input to avg operator]{data-type="index-term"},
        [avg](#ch14.xhtml#idm45207091499872){data-type="index:locator"}
    -   [return by max and min]{data-type="index-term"}, [min and
        max](#ch14.xhtml#idm45207091447696){data-type="index:locator"}
    -   [sorting and]{data-type="index-term"}, [Sorting with sort and
        sort_desc](#ch16.xhtml#idm45207090671552){data-type="index:locator"}
-   [Native Histograms (experimental feature)]{data-type="index-term"},
    [Buckets](#ch03.xhtml#idm45207115773824){data-type="index:locator"}
-   [natural logarithm]{data-type="index-term"}, [ln, log2, and
    log10](#ch16.xhtml#idm45207090936864){data-type="index:locator"}
-   [negative equality matcher (!=)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092625232){data-type="index:locator"}
-   [negative regular expression matcher (!\~)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092612480){data-type="index:locator"}
-   [NetBox]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100517584){data-type="index:locator"}
-   [netdev collector]{data-type="index-term"}, [Netdev
    Collector](#ch07.xhtml#idm45207100696576){data-type="index:locator"}
-   [network bandwidth]{data-type="index-term"},
    [Hardware](#ch21.xhtml#idm45207087294048){data-type="index:locator"}
-   [networks]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#ix_netw){data-type="index:locator"}-[Networks
    and
    Authentication](#ch21.xhtml#idm45207087232304){data-type="index:locator"}
-   [New dashboard (Grafana)]{data-type="index-term"}, [Time Series
    Panel](#ch06.xhtml#idm45207100919328){data-type="index:locator"}
-   [NewCounter]{data-type="index-term"},
    [Go](#ch04.xhtml#idm45207103402288){data-type="index:locator"}
-   [NewDesc]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094274032){data-type="index:locator"}
-   [nice mode]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100756480){data-type="index:locator"}
-   [Node Exporter]{data-type="index-term"}, [Running the Node
    Exporter](#ch02.xhtml#ix_NdeExp){data-type="index:locator"}-[Running
    the Node
    Exporter](#ch02.xhtml#idm45207118797472){data-type="index:locator"},
    [Node
    Exporter](#ch07.xhtml#ix_NdE){data-type="index:locator"}-[Timestamps](#ch07.xhtml#idm45207100530672){data-type="index:locator"},
    [Planning a
    Rollout](#ch21.xhtml#idm45207087559232){data-type="index:locator"}
    -   [configuring Prometheus to monitor]{data-type="index-term"},
        [Running the Node
        Exporter](#ch02.xhtml#idm45207118062032){data-type="index:locator"}
    -   [cpu collector]{data-type="index-term"}, [CPU
        Collector](#ch07.xhtml#idm45207100768816){data-type="index:locator"}
    -   [diskstats collector]{data-type="index-term"}, [Diskstats
        Collector](#ch07.xhtml#idm45207100725392){data-type="index:locator"}
    -   [downloading and installing]{data-type="index-term"}, [Running
        the Node
        Exporter](#ch02.xhtml#idm45207118069472){data-type="index:locator"}
    -   [filesystem collector]{data-type="index-term"}, [Filesystem
        Collector](#ch07.xhtml#idm45207100748864){data-type="index:locator"}
    -   [hwmon collector]{data-type="index-term"}, [Hwmon
        Collector](#ch07.xhtml#idm45207100667264){data-type="index:locator"}
    -   [loadavg collector]{data-type="index-term"}, [Loadavg
        Collector](#ch07.xhtml#idm45207100615472){data-type="index:locator"}
    -   [meminfo collector]{data-type="index-term"}, [Meminfo
        Collector](#ch07.xhtml#idm45207100684128){data-type="index:locator"}
    -   [netdev collector]{data-type="index-term"}, [Netdev
        Collector](#ch07.xhtml#idm45207100695872){data-type="index:locator"}
    -   [node_filesystem_size_bytes metric]{data-type="index-term"},
        [Gauge](#ch13.xhtml#idm45207092782096){data-type="index:locator"}
    -   [OS collector]{data-type="index-term"}, [OS
        Collector](#ch07.xhtml#idm45207100624464){data-type="index:locator"}
    -   [pressure collector]{data-type="index-term"}, [Pressure
        Collector](#ch07.xhtml#idm45207100605024){data-type="index:locator"}
    -   [running with Consul]{data-type="index-term"},
        [Consul](#ch08.xhtml#idm45207099916176){data-type="index:locator"}
    -   [running within Docker]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100786016){data-type="index:locator"}
    -   [stat collector]{data-type="index-term"}, [Stat
        Collector](#ch07.xhtml#idm45207100651456){data-type="index:locator"}
    -   [textfile collector]{data-type="index-term"}, [Textfile
        Collector](#ch07.xhtml#ix_NdEtxtfl){data-type="index:locator"}-[Timestamps](#ch07.xhtml#idm45207100532864){data-type="index:locator"}
    -   [uname collector]{data-type="index-term"}, [Uname
        Collector](#ch07.xhtml#idm45207100636272){data-type="index:locator"}
    -   [version 1.4.0 with 5.18.0 Linux kernel, metrics
        from]{data-type="index-term"}, [Node
        Exporter](#ch07.xhtml#idm45207100776176){data-type="index:locator"}
-   [node service discovery]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097422944){data-type="index:locator"}
-   [nodename label]{data-type="index-term"}, [Uname
    Collector](#ch07.xhtml#idm45207100631456){data-type="index:locator"}
-   [node_boot_time_seconds]{data-type="index-term"}, [Stat
    Collector](#ch07.xhtml#idm45207100648544){data-type="index:locator"}
-   [node_cpu_guest_seconds_total]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100757856){data-type="index:locator"}
-   [node_cpu_seconds_total]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100770224){data-type="index:locator"}
-   [node_disk_io_time_seconds_total]{data-type="index-term"},
    [Diskstats
    Collector](#ch07.xhtml#idm45207100702768){data-type="index:locator"}
-   [node_filesystem prefix (metrics)]{data-type="index-term"},
    [Filesystem
    Collector](#ch07.xhtml#idm45207100744304){data-type="index:locator"}
-   [node_filesystem_avail_bytes versus
    node_filesystem_free_bytes]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100736848){data-type="index:locator"}
-   [node_filesystem_files]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100730736){data-type="index:locator"}
-   [node_filesystem_files_free]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100731536){data-type="index:locator"}
-   [node_filesystem_size_bytes]{data-type="index-term"},
    [Grouping](#ch14.xhtml#idm45207091649232){data-type="index:locator"}
    -   [aggregation of]{data-type="index-term"},
        [without](#ch14.xhtml#idm45207091620400){data-type="index:locator"}
-   [node_hwmon prefix (metrics)]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100669360){data-type="index:locator"}
-   [node_hwmon_sensor_label]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100660720){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091144752){data-type="index:locator"}
    -   [using or operator to substitute missing time
        series]{data-type="index-term"}, [or
        operator](#ch15.xhtml#idm45207091103344){data-type="index:locator"}
-   [node_hwmon_temp_celsius]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100662128){data-type="index:locator"}
-   [node_intr_total]{data-type="index-term"}, [Stat
    Collector](#ch07.xhtml#idm45207100645648){data-type="index:locator"}
-   [node_memory_Buffers_bytes]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100676752){data-type="index:locator"}
-   [node_memory_Cached_bytes]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100676032){data-type="index:locator"}
-   [node_memory_MemAvailable]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100672736){data-type="index:locator"}
-   [node_memory_MemFree_bytes]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100679200){data-type="index:locator"}
-   [node_memory_MemTotal_bytes]{data-type="index-term"}, [Meminfo
    Collector](#ch07.xhtml#idm45207100678432){data-type="index:locator"}
-   [node_network prefix (metrics)]{data-type="index-term"}, [Netdev
    Collector](#ch07.xhtml#idm45207100694928){data-type="index:locator"}
-   [node_network_receive_bytes_total]{data-type="index-term"}, [Netdev
    Collector](#ch07.xhtml#idm45207100689872){data-type="index:locator"}
-   [node_network_transmit_bytes_total]{data-type="index-term"}, [Netdev
    Collector](#ch07.xhtml#idm45207100690608){data-type="index:locator"}
-   [node_os_info]{data-type="index-term"}, [OS
    Collector](#ch07.xhtml#idm45207100623520){data-type="index:locator"}
-   [node_os_version]{data-type="index-term"}, [OS
    Collector](#ch07.xhtml#idm45207100622848){data-type="index:locator"}
-   [node_uname_info]{data-type="index-term"}, [Uname
    Collector](#ch07.xhtml#idm45207100637680){data-type="index:locator"}
-   [notifications]{data-type="index-term"},
    [Alerting](#ch18.xhtml#idm45207090060880){data-type="index:locator"},
    [Notification
    Pipeline](#ch19.xhtml#idm45207089800704){data-type="index:locator"}
    -   [Alertmanager notification pipeline]{data-type="index-term"},
        [Notification
        Pipeline](#ch19.xhtml#idm45207089823488){data-type="index:locator"}
    -   [resolved]{data-type="index-term"}, [Resolved
        notifications](#ch19.xhtml#idm45207087937248){data-type="index:locator"}
    -   [sending only at certain times for an
        alert]{data-type="index-term"}, [Alerting
        Rules](#ch18.xhtml#idm45207090017808){data-type="index:locator"}
    -   [templating]{data-type="index-term"}, [Annotations and
        Templates](#ch18.xhtml#idm45207089900720){data-type="index:locator"},
        [Notification
        templates](#ch19.xhtml#ix_notitmpl){data-type="index:locator"}-[Notification
        templates](#ch19.xhtml#idm45207087989408){data-type="index:locator"}
-   [notifiers]{data-type="index-term"},
    [Receivers](#ch19.xhtml#ix_notify){data-type="index:locator"}-[Receivers](#ch19.xhtml#idm45207088597008){data-type="index:locator"},
    [Notification
    templates](#ch19.xhtml#idm45207088343616){data-type="index:locator"}
-   [NRPE Exporter]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095550448){data-type="index:locator"}
-   [Nyquist-Shannon sampling theorem]{data-type="index-term"}, [Time
    Controls](#ch06.xhtml#idm45207100900976){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### O

-   [OAuth2]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098176000){data-type="index:locator"}
-   [observe method]{data-type="index-term"}, [The
    Summary](#ch03.xhtml#idm45207113548960){data-type="index:locator"}
-   [offline-serving systems]{data-type="index-term"}, [Service
    instrumentation](#ch03.xhtml#idm45207116513776){data-type="index:locator"}
-   [offset]{data-type="index-term"},
    [Offset](#ch13.xhtml#idm45207092513200){data-type="index:locator"}
-   [on clause]{data-type="index-term"},
    [One-to-One](#ch15.xhtml#idm45207091199136){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091151616){data-type="index:locator"},
    [Alerting
    Rules](#ch18.xhtml#idm45207090011920){data-type="index:locator"}
    -   [use with and operator]{data-type="index-term"}, [and
        operator](#ch15.xhtml#idm45207091044384){data-type="index:locator"}
-   [one-to-one vector matching]{data-type="index-term"},
    [One-to-One](#ch15.xhtml#idm45207091227248){data-type="index:locator"}
-   [online-serving systems]{data-type="index-term"}, [Service
    instrumentation](#ch03.xhtml#idm45207115647024){data-type="index:locator"}
-   [OpenMetrics]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119487504){data-type="index:locator"},
    [OpenMetrics](#ch04.xhtml#ix_OpnM){data-type="index:locator"}-[Timestamps](#ch04.xhtml#idm45207102065344){data-type="index:locator"}
    -   [metric suffixes]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113640848){data-type="index:locator"}
    -   [metric types]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102102208){data-type="index:locator"}
    -   [support for format by Python client
        library]{data-type="index-term"}, [Multiprocess with
        Gunicorn](#ch04.xhtml#idm45207114237024){data-type="index:locator"}
    -   [timestamps]{data-type="index-term"},
        [Timestamps](#ch04.xhtml#idm45207102123936){data-type="index:locator"},
        [Timestamps](#ch04.xhtml#idm45207102074160){data-type="index:locator"}
-   [OpenTelemetry (OTel)]{data-type="index-term"}, [A Brief and
    Incomplete History of
    Monitoring](#ch01.xhtml#idm45207119588304){data-type="index:locator"}
-   [operational monitoring of computer
    systems]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#idm45207119697472){data-type="index:locator"}
-   [operations (recording rule names)]{data-type="index-term"}, [Naming
    of Recording
    Rules](#ch17.xhtml#idm45207090106016){data-type="index:locator"}
-   [or operator]{data-type="index-term"}, [or
    operator](#ch15.xhtml#idm45207091108640){data-type="index:locator"}
-   [OS collector]{data-type="index-term"}, [OS
    Collector](#ch07.xhtml#idm45207100625200){data-type="index:locator"}
-   [OTel (OpenTelemetry)]{data-type="index-term"}, [A Brief and
    Incomplete History of
    Monitoring](#ch01.xhtml#idm45207119589008){data-type="index:locator"}
-   [outliers, detecting]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091472320){data-type="index:locator"}
-   [over_time functions]{data-type="index-term"}, [Composing Range
    Vector
    Functions](#ch17.xhtml#idm45207090172144){data-type="index:locator"},
    [for](#ch18.xhtml#idm45207089981200){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### P

-   [pager storm]{data-type="index-term"},
    [Grouping](#ch19.xhtml#idm45207089068048){data-type="index:locator"}
-   [PagerDuty notifier]{data-type="index-term"},
    [Receivers](#ch19.xhtml#idm45207088854672){data-type="index:locator"}
-   [parsers]{data-type="index-term"},
    [Parsers](#ch04.xhtml#idm45207102296448){data-type="index:locator"}
-   [passwords]{data-type="index-term"}, [Enabling Basic
    Authentication](#ch20.xhtml#idm45207087691392){data-type="index:locator"}
-   [path labels]{data-type="index-term"}, [What Are
    Labels?](#ch05.xhtml#idm45207102051968){data-type="index:locator"},
    [Grok
    Exporter](#ch10.xhtml#idm45207096218272){data-type="index:locator"}
    -   [aggregating away]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101465056){data-type="index:locator"}
-   [paths]{data-type="index-term"}
    -   [HTTP requests broken out by]{data-type="index-term"}, [What Are
        Labels?](#ch05.xhtml#idm45207102054992){data-type="index:locator"}
    -   [options in scrape config]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#idm45207098305520){data-type="index:locator"}
-   [patterns (regular expressions)]{data-type="index-term"}, [Choosing
    What to
    Scrape](#ch08.xhtml#idm45207099395488){data-type="index:locator"}
    -   [use by Grok Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096440800){data-type="index:locator"}
-   [PDUs (Power Distribution Units)]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100507856){data-type="index:locator"}
-   [pending alerts]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090025072){data-type="index:locator"},
    [for](#ch18.xhtml#idm45207089997632){data-type="index:locator"}
-   [percentiles]{data-type="index-term"}, [The
    Histogram](#ch03.xhtml#idm45207119104384){data-type="index:locator"}
    -   [median, 25th, and 75th]{data-type="index-term"},
        [quantile](#ch14.xhtml#idm45207091408544){data-type="index:locator"}
-   [performance]{data-type="index-term"}
    -   [aggregate cAdvisor metrics, issue
        with]{data-type="index-term"},
        [CPU](#ch09.xhtml#idm45207097607984){data-type="index:locator"}
    -   [importance for client libraries]{data-type="index-term"},
        [Multiprocess with
        Gunicorn](#ch04.xhtml#idm45207114248128){data-type="index:locator"}
    -   [managing]{data-type="index-term"}, [Managing
        Performance](#ch21.xhtml#ix_perf){data-type="index:locator"}-[Horizontal
        Sharding](#ch21.xhtml#idm45207087070368){data-type="index:locator"}
        -   [detecting a problem]{data-type="index-term"}, [Detecting a
            Problem](#ch21.xhtml#idm45207087154704){data-type="index:locator"}
        -   [finding expensive targets and
            metrics]{data-type="index-term"}, [Finding Expensive Metrics
            and
            Targets](#ch21.xhtml#idm45207087133408){data-type="index:locator"}
        -   [hashmod relabel action]{data-type="index-term"},
            [Hashmod](#ch21.xhtml#idm45207087124288){data-type="index:locator"}
        -   [horizontal sharding]{data-type="index-term"}, [Horizontal
            Sharding](#ch21.xhtml#idm45207087088784){data-type="index:locator"}
        -   [reducing load]{data-type="index-term"}, [Reducing
            Load](#ch21.xhtml#idm45207087109840){data-type="index:locator"}
-   [Perl]{data-type="index-term"},
    [StatsD](#ch11.xhtml#idm45207095397216){data-type="index:locator"}
-   [pgw]{data-type="index-term"} ([see]{gentext="see"} Pushgateway)
-   [PHP]{data-type="index-term"},
    [StatsD](#ch11.xhtml#idm45207095397920){data-type="index:locator"}
-   [pid (process ID)]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090387184){data-type="index:locator"}
-   [planning a rollout]{data-type="index-term"}, [Planning a
    Rollout](#ch21.xhtml#ix_plan){data-type="index:locator"}-[Planning a
    Rollout](#ch21.xhtml#idm45207087499296){data-type="index:locator"}
-   [playbook for alerts]{data-type="index-term"}, [Annotations and
    Templates](#ch18.xhtml#idm45207089899360){data-type="index:locator"}
-   [plumbing]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#idm45207119495056){data-type="index:locator"}
-   [pods (Kubernetes)]{data-type="index-term"},
    [Endpointslice](#ch09.xhtml#idm45207097259440){data-type="index:locator"}
    -   [backing all Kubernetes services except API servers,
        scraping]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207097043888){data-type="index:locator"}
    -   [service discovery]{data-type="index-term"},
        [Pod](#ch09.xhtml#idm45207096791520){data-type="index:locator"}
-   [pod_name labels]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097299584){data-type="index:locator"}
-   [population standard deviation]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091473936){data-type="index:locator"}
-   [ports, exporter default]{data-type="index-term"},
    [MySQLd](#ch10.xhtml#idm45207096480720){data-type="index:locator"}
-   [POST method (HTTP)]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090385808){data-type="index:locator"}
-   [post-order tree transversal]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089651696){data-type="index:locator"}
-   [Power Distribution Units (PDUs)]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100507216){data-type="index:locator"}
-   [precedence (operator)]{data-type="index-term"}, [Operator
    Precedence](#ch15.xhtml#idm45207091027920){data-type="index:locator"}
-   [predict_linear function]{data-type="index-term"},
    [predict_linear](#ch16.xhtml#idm45207090543904){data-type="index:locator"}
-   [present_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090479072){data-type="index:locator"}
-   [Pressure Stall Information (PSI)]{data-type="index-term"},
    [Pressure
    Collector](#ch07.xhtml#idm45207100606464){data-type="index:locator"}
-   [ProbeFailing alert]{data-type="index-term"},
    [for](#ch18.xhtml#idm45207089974352){data-type="index:locator"}
-   [probe_ip_protocol]{data-type="index-term"},
    [ICMP](#ch10.xhtml#idm45207096041184){data-type="index:locator"}
-   [probe_success]{data-type="index-term"},
    [ICMP](#ch10.xhtml#idm45207096069616){data-type="index:locator"}
-   [process ID (pid)]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090386480){data-type="index:locator"}
-   [process library]{data-type="index-term"},
    [Library](#ch03.xhtml#idm45207116432160){data-type="index:locator"}
-   [processes]{data-type="index-term"}
    -   [blocked or running, metrics on]{data-type="index-term"}, [Stat
        Collector](#ch07.xhtml#idm45207100640608){data-type="index:locator"}
    -   [long-lived and multithreaded in
        Prometheus]{data-type="index-term"},
        [StatsD](#ch11.xhtml#idm45207095394944){data-type="index:locator"}
    -   [moving from checks on individual processes to service health as
        a whole]{data-type="index-term"}, [A Brief and Incomplete
        History of
        Monitoring](#ch01.xhtml#idm45207119803744){data-type="index:locator"}
    -   [multiprocess with Gunicorn]{data-type="index-term"},
        [Multiprocess with
        Gunicorn](#ch04.xhtml#ix_prcmlti){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114229232){data-type="index:locator"}
-   [process_cpu_seconds_total]{data-type="index-term"},
    [Selectors](#ch13.xhtml#idm45207092651328){data-type="index:locator"}
-   [process_resident_memory_bytes]{data-type="index-term"}, [Using the
    Expression
    Browser](#ch02.xhtml#idm45207119393600){data-type="index:locator"},
    [Running the Node
    Exporter](#ch02.xhtml#idm45207118803920){data-type="index:locator"}
    -   [in Grafana graph editor]{data-type="index-term"}, [Time Series
        Panel](#ch06.xhtml#idm45207100929216){data-type="index:locator"}
    -   [graph of in expression browser)]{data-type="index-term"},
        [Using the Expression
        Browser](#ch02.xhtml#idm45207113337120){data-type="index:locator"}
-   [process_start_time_seconds]{data-type="index-term"},
    [changes](#ch16.xhtml#idm45207090564848){data-type="index:locator"}
-   [production tags (Consul)]{data-type="index-term"}, [Choosing What
    to Scrape](#ch08.xhtml#idm45207099450464){data-type="index:locator"}
-   [profiling]{data-type="index-term"},
    [Profiling](#ch01.xhtml#idm45207119685760){data-type="index:locator"}
-   [promauto]{data-type="index-term"},
    [Go](#ch04.xhtml#idm45207103404576){data-type="index:locator"}
-   [Prometheus]{data-type="index-term"}
    -   [about]{data-type="index-term"}, [What Is
        Prometheus?](#ch01.xhtml#idm45207119595040){data-type="index:locator"}
    -   [architecture]{data-type="index-term"}, [Prometheus
        Architecture](#ch01.xhtml#ix_Prmtarch){data-type="index:locator"}-[Long-Term
        Storage](#ch01.xhtml#idm45207118951904){data-type="index:locator"}
    -   [use cases not suited for]{data-type="index-term"}, [What
        Prometheus Is
        Not](#ch01.xhtml#idm45207118948768){data-type="index:locator"}
-   [Prometheus Community Kubernetes Helm
    Charts]{data-type="index-term"}, [Alternative
    Deployments](#ch09.xhtml#idm45207096626528){data-type="index:locator"}
-   [Prometheus Operator project]{data-type="index-term"}, [Alternative
    Deployments](#ch09.xhtml#idm45207096625760){data-type="index:locator"}
-   [Prometheus vCloud Director SD]{data-type="index-term"}, [Service
    Discovery](#ch08.xhtml#idm45207100514624){data-type="index:locator"}
-   [prometheus.Collector interface]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094279424){data-type="index:locator"}
-   [prometheus.MustNewConstMetric]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094260576){data-type="index:locator"}
-   [prometheus_build_info]{data-type="index-term"}, [Many-to-One and
    group_left](#ch15.xhtml#idm45207091163520){data-type="index:locator"}
-   [prometheus_multiproc_dir environment
    variable]{data-type="index-term"}, [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207118190640){data-type="index:locator"},
    [Multiprocess with
    Gunicorn](#ch04.xhtml#idm45207114242128){data-type="index:locator"}
-   [prometheus_sd_http_failures_total]{data-type="index-term"},
    [HTTP](#ch08.xhtml#idm45207100060384){data-type="index:locator"}
-   [promhttp.Handler]{data-type="index-term"},
    [Go](#ch04.xhtml#idm45207114229936){data-type="index:locator"}
-   [PromQL]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119485616){data-type="index:locator"}
    -   [about]{data-type="index-term"}, [Introduction to
        PromQL](#ch13.xhtml#idm45207092803232){data-type="index:locator"}
    -   [aggregation basics]{data-type="index-term"}, [Aggregation
        Basics](#ch13.xhtml#ix_PQLaggbsc){data-type="index:locator"}-[Selectors](#ch13.xhtml#idm45207092657232){data-type="index:locator"}
        -   [counter]{data-type="index-term"},
            [Counter](#ch13.xhtml#idm45207092754544){data-type="index:locator"}
        -   [gauge]{data-type="index-term"},
            [Gauge](#ch13.xhtml#ix_PQLaggbscgauge){data-type="index:locator"}-[Counter](#ch13.xhtml#idm45207092759712){data-type="index:locator"}
        -   [histogram]{data-type="index-term"},
            [Histogram](#ch13.xhtml#ix_PQLaggbschst){data-type="index:locator"}-[Selectors](#ch13.xhtml#idm45207092658720){data-type="index:locator"}
        -   [summary]{data-type="index-term"},
            [Summary](#ch13.xhtml#idm45207092723600){data-type="index:locator"}
    -   [aggregation operators]{data-type="index-term"}, [Aggregation
        Operators](#ch14.xhtml#ix_PQLaggop){data-type="index:locator"}-[count_values](#ch14.xhtml#idm45207091368496){data-type="index:locator"}
        -   [avg]{data-type="index-term"},
            [avg](#ch14.xhtml#idm45207091513840){data-type="index:locator"}
        -   [count]{data-type="index-term"},
            [count](#ch14.xhtml#idm45207091544784){data-type="index:locator"}
        -   [count_values]{data-type="index-term"},
            [count_values](#ch14.xhtml#idm45207091389440){data-type="index:locator"}
        -   [group]{data-type="index-term"},
            [group](#ch14.xhtml#idm45207091489952){data-type="index:locator"}
        -   [grouping]{data-type="index-term"},
            [Grouping](#ch14.xhtml#ix_PQLaggopgrp){data-type="index:locator"}-[by](#ch14.xhtml#idm45207091572464){data-type="index:locator"}
        -   [min and max]{data-type="index-term"}, [min and
            max](#ch14.xhtml#idm45207091457792){data-type="index:locator"}
        -   [quantile]{data-type="index-term"},
            [quantile](#ch14.xhtml#idm45207091415616){data-type="index:locator"}
        -   [stddev and stdvar]{data-type="index-term"}, [stddev and
            stdvar](#ch14.xhtml#idm45207091481760){data-type="index:locator"}
        -   [sum]{data-type="index-term"},
            [sum](#ch14.xhtml#idm45207091563696){data-type="index:locator"}
        -   [topk and bottomk]{data-type="index-term"}, [topk and
            bottomk](#ch14.xhtml#idm45207091438384){data-type="index:locator"}
    -   [alerting rule, expression for]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118772592){data-type="index:locator"}
    -   [binary operators]{data-type="index-term"}, [Binary
        Operators](#ch15.xhtml#ix_PQLbinop){data-type="index:locator"}-[Operator
        Precedence](#ch15.xhtml#idm45207091006208){data-type="index:locator"}
        -   [operator precedence]{data-type="index-term"}, [Operator
            Precedence](#ch15.xhtml#idm45207091027248){data-type="index:locator"}
        -   [vector matching]{data-type="index-term"}, [Vector
            Matching](#ch15.xhtml#ix_PQLbinopvec){data-type="index:locator"}-[and
            operator](#ch15.xhtml#idm45207091034080){data-type="index:locator"}
        -   [working with scalars]{data-type="index-term"}, [Working
            with
            Scalars](#ch15.xhtml#ix_PQLbinopsclr){data-type="index:locator"}-[Vector
            Matching](#ch15.xhtml#idm45207091241424){data-type="index:locator"}
    -   [functions]{data-type="index-term"},
        [Functions](#ch16.xhtml#ix_PQLfnc){data-type="index:locator"}-[Aggregation
        Over
        Time](#ch16.xhtml#idm45207090461296){data-type="index:locator"}
        -   [aggregation over time]{data-type="index-term"},
            [Aggregation Over
            Time](#ch16.xhtml#ix_PQLfncAOT){data-type="index:locator"}-[Aggregation
            Over
            Time](#ch16.xhtml#idm45207090462784){data-type="index:locator"}
        -   [changing type]{data-type="index-term"}, [Changing
            Type](#ch16.xhtml#idm45207090987552){data-type="index:locator"}
        -   [counters]{data-type="index-term"},
            [Counters](#ch16.xhtml#ix_PQLfncctr){data-type="index:locator"}-[resets](#ch16.xhtml#idm45207090582240){data-type="index:locator"}
        -   [histogram_quantile]{data-type="index-term"}, [Histograms
            with
            histogram_quantile](#ch16.xhtml#idm45207090663008){data-type="index:locator"}
        -   [label]{data-type="index-term"},
            [Labels](#ch16.xhtml#idm45207090747808){data-type="index:locator"}
        -   [math functions]{data-type="index-term"},
            [Math](#ch16.xhtml#ix_PQLfncmth){data-type="index:locator"}-[Time
            and
            Date](#ch16.xhtml#idm45207090838720){data-type="index:locator"}
        -   [missing series, absent and
            absent_over_time]{data-type="index-term"}, [Missing Series,
            absent, and
            absent_over_time](#ch16.xhtml#idm45207090710944){data-type="index:locator"}
        -   [sorting with sort and sort_desc]{data-type="index-term"},
            [Sorting with sort and
            sort_desc](#ch16.xhtml#idm45207090677664){data-type="index:locator"}
        -   [time and date]{data-type="index-term"}, [Time and
            Date](#ch16.xhtml#ix_PQLfnctmda){data-type="index:locator"}-[timestamp](#ch16.xhtml#idm45207090753104){data-type="index:locator"}
    -   [HTTP API]{data-type="index-term"}, [HTTP
        API](#ch13.xhtml#ix_PQLHTTP){data-type="index:locator"}-[Aligned
        data](#ch13.xhtml#idm45207091663968){data-type="index:locator"}
        -   [aligned data]{data-type="index-term"}, [Aligned
            data](#ch13.xhtml#idm45207091802960){data-type="index:locator"}
        -   [query]{data-type="index-term"},
            [query](#ch13.xhtml#ix_PQLHTTPqry){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091977728){data-type="index:locator"}
        -   [query_range]{data-type="index-term"},
            [query_range](#ch13.xhtml#ix_PQLHTTPqryrng){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091810320){data-type="index:locator"}
    -   [recording rules]{data-type="index-term"}, [Recording
        Rules](#ch17.xhtml#ix_PQLrec){data-type="index:locator"}-[Naming
        of Recording
        Rules](#ch17.xhtml#idm45207090073920){data-type="index:locator"}
        -   [naming]{data-type="index-term"}, [Naming of Recording
            Rules](#ch17.xhtml#ix_PQLrecnm){data-type="index:locator"}-[Naming
            of Recording
            Rules](#ch17.xhtml#idm45207090077008){data-type="index:locator"}
        -   [using]{data-type="index-term"}, [Using Recording
            Rules](#ch17.xhtml#ix_PQLrecuse){data-type="index:locator"}-[Using
            Recording
            Rules](#ch17.xhtml#idm45207090213632){data-type="index:locator"}
        -   [when to use]{data-type="index-term"}, [When to Use
            Recording
            Rules](#ch17.xhtml#ix_PQLrecwhen){data-type="index:locator"}-[How
            Not to Use
            Rules](#ch17.xhtml#idm45207090133536){data-type="index:locator"}
    -   [selectors]{data-type="index-term"},
        [Selectors](#ch13.xhtml#ix_PQLsel){data-type="index:locator"}-[At
        Modifier](#ch13.xhtml#idm45207092475136){data-type="index:locator"}
        -   [at (@) modifier]{data-type="index-term"}, [At
            Modifier](#ch13.xhtml#idm45207092491456){data-type="index:locator"}
        -   [matchers]{data-type="index-term"},
            [Matchers](#ch13.xhtml#idm45207092635312){data-type="index:locator"}
        -   [offset]{data-type="index-term"},
            [Offset](#ch13.xhtml#idm45207092511552){data-type="index:locator"}
        -   [range vector]{data-type="index-term"}, [Range
            Vector](#ch13.xhtml#idm45207092575376){data-type="index:locator"}
        -   [subqueries]{data-type="index-term"},
            [Subqueries](#ch13.xhtml#idm45207092525648){data-type="index:locator"}
-   [promtool]{data-type="index-term"}
    -   [check metrics]{data-type="index-term"}, [check
        metrics](#ch04.xhtml#idm45207102118640){data-type="index:locator"}
    -   [check rules]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#idm45207090312016){data-type="index:locator"}
-   [proxy_url]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098253248){data-type="index:locator"}
-   [pull]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#idm45207087247376){data-type="index:locator"}
-   [push]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#idm45207087252192){data-type="index:locator"}
-   [pushadd_to_gateway]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102512992){data-type="index:locator"}
-   [Pushgateway]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#ix_pshgt){data-type="index:locator"}-[Pushgateway](#ch04.xhtml#idm45207102353904){data-type="index:locator"}
    -   [improper use of]{data-type="index-term"}, [Networks and
        Authentication](#ch21.xhtml#idm45207087251488){data-type="index:locator"}
    -   [target labels]{data-type="index-term"}, [Instrumentation and
        Target
        Labels](#ch05.xhtml#idm45207102040800){data-type="index:locator"}
-   [push_to_gateway]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102518672){data-type="index:locator"}
-   [Python]{data-type="index-term"}
    -   [application using label for counter
        metric]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#idm45207102029264){data-type="index:locator"}
    -   [client libraries in Python 3]{data-type="index-term"},
        [Instrumentation](#ch03.xhtml#idm45207116017696){data-type="index:locator"}
    -   [client library]{data-type="index-term"}, [Reducing
        Load](#ch21.xhtml#idm45207087101776){data-type="index:locator"}
    -   [Consul metrics exporter written in]{data-type="index-term"},
        [Custom
        Collectors](#ch12.xhtml#idm45207093380816){data-type="index:locator"}
    -   [exposition from batch jobs using
        Pushgateway]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102522848){data-type="index:locator"}
    -   [exposition in client libraries]{data-type="index-term"},
        [Exposition](#ch04.xhtml#ix_Pyexpo){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114223120){data-type="index:locator"}
        -   [multiprocess with Gunicorn]{data-type="index-term"},
            [Multiprocess with
            Gunicorn](#ch04.xhtml#ix_Pyexpomlti){data-type="index:locator"}-[Go](#ch04.xhtml#idm45207114226144){data-type="index:locator"}
        -   [Twisted]{data-type="index-term"},
            [Twisted](#ch04.xhtml#idm45207116237312){data-type="index:locator"}
        -   [WSGI]{data-type="index-term"},
            [WSGI](#ch04.xhtml#idm45207116350480){data-type="index:locator"}
    -   [exposition using Graphite bridge]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102346976){data-type="index:locator"}
    -   [unit testing a counter in]{data-type="index-term"}, [Unit
        Testing
        Instrumentation](#ch03.xhtml#idm45207115712352){data-type="index:locator"}
-   [python_info expression]{data-type="index-term"}, [A Simple
    Program](#ch03.xhtml#idm45207113819712){data-type="index:locator"},
    [Info](#ch05.xhtml#idm45207101155104){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### Q

-   [quantile operator]{data-type="index-term"},
    [quantile](#ch14.xhtml#idm45207091417296){data-type="index:locator"}
-   [quantiles]{data-type="index-term"}, [The
    Summary](#ch03.xhtml#idm45207119046464){data-type="index:locator"},
    [Summary](#ch13.xhtml#idm45207092693536){data-type="index:locator"}
    -   [calculating with histograms]{data-type="index-term"},
        [Histogram](#ch13.xhtml#idm45207092687600){data-type="index:locator"}
    -   [latency SLAs and]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115733264){data-type="index:locator"}
    -   [limitations of]{data-type="index-term"},
        [Buckets](#ch03.xhtml#idm45207115728624){data-type="index:locator"}
    -   [and percentiles]{data-type="index-term"}, [The
        Histogram](#ch03.xhtml#idm45207119105632){data-type="index:locator"}
-   [quantile_over_time]{data-type="index-term"},
    [quantile](#ch14.xhtml#idm45207091403392){data-type="index:locator"},
    [Aggregation Over
    Time](#ch16.xhtml#idm45207090491296){data-type="index:locator"}
-   [quartiles (1st and 3rd)]{data-type="index-term"},
    [quantile](#ch14.xhtml#idm45207091405408){data-type="index:locator"}
-   [query]{data-type="index-term"},
    [query](#ch13.xhtml#ix_qry){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091968624){data-type="index:locator"},
    [timestamp](#ch16.xhtml#idm45207090761520){data-type="index:locator"},
    [Annotations and
    Templates](#ch18.xhtml#idm45207089914880){data-type="index:locator"}
-   [query endpoint]{data-type="index-term"},
    [query](#ch13.xhtml#idm45207092460336){data-type="index:locator"}
-   [query range endpoint]{data-type="index-term"},
    [query_range](#ch13.xhtml#idm45207091966688){data-type="index:locator"}
    -   ([see also]{gentext="see"} query_range)
-   [query_range]{data-type="index-term"},
    [query_range](#ch13.xhtml#ix_qryrng){data-type="index:locator"}-[query_range](#ch13.xhtml#idm45207091811296){data-type="index:locator"},
    [timestamp](#ch16.xhtml#idm45207090760816){data-type="index:locator"},
    [Reducing
    Cardinality](#ch17.xhtml#idm45207090199184){data-type="index:locator"}
    -   [gotcha when using with topk and
        bottomk]{data-type="index-term"}, [topk and
        bottomk](#ch14.xhtml#idm45207091426208){data-type="index:locator"}
    -   [using time function with]{data-type="index-term"},
        [time](#ch16.xhtml#idm45207090822224){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### R

-   [race conditions]{data-type="index-term"},
    [for](#ch18.xhtml#idm45207090002880){data-type="index:locator"},
    [Going Global with
    Federation](#ch21.xhtml#idm45207087359280){data-type="index:locator"}
-   [radians and degrees, converting between]{data-type="index-term"},
    [Trigonometric
    Functions](#ch16.xhtml#idm45207090847168){data-type="index:locator"}
-   [RAM]{data-type="index-term"},
    [Hardware](#ch21.xhtml#idm45207087297648){data-type="index:locator"}
-   [range loop]{data-type="index-term"}, [Annotations and
    Templates](#ch18.xhtml#idm45207089910448){data-type="index:locator"}
-   [range vector selector]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092574160){data-type="index:locator"}
-   [range vectors]{data-type="index-term"},
    [Subqueries](#ch13.xhtml#idm45207092523760){data-type="index:locator"}
    -   [composing range vector functions]{data-type="index-term"},
        [Composing Range Vector
        Functions](#ch17.xhtml#idm45207090178176){data-type="index:locator"}
    -   [functions and]{data-type="index-term"},
        [Functions](#ch16.xhtml#idm45207090997168){data-type="index:locator"}
    -   [gauge-changing functions taking]{data-type="index-term"},
        [Changing
        Gauges](#ch16.xhtml#idm45207090573680){data-type="index:locator"}
    -   [in recording rules]{data-type="index-term"}, [Reducing
        Cardinality](#ch17.xhtml#idm45207090191888){data-type="index:locator"}
    -   [use with query endpoint]{data-type="index-term"},
        [query](#ch13.xhtml#idm45207092344384){data-type="index:locator"}
    -   [use with query_result]{data-type="index-term"},
        [query_range](#ch13.xhtml#idm45207091951872){data-type="index:locator"}
-   [rate function]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207113329056){data-type="index:locator"},
    [Running the Node
    Exporter](#ch02.xhtml#idm45207118802960){data-type="index:locator"},
    [The
    Counter](#ch03.xhtml#idm45207117170400){data-type="index:locator"},
    [The
    Summary](#ch03.xhtml#idm45207119194480){data-type="index:locator"},
    [Time
    Controls](#ch06.xhtml#idm45207100904512){data-type="index:locator"},
    [Counter](#ch13.xhtml#idm45207092750640){data-type="index:locator"},
    [Summary](#ch13.xhtml#idm45207092716496){data-type="index:locator"},
    [Histogram](#ch13.xhtml#idm45207092678112){data-type="index:locator"}
    -   [about]{data-type="index-term"},
        [rate](#ch16.xhtml#idm45207090640736){data-type="index:locator"}
    -   [increase function and]{data-type="index-term"},
        [increase](#ch16.xhtml#idm45207090619248){data-type="index:locator"}
    -   [irate function and]{data-type="index-term"},
        [irate](#ch16.xhtml#idm45207090596736){data-type="index:locator"}
    -   [not using with a counter]{data-type="index-term"},
        [count](#ch14.xhtml#idm45207091539520){data-type="index:locator"}
    -   [offset and]{data-type="index-term"},
        [Offset](#ch13.xhtml#idm45207092504624){data-type="index:locator"}
    -   [use with range vectors]{data-type="index-term"}, [Range
        Vector](#ch13.xhtml#idm45207092571152){data-type="index:locator"}
    -   [using at (@) modifier with]{data-type="index-term"}, [At
        Modifier](#ch13.xhtml#idm45207092486352){data-type="index:locator"}
    -   [using before sum with counters]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091560128){data-type="index:locator"}
    -   [using first for buckets exposed by histogram metric
        type]{data-type="index-term"}, [Histograms with
        histogram_quantile](#ch16.xhtml#idm45207090656464){data-type="index:locator"}
    -   [using to determine if resources are
        overloaded]{data-type="index-term"}, [Pressure
        Collector](#ch07.xhtml#idm45207100596336){data-type="index:locator"}
    -   [using with max_over_time]{data-type="index-term"},
        [Subqueries](#ch13.xhtml#idm45207092521392){data-type="index:locator"}
    -   [using with sum]{data-type="index-term"}, [Composing Range
        Vector
        Functions](#ch17.xhtml#idm45207090170016){data-type="index:locator"}
-   [rate, errors, and duration (RED method)]{data-type="index-term"},
    [Service
    instrumentation](#ch03.xhtml#idm45207116517104){data-type="index:locator"}
-   [ratio, calculating for exceptions]{data-type="index-term"},
    [Counting
    Exceptions](#ch03.xhtml#idm45207113263920){data-type="index:locator"}
-   [RE2 engine for regular expressions]{data-type="index-term"},
    [Choosing What to
    Scrape](#ch08.xhtml#idm45207099441632){data-type="index:locator"}
-   [read_recent: true]{data-type="index-term"}, [Horizontal
    Sharding](#ch21.xhtml#idm45207087076768){data-type="index:locator"}
-   [receivers]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089799968){data-type="index:locator"}
    -   [backend-ticket]{data-type="index-term"}, [Routing
        Tree](#ch19.xhtml#idm45207089280016){data-type="index:locator"}
    -   [configuring]{data-type="index-term"},
        [Receivers](#ch19.xhtml#ix_recv){data-type="index:locator"}-[Resolved
        notifications](#ch19.xhtml#idm45207087925808){data-type="index:locator"}
        -   [notification templates]{data-type="index-term"},
            [Notification
            templates](#ch19.xhtml#ix_reccfgnottmpl){data-type="index:locator"}-[Notification
            templates](#ch19.xhtml#idm45207087986736){data-type="index:locator"}
        -   [resolved notifications]{data-type="index-term"}, [Resolved
            notifications](#ch19.xhtml#idm45207087938464){data-type="index:locator"}
-   [record field]{data-type="index-term"}, [Alerting
    Rules](#ch18.xhtml#idm45207090043936){data-type="index:locator"}
-   [recording rules]{data-type="index-term"}, [Recording Rules and
    Alerts](#ch01.xhtml#idm45207118968560){data-type="index:locator"},
    [Recording
    Rules](#ch17.xhtml#ix_recru){data-type="index:locator"}-[Naming of
    Recording
    Rules](#ch17.xhtml#idm45207090072672){data-type="index:locator"},
    [Alerting
    Rules](#ch18.xhtml#idm45207090047936){data-type="index:locator"}
    -   [detecting bad rules with promtool check
        rules]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#idm45207090312928){data-type="index:locator"}
    -   [naming]{data-type="index-term"}, [Naming of Recording
        Rules](#ch17.xhtml#ix_recrunm){data-type="index:locator"}-[Naming
        of Recording
        Rules](#ch17.xhtml#idm45207090075520){data-type="index:locator"}
        -   [level, metric, and operations]{data-type="index-term"},
            [Naming of Recording
            Rules](#ch17.xhtml#idm45207090119536){data-type="index:locator"}
    -   [using]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#ix_recruuse){data-type="index:locator"}-[Using
        Recording
        Rules](#ch17.xhtml#idm45207090212112){data-type="index:locator"}
    -   [when to use]{data-type="index-term"}, [When to Use Recording
        Rules](#ch17.xhtml#ix_recruwhen){data-type="index:locator"}-[How
        Not to Use
        Rules](#ch17.xhtml#idm45207090132016){data-type="index:locator"}
        -   [composing range vector functions]{data-type="index-term"},
            [Composing Range Vector
            Functions](#ch17.xhtml#idm45207090180368){data-type="index:locator"}
        -   [how not to use recording rules]{data-type="index-term"},
            [How Not to Use
            Rules](#ch17.xhtml#idm45207090154624){data-type="index:locator"}
        -   [reducing cardinality]{data-type="index-term"}, [Reducing
            Cardinality](#ch17.xhtml#idm45207090201040){data-type="index:locator"}
        -   [rules for APIs]{data-type="index-term"}, [Rules for
            APIs](#ch17.xhtml#idm45207090159840){data-type="index:locator"}
-   [RED method (rate, errors, duration)]{data-type="index-term"},
    [Service
    instrumentation](#ch03.xhtml#idm45207115646192){data-type="index:locator"}
-   [refresh interval menu (Grafana)]{data-type="index-term"}, [Time
    Controls](#ch06.xhtml#idm45207100912912){data-type="index:locator"}
-   [region labels]{data-type="index-term"}, [Alert
    Labels](#ch18.xhtml#idm45207089952880){data-type="index:locator"},
    [External
    Labels](#ch18.xhtml#idm45207089837152){data-type="index:locator"},
    [Grouping](#ch19.xhtml#idm45207089078512){data-type="index:locator"}
    -   [in Slack]{data-type="index-term"}, [Notification
        templates](#ch19.xhtml#idm45207088339952){data-type="index:locator"}
-   [registry]{data-type="index-term"}, [The
    Counter](#ch03.xhtml#idm45207117178752){data-type="index:locator"},
    [Exposition](#ch04.xhtml#idm45207116404240){data-type="index:locator"}
    -   [custom collector registered with default
        registry]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093544832){data-type="index:locator"}
    -   [custom registry for multiprocess exposition with
        Gunicorn]{data-type="index-term"}, [Multiprocess with
        Gunicorn](#ch04.xhtml#idm45207118293824){data-type="index:locator"}
    -   [custom registry for Python client
        library]{data-type="index-term"},
        [Pushgateway](#ch04.xhtml#idm45207102522000){data-type="index:locator"}
    -   [metrics pushed to Graphite using a
        bridge]{data-type="index-term"},
        [Bridges](#ch04.xhtml#idm45207102310432){data-type="index:locator"}
    -   [registration with Go client library]{data-type="index-term"},
        [Go](#ch04.xhtml#idm45207103403872){data-type="index:locator"}
-   [Registry.Gather]{data-type="index-term"},
    [Bridges](#ch04.xhtml#idm45207102305408){data-type="index:locator"}
-   [regression]{data-type="index-term"},
    [deriv](#ch16.xhtml#idm45207090550368){data-type="index:locator"}
-   [regular expression matcher (=\~)]{data-type="index-term"},
    [Matchers](#ch13.xhtml#idm45207092620352){data-type="index:locator"}
-   [regular expressions]{data-type="index-term"}
    -   [expect regex failing in TCP probe]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207096007776){data-type="index:locator"}
    -   [matchers]{data-type="index-term"},
        [Selectors](#ch13.xhtml#idm45207092639568){data-type="index:locator"},
        [Routing
        Tree](#ch19.xhtml#idm45207089649296){data-type="index:locator"}
    -   [patterns based on, use by Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096221488){data-type="index:locator"}
    -   [quick primer on]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099394752){data-type="index:locator"}
    -   [RE2 engine for]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099440048){data-type="index:locator"}
    -   [use in replace relabel action]{data-type="index-term"},
        [replace](#ch08.xhtml#idm45207099337296){data-type="index:locator"}
    -   [using to match targets in relabeling]{data-type="index-term"},
        [Choosing What to
        Scrape](#ch08.xhtml#idm45207099783008){data-type="index:locator"}
-   [relabeling]{data-type="index-term"}, [Service
    Discovery](#ch01.xhtml#idm45207119371568){data-type="index:locator"},
    [Relabeling](#ch08.xhtml#ix_relbl){data-type="index:locator"}-[Lists](#ch08.xhtml#idm45207098349120){data-type="index:locator"},
    [Configuring Alertmanagers in
    Prometheus](#ch18.xhtml#idm45207089859088){data-type="index:locator"}
    -   [alert_relabel_configs]{data-type="index-term"}, [Configuring
        Alertmanagers in
        Prometheus](#ch18.xhtml#idm45207089851792){data-type="index:locator"}
    -   [automatic deduplication with]{data-type="index-term"},
        [replace](#ch08.xhtml#idm45207098939344){data-type="index:locator"}
    -   [choosing what to scrape]{data-type="index-term"}, [Choosing
        What to
        Scrape](#ch08.xhtml#ix_relblchscrp){data-type="index:locator"}-[Choosing
        What to
        Scrape](#ch08.xhtml#idm45207099367136){data-type="index:locator"}
    -   [hashmod action]{data-type="index-term"},
        [Hashmod](#ch21.xhtml#idm45207087118608){data-type="index:locator"}
    -   [labeldrop and labelkeep actions]{data-type="index-term"},
        [labeldrop and
        labelkeep](#ch08.xhtml#idm45207097823024){data-type="index:locator"}
    -   [metrics, using metric_relabel_configs]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207098022240){data-type="index:locator"}
    -   [providing URL parameters for Blackbox exporters
        in]{data-type="index-term"}, [Prometheus
        Configuration](#ch10.xhtml#idm45207095914736){data-type="index:locator"}
    -   [support by remote write]{data-type="index-term"}, [Long-Term
        Storage](#ch21.xhtml#idm45207087329568){data-type="index:locator"}
    -   [using to add labels from Kubernetes service or pod
        metadata]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207096916464){data-type="index:locator"}
    -   [using to override scrape config
        settings]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#idm45207098170352){data-type="index:locator"}
    -   [using to specify target labels]{data-type="index-term"},
        [Target
        Labels](#ch08.xhtml#ix_relbltrgt){data-type="index:locator"}-[Lists](#ch08.xhtml#idm45207098352464){data-type="index:locator"}
        -   [changing case]{data-type="index-term"},
            [Case](#ch08.xhtml#idm45207098654272){data-type="index:locator"}
        -   [job, instance, and
            \_\_address\_\_]{data-type="index-term"}, [job, instance,
            and
            \_\_address\_\_](#ch08.xhtml#idm45207098893920){data-type="index:locator"}
        -   [labelmap action]{data-type="index-term"},
            [labelmap](#ch08.xhtml#idm45207098787744){data-type="index:locator"}
        -   [lists]{data-type="index-term"},
            [Lists](#ch08.xhtml#idm45207098511184){data-type="index:locator"}
        -   [replace action]{data-type="index-term"},
            [replace](#ch08.xhtml#idm45207099339072){data-type="index:locator"}
-   [relabel_configs]{data-type="index-term"}, [Choosing What to
    Scrape](#ch08.xhtml#idm45207099748480){data-type="index:locator"}
    -   [for Kubernetes API server scrapes]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207097144864){data-type="index:locator"}
    -   [versus metric_relabel_configs]{data-type="index-term"},
        [metric_relabel_configs](#ch08.xhtml#idm45207098013184){data-type="index:locator"}
-   [reliability]{data-type="index-term"}, [Planning for
    Failure](#ch21.xhtml#idm45207087219392){data-type="index:locator"}
-   [remote read]{data-type="index-term"}, [Long-Term
    Storage](#ch21.xhtml#idm45207087332128){data-type="index:locator"},
    [Horizontal
    Sharding](#ch21.xhtml#idm45207087076064){data-type="index:locator"}
-   [remote write]{data-type="index-term"}, [Long-Term
    Storage](#ch21.xhtml#idm45207087333856){data-type="index:locator"}
-   [remote write endpoint]{data-type="index-term"}, [Long-Term
    Storage](#ch21.xhtml#idm45207087326064){data-type="index:locator"}
-   [rename system call]{data-type="index-term"}, [Using the Textfile
    Collector](#ch07.xhtml#idm45207100550864){data-type="index:locator"},
    [File](#ch08.xhtml#idm45207100127664){data-type="index:locator"}
-   [repeat_interval]{data-type="index-term"}, [Throttling and
    repetition](#ch19.xhtml#idm45207089022368){data-type="index:locator"}
-   [repetition of notifications]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089803344){data-type="index:locator"},
    [Throttling and
    repetition](#ch19.xhtml#idm45207089034080){data-type="index:locator"}
-   [replace (relabel action)]{data-type="index-term"},
    [replace](#ch08.xhtml#idm45207099339760){data-type="index:locator"}
-   [reporting_enabled setting]{data-type="index-term"}, [Dashboarding
    with
    Grafana](#ch06.xhtml#idm45207101000256){data-type="index:locator"}
-   [request logs]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207114284896){data-type="index:locator"}
-   [resets function]{data-type="index-term"},
    [resets](#ch16.xhtml#idm45207090590240){data-type="index:locator"}
-   [resident set size (RSS)]{data-type="index-term"},
    [Memory](#ch09.xhtml#idm45207097575024){data-type="index:locator"}
-   [resolved notifications]{data-type="index-term"}, [Resolved
    notifications](#ch19.xhtml#idm45207087939776){data-type="index:locator"}
-   [resource pressure for CPU, memory, and
    I/O]{data-type="index-term"}, [Pressure
    Collector](#ch07.xhtml#idm45207100605792){data-type="index:locator"}
-   [restarts]{data-type="index-term"},
    [resets](#ch16.xhtml#idm45207090585600){data-type="index:locator"}
-   [resultType]{data-type="index-term"}
    -   [matrix]{data-type="index-term"},
        [query](#ch13.xhtml#idm45207092122448){data-type="index:locator"}
    -   [scalar]{data-type="index-term"},
        [query](#ch13.xhtml#idm45207092157712){data-type="index:locator"}
    -   [vector]{data-type="index-term"},
        [query](#ch13.xhtml#idm45207092352736){data-type="index:locator"}
-   [reverse proxy]{data-type="index-term"}, [Enabling Basic
    Authentication](#ch20.xhtml#idm45207087693248){data-type="index:locator"}
    -   [running Prometheus behind]{data-type="index-term"}, [Networks
        and
        Authentication](#ch21.xhtml#idm45207087243904){data-type="index:locator"}
-   [root user]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100735152){data-type="index:locator"}
-   [round function]{data-type="index-term"},
    [round](#ch16.xhtml#idm45207090895872){data-type="index:locator"}
-   [routes field]{data-type="index-term"}, [Routing
    Tree](#ch19.xhtml#idm45207089612048){data-type="index:locator"}
-   [routing]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089812464){data-type="index:locator"}
-   [routing tree]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089810992){data-type="index:locator"}
    -   [configuring for Alertmanager]{data-type="index-term"}, [Routing
        Tree](#ch19.xhtml#ix_rttree){data-type="index:locator"}-[Receivers](#ch19.xhtml#idm45207088862640){data-type="index:locator"}
        -   [grouping]{data-type="index-term"},
            [Grouping](#ch19.xhtml#idm45207089244064){data-type="index:locator"}
        -   [throttling and repetition]{data-type="index-term"},
            [Throttling and
            repetition](#ch19.xhtml#idm45207089040416){data-type="index:locator"}
    -   [visual editor for]{data-type="index-term"}, [Routing
        Tree](#ch19.xhtml#idm45207089323040){data-type="index:locator"}
-   [RSS (resident set size)]{data-type="index-term"},
    [Memory](#ch09.xhtml#idm45207097574320){data-type="index:locator"}
-   [rule files]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090449056){data-type="index:locator"},
    [Configuration
    Management](#ch21.xhtml#idm45207087272848){data-type="index:locator"}
    -   [example]{data-type="index-term"}, [Using Recording
        Rules](#ch17.xhtml#idm45207090307616){data-type="index:locator"}
-   [Rules status page (Prometheus)]{data-type="index-term"}, [Using
    Recording
    Rules](#ch17.xhtml#idm45207090272240){data-type="index:locator"}
-   [rule_files field]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090392912){data-type="index:locator"}
-   [running Prometheus]{data-type="index-term"}, [Getting Started with
    Prometheus](#ch02.xhtml#ix_run){data-type="index:locator"}-[Running
    Prometheus](#ch02.xhtml#idm45207118615776){data-type="index:locator"},
    [Running
    Prometheus](#ch21.xhtml#idm45207087312048){data-type="index:locator"}
    -   ([see also]{gentext="see"} deploying Prometheus)
    -   [configuration]{data-type="index-term"}, [Running
        Prometheus](#ch02.xhtml#idm45207119479008){data-type="index:locator"}
    -   [expression browser]{data-type="index-term"}, [Running
        Prometheus](#ch02.xhtml#idm45207119756768){data-type="index:locator"}
    -   [requirements]{data-type="index-term"}, [Getting Started with
        Prometheus](#ch02.xhtml#idm45207113434608){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### S

-   [SaaS (software as a service ) monitoring
    systems]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095557264){data-type="index:locator"}
-   [sample standard deviation]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091473136){data-type="index:locator"}
-   [sample_limit]{data-type="index-term"}, [Reducing
    Load](#ch21.xhtml#idm45207087092416){data-type="index:locator"}
-   [sampling]{data-type="index-term"}
    -   [fundamental limitation of]{data-type="index-term"}, [Time
        Controls](#ch06.xhtml#idm45207100902016){data-type="index:locator"}
    -   [not catching every possible change]{data-type="index-term"},
        [changes](#ch16.xhtml#idm45207090561808){data-type="index:locator"}
    -   [for tracing]{data-type="index-term"},
        [Tracing](#ch01.xhtml#idm45207114295968){data-type="index:locator"}
-   [scalar function]{data-type="index-term"},
    [scalar](#ch16.xhtml#idm45207090972160){data-type="index:locator"}
-   [scalars]{data-type="index-term"},
    [query](#ch13.xhtml#idm45207092156192){data-type="index:locator"}
    -   [conversion to instant vector]{data-type="index-term"},
        [query_range](#ch13.xhtml#idm45207091954016){data-type="index:locator"}
    -   [working with]{data-type="index-term"}, [Working with
        Scalars](#ch15.xhtml#ix_sclr){data-type="index:locator"}-[Vector
        Matching](#ch15.xhtml#idm45207091238656){data-type="index:locator"}
        -   [arithmetic operators]{data-type="index-term"}, [Arithmetic
            Operators](#ch15.xhtml#ix_sclrmath){data-type="index:locator"}-[Arithmetic
            Operators](#ch15.xhtml#idm45207091314496){data-type="index:locator"}
        -   [comparison operators]{data-type="index-term"}, [Comparison
            Operators](#ch15.xhtml#ix_sclrcmp){data-type="index:locator"}-[bool
            modifier](#ch15.xhtml#idm45207091246736){data-type="index:locator"}
        -   [trigonometric operator atan2]{data-type="index-term"},
            [Trigonometric
            Operator](#ch15.xhtml#idm45207091309328){data-type="index:locator"}
-   [scaling Prometheus]{data-type="index-term"}, [Growing
    Prometheus](#ch21.xhtml#idm45207087492576){data-type="index:locator"},
    [Going Global with
    Federation](#ch21.xhtml#idm45207087394112){data-type="index:locator"}
-   [scheme (scrape config)]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098248352){data-type="index:locator"},
    [Node](#ch09.xhtml#idm45207097319792){data-type="index:locator"}
-   [scrape errors]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118789200){data-type="index:locator"}
-   [scraper labels]{data-type="index-term"}, [Horizontal
    Sharding](#ch21.xhtml#idm45207087074368){data-type="index:locator"}
-   [scrape_configs]{data-type="index-term"},
    [Static](#ch08.xhtml#idm45207100344544){data-type="index:locator"}
    -   [adapting for TLS]{data-type="index-term"}, [Enabling
        TLS](#ch20.xhtml#idm45207087852416){data-type="index:locator"}
    -   [for cgroup scraping with cAdvisor]{data-type="index-term"},
        [cAdvisor](#ch09.xhtml#idm45207097649456){data-type="index:locator"}
    -   [for Kubernetes API servers]{data-type="index-term"},
        [Endpointslice](#ch09.xhtml#idm45207097193936){data-type="index:locator"}
    -   [Prometheus scraping Consul Exporter]{data-type="index-term"},
        [Consul](#ch10.xhtml#idm45207096583296){data-type="index:locator"}
    -   [showing several available options]{data-type="index-term"},
        [How to
        Scrape](#ch08.xhtml#idm45207098306368){data-type="index:locator"}
-   [scrape_interval]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098172016){data-type="index:locator"},
    [Using Recording
    Rules](#ch17.xhtml#idm45207090276592){data-type="index:locator"},
    [Reducing
    Load](#ch21.xhtml#idm45207087099168){data-type="index:locator"}
-   [scrape_samples_post_metric_relabeling]{data-type="index-term"},
    [Reducing
    Load](#ch21.xhtml#idm45207087093264){data-type="index:locator"}
-   [scrape_timeout]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098171408){data-type="index:locator"}
-   [scraping]{data-type="index-term"},
    [Scraping](#ch01.xhtml#idm45207119369552){data-type="index:locator"},
    [Using the Expression
    Browser](#ch02.xhtml#idm45207118607552){data-type="index:locator"},
    [Going Global with
    Federation](#ch21.xhtml#idm45207087361136){data-type="index:locator"}
    -   [catching failed scrapes in Blackbox
        exporter]{data-type="index-term"},
        [for](#ch18.xhtml#idm45207089975456){data-type="index:locator"}
    -   [choosing which targets to scrape in
        relabeling]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#ix_scrptrgt){data-type="index:locator"}-[Choosing
        What to
        Scrape](#ch08.xhtml#idm45207099365888){data-type="index:locator"}
    -   [configuring Prometheus to scrape itself using Basic
        Authentication]{data-type="index-term"}, [Enabling Basic
        Authentication](#ch20.xhtml#idm45207087635584){data-type="index:locator"}
    -   [in custom exporters]{data-type="index-term"},
        [Guidelines](#ch12.xhtml#idm45207092819216){data-type="index:locator"}
    -   [how to scrape]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#ix_scrphow){data-type="index:locator"}-[Label
        Clashes and
        honor_labels](#ch08.xhtml#idm45207097677392){data-type="index:locator"}
        -   [label clashes and honor_labels]{data-type="index-term"},
            [Label Clashes and
            honor_labels](#ch08.xhtml#idm45207097742144){data-type="index:locator"}
        -   [labeldrop and labelkeep]{data-type="index-term"},
            [labeldrop and
            labelkeep](#ch08.xhtml#idm45207097820800){data-type="index:locator"}
        -   [metric_relabel_configs]{data-type="index-term"},
            [metric_relabel_configs](#ch08.xhtml#idm45207098023456){data-type="index:locator"}
    -   [Kubelet embedded cAdvisor]{data-type="index-term"},
        [Node](#ch09.xhtml#idm45207097306336){data-type="index:locator"}
    -   [Prometheus scraping MySQLd Exporter]{data-type="index-term"},
        [MySQLd](#ch10.xhtml#idm45207096475968){data-type="index:locator"}
    -   [prometheus.yml to scrape Grok
        Exporter]{data-type="index-term"}, [Grok
        Exporter](#ch10.xhtml#idm45207096201056){data-type="index:locator"}
    -   [prometheus.yml to scrape InfluxDB
        Exporter]{data-type="index-term"},
        [InfluxDB](#ch11.xhtml#idm45207095525504){data-type="index:locator"}
    -   [scraping Prometheus servers]{data-type="index-term"},
        [Horizontal
        Sharding](#ch21.xhtml#idm45207087081376){data-type="index:locator"}
    -   [scraping subset of targets]{data-type="index-term"},
        [Hashmod](#ch21.xhtml#idm45207087121824){data-type="index:locator"}
-   [SD]{data-type="index-term"} ([see]{gentext="see"} service
    discovery)
-   [seconds]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092551056){data-type="index:locator"}
-   [sectors]{data-type="index-term"}, [Diskstats
    Collector](#ch07.xhtml#idm45207100704752){data-type="index:locator"}
-   [selectors]{data-type="index-term"},
    [Selectors](#ch13.xhtml#ix_slct){data-type="index:locator"}-[At
    Modifier](#ch13.xhtml#idm45207092473920){data-type="index:locator"}
    -   [at (@) modifier]{data-type="index-term"}, [At
        Modifier](#ch13.xhtml#idm45207092492464){data-type="index:locator"}
    -   [matchers]{data-type="index-term"},
        [Matchers](#ch13.xhtml#idm45207092636256){data-type="index:locator"}
    -   [offset modifier]{data-type="index-term"},
        [Offset](#ch13.xhtml#idm45207092512496){data-type="index:locator"}
    -   [range vector]{data-type="index-term"}, [Range
        Vector](#ch13.xhtml#idm45207092576384){data-type="index:locator"}
    -   [subqueries]{data-type="index-term"},
        [Subqueries](#ch13.xhtml#idm45207092526624){data-type="index:locator"}
-   [send_resolved field]{data-type="index-term"}, [Resolved
    notifications](#ch19.xhtml#idm45207087939136){data-type="index:locator"}
-   [sensor labels]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100661392){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091144080){data-type="index:locator"},
    [or
    operator](#ch15.xhtml#idm45207091096640){data-type="index:locator"}
-   [sensors command]{data-type="index-term"}, [Hwmon
    Collector](#ch07.xhtml#idm45207100667936){data-type="index:locator"}
-   [server-side security]{data-type="index-term"}, [Server-Side
    Security](#ch20.xhtml#ix_sersec){data-type="index:locator"},
    [Networks and
    Authentication](#ch21.xhtml#idm45207087244608){data-type="index:locator"}
    -   [advanced TLS options]{data-type="index-term"}, [Advanced TLS
        Options](#ch20.xhtml#idm45207087779568){data-type="index:locator"}
    -   [enabling Basic Authentication]{data-type="index-term"},
        [Enabling Basic
        Authentication](#ch20.xhtml#ix_sersecAuth){data-type="index:locator"}
    -   [enabling TLS]{data-type="index-term"}, [Enabling
        TLS](#ch20.xhtml#ix_sersecenTLS){data-type="index:locator"}-[Advanced
        TLS
        Options](#ch20.xhtml#idm45207087781728){data-type="index:locator"}
    -   [security features from Prometheus]{data-type="index-term"},
        [Security Features Provided by
        Prometheus](#ch20.xhtml#idm45207087876608){data-type="index:locator"}
-   [service discovery]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119275504){data-type="index:locator"},
    [Service
    Discovery](#ch01.xhtml#idm45207118472352){data-type="index:locator"},
    [Service
    Discovery](#ch08.xhtml#ix_serdi){data-type="index:locator"}-[Label
    Clashes and
    honor_labels](#ch08.xhtml#idm45207097676176){data-type="index:locator"}
    -   [how to scrape]{data-type="index-term"}, [How to
        Scrape](#ch08.xhtml#ix_serdiscrp){data-type="index:locator"}-[Label
        Clashes and
        honor_labels](#ch08.xhtml#idm45207097678608){data-type="index:locator"}
        -   [label clashes and honor_labels]{data-type="index-term"},
            [Label Clashes and
            honor_labels](#ch08.xhtml#idm45207097743328){data-type="index:locator"}
        -   [metric_relabel_configs]{data-type="index-term"},
            [metric_relabel_configs](#ch08.xhtml#idm45207098024704){data-type="index:locator"}
    -   [Kubernetes]{data-type="index-term"}, [Service
        Discovery](#ch09.xhtml#ix_serdiKube){data-type="index:locator"}-[Ingress](#ch09.xhtml#idm45207096710512){data-type="index:locator"}
    -   [mechanisms]{data-type="index-term"}, [Service Discovery
        Mechanisms](#ch08.xhtml#ix_serdimch){data-type="index:locator"}-[EC2](#ch08.xhtml#idm45207099844992){data-type="index:locator"}
        -   [Consul]{data-type="index-term"},
            [Consul](#ch08.xhtml#idm45207100002720){data-type="index:locator"}
        -   [EC2]{data-type="index-term"},
            [EC2](#ch08.xhtml#ix_serdimchEC2){data-type="index:locator"}-[EC2](#ch08.xhtml#idm45207099847424){data-type="index:locator"}
        -   [file SD]{data-type="index-term"},
            [File](#ch08.xhtml#ix_serdimchfile){data-type="index:locator"}-[HTTP](#ch08.xhtml#idm45207100097488){data-type="index:locator"}
        -   [HTTP SD]{data-type="index-term"},
            [HTTP](#ch08.xhtml#idm45207100094704){data-type="index:locator"}
        -   [static]{data-type="index-term"},
            [Static](#ch08.xhtml#idm45207100488704){data-type="index:locator"}
        -   [top-down versus bottom-up]{data-type="index-term"},
            [Service Discovery
            Mechanisms](#ch08.xhtml#idm45207100495136){data-type="index:locator"}
    -   [problems in]{data-type="index-term"}, [Detecting a
        Problem](#ch21.xhtml#idm45207087144240){data-type="index:locator"}
    -   [relabeling]{data-type="index-term"},
        [Relabeling](#ch08.xhtml#ix_serdirelbl){data-type="index:locator"}-[Lists](#ch08.xhtml#idm45207098350336){data-type="index:locator"}
        -   [choosing what to scrape]{data-type="index-term"}, [Choosing
            What to
            Scrape](#ch08.xhtml#ix_serdirelblch){data-type="index:locator"}-[Choosing
            What to
            Scrape](#ch08.xhtml#idm45207099364576){data-type="index:locator"}
        -   [using for target labels]{data-type="index-term"}, [Target
            Labels](#ch08.xhtml#ix_serdirelbltrgt){data-type="index:locator"}-[Lists](#ch08.xhtml#idm45207098353952){data-type="index:locator"}
-   [service labels]{data-type="index-term"}, [Alert
    Labels](#ch18.xhtml#idm45207089955312){data-type="index:locator"}
-   [services]{data-type="index-term"}
    -   [administering, manual approach to]{data-type="index-term"}, [A
        Brief and Incomplete History of
        Monitoring](#ch01.xhtml#idm45207119804864){data-type="index:locator"}
    -   [instrumentation]{data-type="index-term"}, [Service
        instrumentation](#ch03.xhtml#idm45207115648464){data-type="index:locator"}
    -   [service role in Kubernetes]{data-type="index-term"},
        [Service](#ch09.xhtml#idm45207097270096){data-type="index:locator"}
-   [servlets]{data-type="index-term"},
    [Servlet](#ch04.xhtml#idm45207103131264){data-type="index:locator"}
-   [set method]{data-type="index-term"}, [Using
    Gauges](#ch03.xhtml#idm45207117324144){data-type="index:locator"}
-   [sets]{data-type="index-term"},
    [Enum](#ch05.xhtml#idm45207101404016){data-type="index:locator"}
-   [set_function method]{data-type="index-term"},
    [Callbacks](#ch03.xhtml#idm45207113577904){data-type="index:locator"}
-   [severity labels]{data-type="index-term"}, [Alert
    Labels](#ch18.xhtml#idm45207089961808){data-type="index:locator"},
    [Routing
    Tree](#ch19.xhtml#idm45207089653904){data-type="index:locator"},
    [Inhibitions](#ch19.xhtml#idm45207087918096){data-type="index:locator"}
-   [sgn function]{data-type="index-term"},
    [sgn](#ch16.xhtml#idm45207090872880){data-type="index:locator"}
-   [shard labels]{data-type="index-term"},
    [Pushgateway](#ch04.xhtml#idm45207102356672){data-type="index:locator"}
-   [SIGHUP signal]{data-type="index-term"}, [Using Recording
    Rules](#ch17.xhtml#idm45207090390176){data-type="index:locator"},
    [Configuration
    Management](#ch21.xhtml#idm45207087279040){data-type="index:locator"}
-   [SIGTERM signal]{data-type="index-term"}, [Configuration
    Management](#ch21.xhtml#idm45207087279744){data-type="index:locator"}
-   [silences]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089814608){data-type="index:locator"}
-   [silencing alerts]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089815968){data-type="index:locator"}
-   [simple linear regression]{data-type="index-term"},
    [deriv](#ch16.xhtml#idm45207090551056){data-type="index:locator"}
-   [simpleclient (Java)]{data-type="index-term"},
    [Java](#ch04.xhtml#idm45207103396576){data-type="index:locator"}
-   [single point of failure (SPOF)]{data-type="index-term"}, [Planning
    for
    Failure](#ch21.xhtml#idm45207087212032){data-type="index:locator"}
-   [size, counting]{data-type="index-term"}, [Counting
    Size](#ch03.xhtml#idm45207113159200){data-type="index:locator"}
-   [Slack]{data-type="index-term"}
    -   [customized message]{data-type="index-term"}, [Notification
        templates](#ch19.xhtml#idm45207088075440){data-type="index:locator"}
    -   [message with region and environment]{data-type="index-term"},
        [Notification
        templates](#ch19.xhtml#idm45207088312480){data-type="index:locator"}
    -   [PagerDuty integration with]{data-type="index-term"},
        [Receivers](#ch19.xhtml#idm45207088852176){data-type="index:locator"}
    -   [slack_configs, region and env labels
        in]{data-type="index-term"}, [Notification
        templates](#ch19.xhtml#idm45207088340864){data-type="index:locator"}
-   [SMART metrics]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#idm45207100587712){data-type="index:locator"}
-   [smartctl command]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#idm45207100586976){data-type="index:locator"}
-   [smoothing factor]{data-type="index-term"},
    [holt_winters](#ch16.xhtml#idm45207090511600){data-type="index:locator"}
-   [SMPT]{data-type="index-term"},
    [Alerting](#ch02.xhtml#idm45207118520144){data-type="index:locator"}
-   [snake case for metric names]{data-type="index-term"},
    [snake_case](#ch03.xhtml#idm45207116466560){data-type="index:locator"}
-   [SNMP]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095568144){data-type="index:locator"}
-   [SNMP-style exporters]{data-type="index-term"},
    [Blackbox](#ch10.xhtml#idm45207096139808){data-type="index:locator"},
    [Other Monitoring
    Systems](#ch11.xhtml#idm45207095562960){data-type="index:locator"},
    [Other Monitoring
    Systems](#ch11.xhtml#idm45207095551760){data-type="index:locator"}
    -   ([see also]{gentext="see"} Blackbox exporters)
    -   [default registry and]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093385072){data-type="index:locator"}
-   [software as a service (SaaS) monitoring
    systems]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095558032){data-type="index:locator"}
-   [sort function]{data-type="index-term"}, [Sorting with sort and
    sort_desc](#ch16.xhtml#idm45207090679008){data-type="index:locator"}
-   [sort_desc function]{data-type="index-term"}, [Sorting with sort and
    sort_desc](#ch16.xhtml#idm45207090678336){data-type="index:locator"}
    -   [using by clause]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091596160){data-type="index:locator"}
-   [source_labels]{data-type="index-term"}, [Choosing What to
    Scrape](#ch08.xhtml#idm45207099783712){data-type="index:locator"}
-   [source_match]{data-type="index-term"},
    [Inhibitions](#ch19.xhtml#idm45207087910464){data-type="index:locator"}
-   [sqrt function]{data-type="index-term"},
    [sqrt](#ch16.xhtml#idm45207090919104){data-type="index:locator"}
-   [SSH]{data-type="index-term"},
    [TCP](#ch10.xhtml#idm45207096021024){data-type="index:locator"}
-   [ssh_banner module]{data-type="index-term"},
    [TCP](#ch10.xhtml#idm45207096012880){data-type="index:locator"}
-   [stability guarantees, Prometheus versions]{data-type="index-term"},
    [Running
    Prometheus](#ch02.xhtml#idm45207113417680){data-type="index:locator"}
-   [stale markers]{data-type="index-term"}, [Instant
    Vector](#ch13.xhtml#idm45207092585600){data-type="index:locator"}
-   [staleness]{data-type="index-term"}, [Instant
    Vector](#ch13.xhtml#idm45207092587296){data-type="index:locator"}
    -   [range vectors and]{data-type="index-term"}, [Range
        Vector](#ch13.xhtml#idm45207092557488){data-type="index:locator"}
    -   [for resolved alerts]{data-type="index-term"}, [Alerting
        Rules](#ch18.xhtml#idm45207090021088){data-type="index:locator"}
-   [stalled metrics]{data-type="index-term"}, [Pressure
    Collector](#ch07.xhtml#idm45207100598736){data-type="index:locator"}
-   [standard deviation]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091479760){data-type="index:locator"}
    -   ([see also]{gentext="see"} stddev)
-   [standard variance]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091465712){data-type="index:locator"}
    -   ([see also]{gentext="see"} stdvar)
-   [start]{data-type="index-term"}, [Aligned
    data](#ch13.xhtml#idm45207091801488){data-type="index:locator"}
-   [start function]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092480624){data-type="index:locator"}
-   [start time]{data-type="index-term"},
    [query_range](#ch13.xhtml#idm45207091959056){data-type="index:locator"}
-   [start_http_server]{data-type="index-term"},
    [Python](#ch04.xhtml#idm45207116385744){data-type="index:locator"}
-   [stat collector]{data-type="index-term"}, [Stat
    Collector](#ch07.xhtml#idm45207100652192){data-type="index:locator"}
-   [Stat panel]{data-type="index-term"}, [Stat
    Panel](#ch06.xhtml#ix_Statpnl){data-type="index:locator"}-[Stat
    Panel](#ch06.xhtml#idm45207100880384){data-type="index:locator"}
-   [State timeline panel]{data-type="index-term"}, [State Timeline
    Panel](#ch06.xhtml#idm45207100861104){data-type="index:locator"}
-   [StateSets]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102098880){data-type="index:locator"}
-   [statfs system call]{data-type="index-term"}, [Filesystem
    Collector](#ch07.xhtml#idm45207100745008){data-type="index:locator"}
-   [static config service discovery]{data-type="index-term"},
    [Static](#ch08.xhtml#idm45207100489456){data-type="index:locator"}
-   [static typing (PromQL)]{data-type="index-term"},
    [Functions](#ch16.xhtml#idm45207090992592){data-type="index:locator"}
-   [static_configs]{data-type="index-term"},
    [Static](#ch08.xhtml#idm45207100443664){data-type="index:locator"},
    [Missing Series, absent, and
    absent_over_time](#ch16.xhtml#idm45207090688720){data-type="index:locator"}
-   [StatsD]{data-type="index-term"}, [Other Monitoring
    Systems](#ch11.xhtml#idm45207095572496){data-type="index:locator"},
    [StatsD](#ch11.xhtml#ix_StD){data-type="index:locator"}-[StatsD](#ch11.xhtml#idm45207095388560){data-type="index:locator"}
-   [StatsD Exporter]{data-type="index-term"}, [Planning a
    Rollout](#ch21.xhtml#idm45207087503280){data-type="index:locator"}
-   [stddev]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091480432){data-type="index:locator"}
-   [stddev_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090493984){data-type="index:locator"}
-   [stdvar]{data-type="index-term"}, [stddev and
    stdvar](#ch14.xhtml#idm45207091464704){data-type="index:locator"}
-   [stdvar_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090493312){data-type="index:locator"}
-   [step]{data-type="index-term"},
    [query_range](#ch13.xhtml#idm45207091959792){data-type="index:locator"},
    [Aligned
    data](#ch13.xhtml#idm45207091664832){data-type="index:locator"}
    -   [maximum number for query_range]{data-type="index-term"},
        [query_range](#ch13.xhtml#idm45207091813360){data-type="index:locator"}
-   [storage]{data-type="index-term"},
    [Storage](#ch01.xhtml#idm45207119362640){data-type="index:locator"}
    -   [long-term]{data-type="index-term"}, [Long-Term
        Storage](#ch01.xhtml#idm45207118957216){data-type="index:locator"},
        [Long-Term
        Storage](#ch21.xhtml#ix_storeLT){data-type="index:locator"}-[Long-Term
        Storage](#ch21.xhtml#idm45207087319776){data-type="index:locator"}
-   [storage layer, problem in]{data-type="index-term"}, [Detecting a
    Problem](#ch21.xhtml#idm45207087140832){data-type="index:locator"}
-   [string type]{data-type="index-term"}, [Working with
    Scalars](#ch15.xhtml#idm45207091350336){data-type="index:locator"}
-   [subqueries]{data-type="index-term"},
    [Subqueries](#ch13.xhtml#idm45207092524432){data-type="index:locator"},
    [Composing Range Vector
    Functions](#ch17.xhtml#idm45207090175936){data-type="index:locator"}
-   [suffixes (metrics)]{data-type="index-term"}, [Using
    Gauges](#ch03.xhtml#idm45207113642160){data-type="index:locator"},
    [Metric
    suffixes](#ch03.xhtml#idm45207116461904){data-type="index:locator"}
-   [sum]{data-type="index-term"},
    [Info](#ch05.xhtml#idm45207101047632){data-type="index:locator"},
    [Cardinality](#ch05.xhtml#idm45207101022784){data-type="index:locator"},
    [avg](#ch14.xhtml#idm45207091504432){data-type="index:locator"}
    -   [about]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091565616){data-type="index:locator"}
    -   [by clause]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101430336){data-type="index:locator"},
        [by](#ch14.xhtml#idm45207091608464){data-type="index:locator"}
        -   [using empty by or omitting by]{data-type="index-term"},
            [by](#ch14.xhtml#idm45207091585184){data-type="index:locator"}
    -   [using in counter aggregation]{data-type="index-term"},
        [Counter](#ch13.xhtml#idm45207092740032){data-type="index:locator"}
    -   [using in histogram of aggregates]{data-type="index-term"},
        [Histogram](#ch13.xhtml#idm45207092672304){data-type="index:locator"}
    -   [using rate before]{data-type="index-term"}, [Composing Range
        Vector
        Functions](#ch17.xhtml#idm45207090169040){data-type="index:locator"}
    -   [using with gauges]{data-type="index-term"},
        [Gauge](#ch13.xhtml#idm45207092787568){data-type="index:locator"}
    -   [using with summary]{data-type="index-term"},
        [Summary](#ch13.xhtml#idm45207092715136){data-type="index:locator"}
    -   [without clause]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101464048){data-type="index:locator"},
        [Enum](#ch05.xhtml#idm45207101405808){data-type="index:locator"},
        [without](#ch14.xhtml#idm45207091631456){data-type="index:locator"}
-   [summary]{data-type="index-term"}, [The
    Summary](#ch03.xhtml#idm45207113551728){data-type="index:locator"}
    -   [aggregating]{data-type="index-term"},
        [Summary](#ch13.xhtml#idm45207092724544){data-type="index:locator"}
    -   [example in Prometheus text exposition
        format]{data-type="index-term"},
        [Labels](#ch04.xhtml#idm45207102147888){data-type="index:locator"}
    -   [metrics.Sample]{data-type="index-term"}, [Custom
        Collectors](#ch12.xhtml#idm45207093696608){data-type="index:locator"}
    -   [text exposition format for]{data-type="index-term"}, [Metric
        Types](#ch04.xhtml#idm45207102169568){data-type="index:locator"}
    -   [using rate before sum on]{data-type="index-term"},
        [sum](#ch14.xhtml#idm45207091558256){data-type="index:locator"}
-   [sum_over_time]{data-type="index-term"}, [Aggregation Over
    Time](#ch16.xhtml#idm45207090496032){data-type="index:locator"}
-   [symptoms, alerting on]{data-type="index-term"}, [What Are Good
    Alerts?](#ch18.xhtml#idm45207089882464){data-type="index:locator"},
    [Grouping](#ch19.xhtml#idm45207089238144){data-type="index:locator"},
    [Inhibitions](#ch19.xhtml#idm45207087907808){data-type="index:locator"}
-   [systemd]{data-type="index-term"},
    [cAdvisor](#ch09.xhtml#idm45207097651360){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### T

-   [table exceptions]{data-type="index-term"}, [When to Use
    Labels](#ch05.xhtml#idm45207101032944){data-type="index:locator"}
-   [Table panel]{data-type="index-term"}, [Table
    Panel](#ch06.xhtml#idm45207100875568){data-type="index:locator"}
-   [target labels]{data-type="index-term"}, [Instrumentation and Target
    Labels](#ch05.xhtml#idm45207102043488){data-type="index:locator"},
    [Service
    Discovery](#ch08.xhtml#idm45207100511184){data-type="index:locator"},
    [Target
    Labels](#ch08.xhtml#ix_trgtlbl){data-type="index:locator"}-[Lists](#ch08.xhtml#idm45207098351280){data-type="index:locator"}
    -   [clashes in and honor_labels]{data-type="index-term"}, [Label
        Clashes and
        honor_labels](#ch08.xhtml#idm45207097738288){data-type="index:locator"}
    -   [hierarchy of]{data-type="index-term"}, [Target
        Labels](#ch08.xhtml#idm45207099349728){data-type="index:locator"}
    -   [instance target label]{data-type="index-term"}, [Uname
        Collector](#ch07.xhtml#idm45207100630704){data-type="index:locator"}
    -   [job and instance]{data-type="index-term"},
        [Grouping](#ch14.xhtml#idm45207091641552){data-type="index:locator"}
    -   [metadata for, in file SD]{data-type="index-term"},
        [File](#ch08.xhtml#idm45207100158416){data-type="index:locator"}
    -   [using relabeling to specify]{data-type="index-term"}
        -   [case]{data-type="index-term"},
            [Case](#ch08.xhtml#idm45207098653088){data-type="index:locator"}
        -   [job, instance, and
            \_\_address\_\_]{data-type="index-term"}, [job, instance,
            and
            \_\_address\_\_](#ch08.xhtml#idm45207098892704){data-type="index:locator"}
        -   [labelmap action]{data-type="index-term"},
            [labelmap](#ch08.xhtml#idm45207098786560){data-type="index:locator"}
        -   [lists]{data-type="index-term"},
            [Lists](#ch08.xhtml#idm45207098512368){data-type="index:locator"}
        -   [replace action]{data-type="index-term"},
            [replace](#ch08.xhtml#idm45207099341104){data-type="index:locator"}
    -   [variances in]{data-type="index-term"},
        [without](#ch14.xhtml#idm45207091635792){data-type="index:locator"}
-   [targets]{data-type="index-term"}
    -   [creating in static config service
        discovery]{data-type="index-term"},
        [Static](#ch08.xhtml#idm45207100477536){data-type="index:locator"}
    -   [discovered by EC2]{data-type="index-term"},
        [EC2](#ch08.xhtml#idm45207099814160){data-type="index:locator"}
    -   [for each individual application
        instance]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099445600){data-type="index:locator"}
    -   [finding expensive targets]{data-type="index-term"}, [Finding
        Expensive Metrics and
        Targets](#ch21.xhtml#idm45207087135408){data-type="index:locator"}
    -   [in HTTP service discovery]{data-type="index-term"},
        [HTTP](#ch08.xhtml#idm45207100093488){data-type="index:locator"}
    -   [mapping from metadata using
        relabeling]{data-type="index-term"},
        [Relabeling](#ch08.xhtml#idm45207099798656){data-type="index:locator"}
    -   [provided by Consul service discovery]{data-type="index-term"},
        [Consul](#ch08.xhtml#idm45207099892576){data-type="index:locator"}
    -   [scraping subset of]{data-type="index-term"},
        [Hashmod](#ch21.xhtml#idm45207087120912){data-type="index:locator"}
    -   [spotting metric missing from target using
        unless]{data-type="index-term"}, [unless
        operator](#ch15.xhtml#idm45207091063456){data-type="index:locator"}
-   [Targets page]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207116159712){data-type="index:locator"}
    -   [scrape errors on]{data-type="index-term"},
        [Alerting](#ch02.xhtml#idm45207118792256){data-type="index:locator"}
    -   [showing Prometheus and Node Exporter]{data-type="index-term"},
        [Running the Node
        Exporter](#ch02.xhtml#idm45207118847024){data-type="index:locator"}
-   [target_match]{data-type="index-term"},
    [Inhibitions](#ch19.xhtml#idm45207087909760){data-type="index:locator"}
-   [TCP probing]{data-type="index-term"},
    [TCP](#ch10.xhtml#ix_TCP){data-type="index:locator"}-[TCP](#ch10.xhtml#idm45207095986016){data-type="index:locator"}
    -   [checking if local SSH server is listening on port
        22]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207096020320){data-type="index:locator"}
    -   [ssh_banner module]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207096012176){data-type="index:locator"}
    -   [tcp_connect module]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207096016016){data-type="index:locator"}
    -   [tcp_connect_tls]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207096000208){data-type="index:locator"}
-   [tcpdump]{data-type="index-term"},
    [Profiling](#ch01.xhtml#idm45207119684672){data-type="index:locator"}
-   [team labels]{data-type="index-term"},
    [File](#ch08.xhtml#idm45207100135664){data-type="index:locator"},
    [Alert
    Labels](#ch18.xhtml#idm45207089956048){data-type="index:locator"}
    -   [EC2 instances]{data-type="index-term"}, [Choosing What to
        Scrape](#ch08.xhtml#idm45207099448064){data-type="index:locator"}
    -   [removing using replace relabel action]{data-type="index-term"},
        [replace](#ch08.xhtml#idm45207099181824){data-type="index:locator"}
    -   [using defaults to remove succinctly]{data-type="index-term"},
        [replace](#ch08.xhtml#idm45207099052832){data-type="index:locator"}
    -   [using replace relabel action with]{data-type="index-term"},
        [replace](#ch08.xhtml#idm45207099331824){data-type="index:locator"}
-   [teams]{data-type="index-term"}
    -   [alerts grouped by]{data-type="index-term"},
        [Grouping](#ch19.xhtml#idm45207089166928){data-type="index:locator"}
    -   [routing alerts and notifications to]{data-type="index-term"},
        [Routing
        Tree](#ch19.xhtml#idm45207089556864){data-type="index:locator"}
-   [templating]{data-type="index-term"}
    -   [alerts]{data-type="index-term"}, [Annotations and
        Templates](#ch18.xhtml#ix_tmplalrt){data-type="index:locator"}-[Annotations
        and
        Templates](#ch18.xhtml#idm45207089891968){data-type="index:locator"}
    -   [Go templating language]{data-type="index-term"}, [Dashboarding
        with
        Grafana](#ch06.xhtml#idm45207100995072){data-type="index:locator"}
    -   [of notifications by Alertmanager]{data-type="index-term"},
        [Notification
        templates](#ch19.xhtml#ix_tmplnoti){data-type="index:locator"}-[Notification
        templates](#ch19.xhtml#idm45207087988192){data-type="index:locator"}
    -   [template variables in Grafana]{data-type="index-term"},
        [Template
        Variables](#ch06.xhtml#idm45207100842896){data-type="index:locator"}
    -   [using Ansible jinja2 to create targets for Node
        Exporter]{data-type="index-term"},
        [Static](#ch08.xhtml#idm45207100478992){data-type="index:locator"}
-   [text format (Prometheus)]{data-type="index-term"}, [What Is
    Prometheus?](#ch01.xhtml#idm45207119488144){data-type="index:locator"},
    [Text Exposition
    Format](#ch04.xhtml#idm45207102214048){data-type="index:locator"}
    -   ([see also]{gentext="see"} exposition formats)
    -   [parsers for]{data-type="index-term"},
        [Parsers](#ch04.xhtml#idm45207102294272){data-type="index:locator"}
-   [textfile collector]{data-type="index-term"}, [Textfile
    Collector](#ch07.xhtml#ix_txtfl){data-type="index:locator"}-[Timestamps](#ch07.xhtml#idm45207100531616){data-type="index:locator"}
    -   [timestamps]{data-type="index-term"},
        [Timestamps](#ch07.xhtml#idm45207100539552){data-type="index:locator"}
    -   [using]{data-type="index-term"}, [Using the Textfile
        Collector](#ch07.xhtml#idm45207100568912){data-type="index:locator"}
-   [thread and worker pools, instrumentation]{data-type="index-term"},
    [Library
    instrumentation](#ch03.xhtml#idm45207116494416){data-type="index:locator"}
-   [throttling notifications]{data-type="index-term"}, [Notification
    Pipeline](#ch19.xhtml#idm45207089804864){data-type="index:locator"},
    [Throttling and
    repetition](#ch19.xhtml#idm45207089039440){data-type="index:locator"}
-   [time]{data-type="index-term"}, [Using
    Gauges](#ch03.xhtml#idm45207113741072){data-type="index:locator"},
    [Units](#ch03.xhtml#idm45207116455680){data-type="index:locator"}
    -   [context manager and function
        decorator]{data-type="index-term"}, [The
        Summary](#ch03.xhtml#idm45207119189472){data-type="index:locator"}
    -   [durations in Prometheus as used in
        PromQL]{data-type="index-term"}, [Range
        Vector](#ch13.xhtml#idm45207092552096){data-type="index:locator"}
    -   [precision in Prometheus]{data-type="index-term"}, [The
        Summary](#ch03.xhtml#idm45207119192304){data-type="index:locator"}
    -   [time controls in Grafana]{data-type="index-term"}, [Time
        Controls](#ch06.xhtml#idm45207100915872){data-type="index:locator"}
    -   [time URL parameter]{data-type="index-term"},
        [query](#ch13.xhtml#idm45207092348112){data-type="index:locator"}
    -   [tracking latency]{data-type="index-term"}, [The
        Summary](#ch03.xhtml#idm45207113548256){data-type="index:locator"}
-   [time and date functions]{data-type="index-term"}, [Time and
    Date](#ch16.xhtml#ix_tmdafnc){data-type="index:locator"}-[timestamp](#ch16.xhtml#idm45207090751584){data-type="index:locator"}
-   [time function]{data-type="index-term"},
    [time](#ch16.xhtml#idm45207090826336){data-type="index:locator"}
-   [time series]{data-type="index-term"},
    [Metric](#ch05.xhtml#idm45207101857600){data-type="index:locator"}
    -   [finding how many have same name]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091594016){data-type="index:locator"}
    -   [number of, or cardinality]{data-type="index-term"},
        [Cardinality](#ch05.xhtml#idm45207101026304){data-type="index:locator"}
    -   [total of rest of metric, avoiding]{data-type="index-term"},
        [When to Use
        Labels](#ch05.xhtml#idm45207101038816){data-type="index:locator"}
-   [Time series panel]{data-type="index-term"}, [Time Series
    Panel](#ch06.xhtml#ix_tmserpnl){data-type="index:locator"}-[Time
    Controls](#ch06.xhtml#idm45207100898384){data-type="index:locator"}
    -   [time controls]{data-type="index-term"}, [Time
        Controls](#ch06.xhtml#idm45207100916848){data-type="index:locator"}
-   [timers]{data-type="index-term"}, [Consul
    Telemetry](#ch12.xhtml#idm45207095142368){data-type="index:locator"}
-   [timestamp function]{data-type="index-term"},
    [timestamp](#ch16.xhtml#idm45207090764880){data-type="index:locator"}
-   [timestamps]{data-type="index-term"}
    -   [aligning with start time and step]{data-type="index-term"},
        [query_range](#ch13.xhtml#idm45207091738128){data-type="index:locator"}
    -   [exporters exposing, staleness and]{data-type="index-term"},
        [Instant
        Vector](#ch13.xhtml#idm45207092581152){data-type="index:locator"}
    -   [InfluxDB and]{data-type="index-term"},
        [InfluxDB](#ch11.xhtml#idm45207095527152){data-type="index:locator"}
    -   [in OpenMetrics format]{data-type="index-term"},
        [Timestamps](#ch04.xhtml#idm45207102075408){data-type="index:locator"}
    -   [in Prometheus text exposition format]{data-type="index-term"},
        [Timestamps](#ch04.xhtml#idm45207102132256){data-type="index:locator"}
    -   [returned by instant vector selector]{data-type="index-term"},
        [Instant
        Vector](#ch13.xhtml#idm45207092591376){data-type="index:locator"}
    -   [textfile collector and]{data-type="index-term"},
        [Timestamps](#ch07.xhtml#idm45207100538576){data-type="index:locator"}
    -   [Unix, use of @ modifier with]{data-type="index-term"}, [At
        Modifier](#ch13.xhtml#idm45207092487264){data-type="index:locator"}
-   [TLS]{data-type="index-term"}, [Enabling Basic
    Authentication](#ch20.xhtml#idm45207087692576){data-type="index:locator"}
    -   [advanced options]{data-type="index-term"}, [Advanced TLS
        Options](#ch20.xhtml#idm45207087780512){data-type="index:locator"}
    -   [enabling]{data-type="index-term"}, [Enabling
        TLS](#ch20.xhtml#ix_TLSen){data-type="index:locator"}-[Advanced
        TLS
        Options](#ch20.xhtml#idm45207087782976){data-type="index:locator"}
    -   [tcp probe connecting via]{data-type="index-term"},
        [TCP](#ch10.xhtml#idm45207095999232){data-type="index:locator"}
-   [TLS certificates]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097380400){data-type="index:locator"}
-   [TLS client authentication]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098249024){data-type="index:locator"}
-   [TLS/SSL certificate expiring]{data-type="index-term"},
    [TCP](#ch10.xhtml#idm45207095990416){data-type="index:locator"}
-   [tls_config]{data-type="index-term"},
    [Node](#ch09.xhtml#idm45207097312944){data-type="index:locator"}
-   [tls_server_config]{data-type="index-term"}, [Advanced TLS
    Options](#ch20.xhtml#idm45207087778624){data-type="index:locator"}
-   [\_\_tmp prefix for labels]{data-type="index-term"},
    [Lists](#ch08.xhtml#idm45207098354864){data-type="index:locator"}
-   [topk operator]{data-type="index-term"}, [At
    Modifier](#ch13.xhtml#idm45207092475808){data-type="index:locator"},
    [min and
    max](#ch14.xhtml#idm45207091454256){data-type="index:locator"}
-   [tracing]{data-type="index-term"},
    [Tracing](#ch01.xhtml#idm45207119675360){data-type="index:locator"}
-   [transaction logs]{data-type="index-term"},
    [Logging](#ch01.xhtml#idm45207114287312){data-type="index:locator"}
-   [trend factor]{data-type="index-term"},
    [holt_winters](#ch16.xhtml#idm45207090510896){data-type="index:locator"}
-   [trending]{data-type="index-term"}, [What Is
    Monitoring?](#ch01.xhtml#idm45207119497168){data-type="index:locator"}
-   [trigonometric functions]{data-type="index-term"}, [Trigonometric
    Functions](#ch16.xhtml#idm45207090864304){data-type="index:locator"}
-   [trigonometric operators]{data-type="index-term"}, [Trigonometric
    Operator](#ch15.xhtml#idm45207091308032){data-type="index:locator"}
-   [Twisted]{data-type="index-term"},
    [Twisted](#ch04.xhtml#idm45207116238016){data-type="index:locator"}
-   [TYPE (metrics)]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102178464){data-type="index:locator"},
    [Metric
    Types](#ch04.xhtml#idm45207102092800){data-type="index:locator"}
-   [types]{data-type="index-term"}
    -   [changing]{data-type="index-term"}, [Changing
        Type](#ch16.xhtml#idm45207090986272){data-type="index:locator"}
    -   [static typing in PromQL]{data-type="index-term"},
        [Functions](#ch16.xhtml#idm45207090993952){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### U

-   [uberagent]{data-type="index-term"}, [Node
    Exporter](#ch07.xhtml#idm45207100792512){data-type="index:locator"}
-   [uname collector]{data-type="index-term"}, [Uname
    Collector](#ch07.xhtml#idm45207100636944){data-type="index:locator"}
-   [unary operators]{data-type="index-term"}, [Binary
    Operators](#ch15.xhtml#idm45207091360912){data-type="index:locator"}
-   [UNIT metadata (metrics)]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102092128){data-type="index:locator"}
-   [unit tests for instrumentation]{data-type="index-term"}, [Unit
    Testing
    Instrumentation](#ch03.xhtml#idm45207115717520){data-type="index:locator"}
-   [units]{data-type="index-term"}
    -   [base units in Prometheus]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113696304){data-type="index:locator"}
    -   [in metric names]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113637024){data-type="index:locator"},
        [Units](#ch03.xhtml#idm45207116456896){data-type="index:locator"}
    -   [quantiles and percentiles]{data-type="index-term"}, [The
        Histogram](#ch03.xhtml#idm45207119107552){data-type="index:locator"}
-   [Unix]{data-type="index-term"}
    -   [epoch]{data-type="index-term"},
        [Timestamps](#ch04.xhtml#idm45207102129776){data-type="index:locator"},
        [Timestamps](#ch04.xhtml#idm45207102071872){data-type="index:locator"}
    -   [filesystems]{data-type="index-term"}, [Filesystem
        Collector](#ch07.xhtml#idm45207100736096){data-type="index:locator"}
    -   [Note Exporter exposing metrics on]{data-type="index-term"},
        [Running the Node
        Exporter](#ch02.xhtml#idm45207118075232){data-type="index:locator"}
    -   [time]{data-type="index-term"}, [Using
        Gauges](#ch03.xhtml#idm45207113698000){data-type="index:locator"}
-   [unless operator]{data-type="index-term"}, [unless
    operator](#ch15.xhtml#idm45207091068400){data-type="index:locator"}
-   [untyped (metric type)]{data-type="index-term"}, [Metric
    Types](#ch04.xhtml#idm45207102172224){data-type="index:locator"}
-   [UntypedValue]{data-type="index-term"}, [Custom
    Collectors](#ch12.xhtml#idm45207094065600){data-type="index:locator"}
-   [up]{data-type="index-term"}, [Using the Expression
    Browser](#ch02.xhtml#idm45207118610784){data-type="index:locator"},
    [Running the Node
    Exporter](#ch02.xhtml#idm45207118846048){data-type="index:locator"},
    [Alerting](#ch02.xhtml#idm45207118778656){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091160672){data-type="index:locator"}
    -   [added by Prometheus after each scrape]{data-type="index-term"},
        [or
        operator](#ch15.xhtml#idm45207091077568){data-type="index:locator"}
    -   [adding version label from python_info to
        all]{data-type="index-term"},
        [Info](#ch05.xhtml#idm45207101096896){data-type="index:locator"}
    -   [consul_up]{data-type="index-term"},
        [Consul](#ch10.xhtml#idm45207096596256){data-type="index:locator"}
    -   [displaying up metrics in Grafana State timeline
        panel]{data-type="index-term"}, [State Timeline
        Panel](#ch06.xhtml#idm45207100858656){data-type="index:locator"}
    -   [failed scrapes and]{data-type="index-term"},
        [for](#ch18.xhtml#idm45207089973680){data-type="index:locator"}
-   [uppercase (relabel action)]{data-type="index-term"},
    [Case](#ch08.xhtml#idm45207098651136){data-type="index:locator"}
-   [URL parameters (scrape config)]{data-type="index-term"}, [How to
    Scrape](#ch08.xhtml#idm45207098255072){data-type="index:locator"}
-   [USE method (utilization, saturation, and
    errors)]{data-type="index-term"}, [Service
    instrumentation](#ch03.xhtml#idm45207116512944){data-type="index:locator"}
-   [user mode]{data-type="index-term"}, [CPU
    Collector](#ch07.xhtml#idm45207100757152){data-type="index:locator"}
-   [UTC (Coordinated Universal Time)]{data-type="index-term"}, [minute,
    hour, day_of_week, day_of_month, day_of_year, days_in_month, month,
    and year](#ch16.xhtml#idm45207090796256){data-type="index:locator"}
-   [UTF-8 encoding]{data-type="index-term"},
    [Escaping](#ch04.xhtml#idm45207102141024){data-type="index:locator"}
    -   [label values]{data-type="index-term"},
        [Instrumentation](#ch05.xhtml#idm45207101868432){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### V

-   [vector function]{data-type="index-term"},
    [vector](#ch16.xhtml#idm45207090981920){data-type="index:locator"}
-   [vector matching]{data-type="index-term"}, [Vector
    Matching](#ch15.xhtml#ix_vecmtch){data-type="index:locator"}-[and
    operator](#ch15.xhtml#idm45207091031376){data-type="index:locator"}
    -   [many-to-many and logical operators]{data-type="index-term"},
        [Many-to-Many and Logical
        Operators](#ch15.xhtml#ix_vecmthmnymnylog){data-type="index:locator"}-[and
        operator](#ch15.xhtml#idm45207091036224){data-type="index:locator"}
    -   [many-to-one and group_left]{data-type="index-term"},
        [Many-to-One and
        group_left](#ch15.xhtml#ix_vecmtchmtoo){data-type="index:locator"}-[Many-to-One
        and
        group_left](#ch15.xhtml#idm45207091128416){data-type="index:locator"}
    -   [one-to-one]{data-type="index-term"},
        [One-to-One](#ch15.xhtml#idm45207091229440){data-type="index:locator"}
-   [vectors]{data-type="index-term"},
    [Selectors](#ch13.xhtml#idm45207092641792){data-type="index:locator"}
    -   ([see also]{gentext="see"} instant vector selector)
-   [version labels]{data-type="index-term"},
    [Info](#ch05.xhtml#idm45207101095920){data-type="index:locator"},
    [Many-to-One and
    group_left](#ch15.xhtml#idm45207091156624){data-type="index:locator"}
-   [versions of Prometheus]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207113424416){data-type="index:locator"}
-   [vertical sharding]{data-type="index-term"}, [Growing
    Prometheus](#ch21.xhtml#idm45207087491744){data-type="index:locator"},
    [Horizontal
    Sharding](#ch21.xhtml#idm45207087083472){data-type="index:locator"}
-   [Vim]{data-type="index-term"},
    [Scraping](#ch01.xhtml#idm45207119365568){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### W

-   [waiting metrics]{data-type="index-term"}, [Pressure
    Collector](#ch07.xhtml#idm45207100599520){data-type="index:locator"}
-   [wall of graphs, avoiding]{data-type="index-term"}, [Avoiding the
    Wall of
    Graphs](#ch06.xhtml#idm45207100950768){data-type="index:locator"}
-   [web configuration files]{data-type="index-term"}, [Enabling
    TLS](#ch20.xhtml#idm45207087862368){data-type="index:locator"}
    -   [launching Prometheus with]{data-type="index-term"}, [Enabling
        TLS](#ch20.xhtml#idm45207087855152){data-type="index:locator"}
-   [web interface (Alertmanager)]{data-type="index-term"},
    [Alertmanager Web
    Interface](#ch19.xhtml#idm45207087900864){data-type="index:locator"}
-   [Web Server Gateway Interface (WSGI)]{data-type="index-term"},
    [WSGI](#ch04.xhtml#idm45207116348624){data-type="index:locator"}
-   [\--web.enable-lifecycle flag]{data-type="index-term"}, [Using
    Recording
    Rules](#ch17.xhtml#idm45207090383360){data-type="index:locator"},
    [Configuration
    Management](#ch21.xhtml#idm45207087275776){data-type="index:locator"}
-   [\--web.external-url flag]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#idm45207087239632){data-type="index:locator"}
-   [\--web.route-prefix flag]{data-type="index-term"}, [Networks and
    Authentication](#ch21.xhtml#idm45207087237488){data-type="index:locator"}
-   [webhook notifier]{data-type="index-term"},
    [Receivers](#ch19.xhtml#idm45207088857264){data-type="index:locator"},
    [Receivers](#ch19.xhtml#idm45207088598400){data-type="index:locator"},
    [Notification
    templates](#ch19.xhtml#idm45207088344224){data-type="index:locator"}
-   [webhook receiver]{data-type="index-term"},
    [Receivers](#ch19.xhtml#idm45207088596000){data-type="index:locator"}
    -   [written in Python 3]{data-type="index-term"},
        [Receivers](#ch19.xhtml#idm45207088532880){data-type="index:locator"}
-   [weeks]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092548368){data-type="index:locator"}
-   [Windows Exporter]{data-type="index-term"}, [Node
    Exporter](#ch07.xhtml#idm45207100795696){data-type="index:locator"}
-   [WithLabelValues]{data-type="index-term"},
    [Instrumentation](#ch05.xhtml#idm45207102026544){data-type="index:locator"}
-   [without clause]{data-type="index-term"},
    [without](#ch14.xhtml#idm45207091636496){data-type="index:locator"}
    -   [avg without]{data-type="index-term"},
        [avg](#ch14.xhtml#idm45207091507952){data-type="index:locator"},
        [Alerting
        Rules](#ch18.xhtml#idm45207090040496){data-type="index:locator"}
    -   [by clause versus]{data-type="index-term"},
        [by](#ch14.xhtml#idm45207091600624){data-type="index:locator"}
    -   [count without]{data-type="index-term"},
        [count](#ch14.xhtml#idm45207091543568){data-type="index:locator"}
    -   [quantile without]{data-type="index-term"},
        [quantile](#ch14.xhtml#idm45207091412992){data-type="index:locator"}
    -   [sum without]{data-type="index-term"},
        [Aggregating](#ch05.xhtml#idm45207101463104){data-type="index:locator"},
        [without](#ch14.xhtml#idm45207091630480){data-type="index:locator"}
-   [worker pools, instrumentation]{data-type="index-term"}, [Library
    instrumentation](#ch03.xhtml#idm45207116493616){data-type="index:locator"}
-   [write_relabel_configs]{data-type="index-term"}, [Long-Term
    Storage](#ch21.xhtml#idm45207087328592){data-type="index:locator"}
-   [writing exporters]{data-type="index-term"} ([see]{gentext="see"}
    exporters)
-   [WSGI (Web Server Gateway Interface)]{data-type="index-term"},
    [WSGI](#ch04.xhtml#idm45207116349264){data-type="index:locator"}
:::

::: {data-type="indexdiv"}
### Y

-   [YAML]{data-type="index-term"}, [Running
    Prometheus](#ch02.xhtml#idm45207113467744){data-type="index:locator"},
    [File](#ch08.xhtml#idm45207100296368){data-type="index:locator"}
    -   [multiline strings in]{data-type="index-term"}, [Reducing
        Cardinality](#ch17.xhtml#idm45207090198064){data-type="index:locator"}
-   [year function]{data-type="index-term"},
    [scalar](#ch16.xhtml#idm45207090964048){data-type="index:locator"},
    [minute, hour, day_of_week, day_of_month, day_of_year,
    days_in_month, month, and
    year](#ch16.xhtml#idm45207090808896){data-type="index:locator"}
-   [years]{data-type="index-term"}, [Range
    Vector](#ch13.xhtml#idm45207092547696){data-type="index:locator"}
:::
:::
:::
:::
:::

[]{#colophon01.xhtml}

::: {#colophon01.xhtml#sbo-rt-content}
::: {.section .abouttheauthor .colophon pdf-bookmark="About the Authors" data-type="colophon"}
::: {#colophon01.xhtml#idm45207087048096 .colophon}
# About the Authors

**Julien Pivotto** is a leading contributor to the Prometheus server and
the CNCF ecosystem, having made significant contributions since 2017.
Currently, he is a principal software architect and cofounder at O11y,
where he specializes in providing top-tier support for various
observability tools, including Prometheus, Cortex, Loki, and Jaeger.
With years of experience, he is dedicated to helping organizations with
the deployment and maintenance of these tools, as well as providing
custom development solutions.

**Brian Brazil** is the founder of Robust Perception and a Prometheus
developer. He works on monitoring issues with companies ranging from
early-stage startups to Fortune 500 corporations. He is well known in
the Prometheus community, has given countless presentations at
conferences, and covers many aspects of Prometheus and monitoring on his
blog on the Robust Perception website.
:::
:::
:::

[]{#colophon02.xhtml}

::: {#colophon02.xhtml#sbo-rt-content}
::: {.section .colophon pdf-bookmark="Colophon" data-type="colophon"}
::: {#colophon02.xhtml#colophon .colophon}
# Colophon

The animal on the cover of *Prometheus: Up & Running* is the tawny eagle
(*Aquila rapax*), a bird of prey native to Africa, the Middle East, and
India. Measuring 60--75 inches in length with a wingspan of 63--75
inches, the tawny eagle is slightly smaller than other members of the
*Aquila* genus. They are brown in color, with their eponymous tawny
coloration most prevalent in the upper body, giving way to darker
feathers on the tail.

Tawny eagles tend to make their nests atop tall trees, where monogamous
breeding pairs lay one to three eggs annually. They favor dry habitats
such as deserts, steppes, and savannas in which they feed on carrion,
reptiles, and small mammals.

Due to their widespread habitat range, tawny eagles are not believed to
be threatened. However, the tawny eagle population is thought to be
declining in West Africa due to the encroachment of cultivated land into
their habitat.

Many of the animals on O'Reilly covers are endangered; all of them are
important to the world.

The cover illustration is by Karen Montgomery, based on an antique line
engraving from *British Birds*. The cover fonts are Gilroy Semibold and
Guardian Sans. The text font is Adobe Minion Pro; the heading font is
Adobe Myriad Condensed; and the code font is Dalton Maag's Ubuntu Mono.
:::
:::
:::
